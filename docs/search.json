[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data graphics\nI started doing data visualization work with an engineering education research group around 2008. The iterative process we developed to “find the story” in our data has been instrumental in shaping my approach to data graphics. You can read more about our research at https://midfield.online/.\nIt ain’t the cover of Rolling Stone, but a Sankey diagram I made was used to inform the cover art of the journal issue in which the article appeared, with the data graphic “enhanced” by their art department.\n\nMusic\nMy dad was a guitar player and taught me my first chords. I’ve been songwriting and performing since my teens. A favorite musical pilgrimage is the annual Swannanoa Gathering near Asheville, NC in late July. I’ve posted some draft tracks to SoundCloud.\n\nBrief bio\nI am a Californian, born in Redding and living lots of places—Yuba City, Sacramento, Alameda, Paso Robles, and Simi Valley to name a few. I am a graduate of California State University, Northridge (1991), and the University of Washington (1993, 1995). I taught mechanical engineering courses at Rose-Hulman Institute of Technology in Terre Haute, Indiana, from 2000-2020.\nI retired from teaching in 2020 and spend my time songwriting, woodworking, R data graphics consulting and blogging, and opening doors for cats who give me doleful looks when a door fails to open on perpetual summer."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "To comment on a post, please use the Comment box at the end of the post. A GitHub account is required.\nFor other matters graphical, I can be reached at\n\n\ngraphdoctor@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Stories",
    "section": "",
    "text": "My previous blog at graphdoctor.com redirects here. I am porting my earlier posts with minor edits plus R code for reproducibility.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nmidfieldr v1.0.1\n\n\n\n\n\n\n\nR\n\n\nengineering education\n\n\nmidfieldr\n\n\n\n\nProvides tools and methods for studying undergraduate student-level records from the MIDFIELD database.\n\n\n\n\n\n\n2023-07-09\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing formatdown\n\n\n\n\n\n\n\nR\n\n\nengineering notation\n\n\nscientific notation\n\n\nunits\n\n\nrmarkdown\n\n\nquarto\n\n\n\n\nProvides powers-of-ten formatting for a numerical vector or data frame column when creating documents in rmarkdown or quarto markdown.\n\n\n\n\n\n\n2022-11-21\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nMultiple studies, one chart\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nggplot2\n\n\n\n\nCombining the results of studies with different time spans in a single chart when changes over time are the story.\n\n\n\n\n\n\n2022-09-05\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nSpiders, facets, and dots, oh my!\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\nData storytelling\n\n\nmagick\n\n\n\n\nA case study comparing the effectiveness of a radar chart to a faceted dot chart.\n\n\n\n\n\n\n2022-08-21\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nAnother case for redesigning dual axis charts\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\n\n\nAn example of replacing a dual axis chart with alternatives better suited to drawing inferences about the data.\n\n\n\n\n\n\n2022-06-06\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nRevisiting a lying chart\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nVisual lies\n\n\n\n\nRevisiting the rhetoric of a deliberate visual lie, updating the data set, and providing data and code for reproducibility.\n\n\n\n\n\n\n2022-05-27\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nFacets that are not small multiples\n\n\n\n\n\n\n\nR\n\n\nggplot2\n\n\n\n\nUsing the scales and space arguments of facet_grid() to manage panel heights of faceted charts when categories are not independent.\n\n\n\n\n\n\n2022-03-09\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nSurvey data I/O with likert\n\n\n\n\n\n\n\nR\n\n\n\n\nHow to prepare different forms of Likert-style survey data for the R likert package and edit the results to create 100% stacked-bar charts.\n\n\n\n\n\n\n2022-02-13\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nSurvey data chart designs\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nEngineering education\n\n\n\n\nComparing chart designs for displaying Likert-style survey results and concluding that the 100% stacked-bar chart is the most effective.\n\n\n\n\n\n\n2022-02-12\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nPlot the variable of interest\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nEngineering education\n\n\n\n\nA chart best serves its rhetorical purpose by directly illustrating the variables of interest discussed in the text.\n\n\n\n\n\n\n2022-02-03\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nStacked-bar alternatives\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nEngineering education\n\n\n\n\nRedesigning two stacked bar charts to better convey the stories in their data.\n\n\n\n\n\n\n2022-01-14\n\n\nRichard Layton\n\n\n\n\n\n\n  \n\n\n\n\nThe missing relation\n\n\n\n\n\n\n\nR\n\n\nData storytelling\n\n\nEngineering education\n\n\n\n\nRedesigning a grouped-bar chart and including missing data to reveal an important change to the story in the data.\n\n\n\n\n\n\n2015-07-20\n\n\nRichard Layton\n\n\n\n\n\n\nNo matching items\n\nSoftware credits\n\n\nR and RStudio\ndata.table for manipulating data\n\nggplot2 for data graphics\n\nggpubfigs for color vision deficient inclusive palettes\nquarto for constructing this blog"
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Open source licenses",
    "section": "",
    "text": "The following licenses apply to the text, images, code, and data in this blog. My goal is to minimize legal encumbrances to the dissemination, sharing, use, and re-use of this work. However, the existing rights of authors whose work is cited are reserved to those authors.\n\nCC-BY 4.0 for text and images\n\nGPL-3 for code\n\nCC0 for data"
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html",
    "href": "posts/2015-07-20-the-missing-relation/index.html",
    "title": "The missing relation",
    "section": "",
    "text": "Summary\n\n\n\nIn redesigning a bar chart, I add data for context that shifts the focus of the story. I also correct a visual distortion by substituting a scatterplot for a bar chart that has a non-zero baseline.\nIn August 2007, Science published a bar graph that illustrates how design decisions made to attract a reader’s eye can also distort and conceal meaning. While the distortions were surely unintentional, the integrity of the story in the data was compromised nevertheless.\nA colleague and I noted the perceptual issues of this bar graph when the article first appeared. With the permission of the first author, Norm Fortenberry, we sent a shorter version of this critique to the Science editor. They responded by posting a scatterplot much like the one shown here in an online addendum.\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")"
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html#the-original-graph",
    "href": "posts/2015-07-20-the-missing-relation/index.html#the-original-graph",
    "title": "The missing relation",
    "section": "The original graph",
    "text": "The original graph\nThe graph shows a time series of retention rates for two cohorts of undergraduate students (Fortenberry et al., 2007). One cohort had matriculated in engineering in a First Year Engineering Program (FYEP) and one cohort had not (non-FYEP). The main story is clear, that retention rates drop over time for both cohorts but that FYEP students are always retained at a higher rate than non-FYEP students. Yet the graph design distorts one comparison and omits another.\n\n\n\nGraph from Science. Reprinted with permission from AAAS."
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html#visual-distortion",
    "href": "posts/2015-07-20-the-missing-relation/index.html#visual-distortion",
    "title": "The missing relation",
    "section": "Visual distortion",
    "text": "Visual distortion\nThe visual design distorts the relationship between the two cohorts. While the printed numbers show that the difference between the two cohorts remains fairly constant (about 10%) over time, the visual message conveyed by the bar heights is that the difference between cohorts increases over time. The FYEP bar is at first approximately 1/3 higher than the adjacent bar, then nearly twice as high, then more than three times as high.\n\n\nR code\n# read data from blog data directory\ndt &lt;- fread(\"data/retention-2007.csv\")\n\n# grouped bar graph \n# use y = pct - 50 to force ggplot to produce a non-zero baseline \n# then relabel the y-axis scale, i.e., 0 is labeled 50%\nggplot(dt, aes(x = term, y = pct-50, color = status, fill = status)) +\n  geom_bar(stat = \"identity\", \n           position = position_dodge(), \n           width = 0.8, \n           color = \"black\") + \n  scale_x_continuous(breaks = c(3, 5, 7), \n                     labels = c(\"Third\", \"Fifth\", \"Seventh\"), \n                     expand = c(0.05, 0.15)) +\n  scale_y_continuous(limits = c(0, 40), \n                     breaks = seq(0, 40, 10), \n                     labels = c(\"50%\", \"60%\", \"70%\", \"80%\", \"90%\"), \n                     expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"gray80\", \"lightgoldenrod1\")) +\n  labs(x = \"Semester\", \n       y = \"Retention rate\") +\n  annotate(\"text\", \n           x = 0.1 + c(3, 5, 7), \n           y = -50 + c(85, 70, 62), \n           label = c(\"aprox. 1/3 higher\", \n                     \"approx. 2x higher\", \n                     \"more than\\n3x higher\"), \n           hjust = 0) + \n  theme_light() + \n  theme(legend.position = \"none\", \n        panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank())\n\n\n\n\n\nFigure 1: A non-zero baseline distorts the visual comparisons of bars.\n\n\n\n\nThe problem is the non-zero baseline—a well-known concern when using bar charts. Naomi Robbins reminds us that not all graphs require a zero baseline, contrary to Darrell Huff’s advice in his 1954 classic How to Lie with Statistics, but a bar graph without a zero baseline inevitably (and sometimes purposefully) exaggerates differences.\nThe caption reinforces the miscommunication by stating that the FYEP course “improved retention…into the third, fifth, and seventh semester,” subtly implying a difference that increases over time rather than remaining constant."
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html#data-structure",
    "href": "posts/2015-07-20-the-missing-relation/index.html#data-structure",
    "title": "The missing relation",
    "section": "Data structure",
    "text": "Data structure\nBecause the structure of a data set is an important factor in designing a chart, I classify the variables in Table 1.\n\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nterm\ncategorical, ordinal, discrete time, 3 levels\n\n\nstatus\ncategorical, nominal, 2 levels\n\n\npercent retention\nquantitative\n\n\n\nTable 1: Data structure\n\n\nThe data are available in the blog data directory as a CSV file.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/retention-2007.csv\")"
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html#redesign",
    "href": "posts/2015-07-20-the-missing-relation/index.html#redesign",
    "title": "The missing relation",
    "section": "Redesign",
    "text": "Redesign\nA prominent element of this grouped-bar design is time on the horizontal axis, implying that time is the independent variable. Conventionally, time-dependent variables are best graphed as scatterplots with connected dots to display their evolution over time (Doumont, 2009, p. 141). The difference between the two cohorts is clearly seen to be nearly constant.\n\n\nR code\n# redesign the bar chart as a scatterplot\nf &lt;- ggplot(data = dt, mapping = aes(x = term, y = pct, fill = status)) +\n  \n  # helper line to almost connect the dots \n  geom_line(linetype = 2, color = \"gray60\") +\n  \n  # white point overprints the line\n  geom_point(size = 9, shape = 16, color = \"white\") +\n  \n  # data marker overprints the white space \n  geom_point(size = 3, shape = 16, color = \"black\") +\n  \n  # scales \n  scale_x_continuous(limits = c(1, 7), breaks = c(1, 3, 5, 7)) +\n  scale_y_continuous(limits = c(50, 100), breaks = seq(0, 100, 20)) + \n  labs(x = \"Semester\", y = \"Retention rate\") + \n  \n  # edit theme \n  theme_light() + \n  theme(legend.position = \"none\", \n        panel.grid = element_blank(), \n        axis.text = element_text(size = 11), \n        axis.title = element_text(size = 11))\n\n# print\nf\n\n\n\n\n\nFigure 2: Scatterplot of the numerical values in the original bar chart.\n\n\n\n\nHowever, a significant story in these data is (inadvertently) overlooked by omitting semester one, the point in time at which both cohorts would be considered 100% retained—the “missing relation” in the title. Only by including the starting time point can we see the importance of the early semesters.\nThe overlooked story is the early impact of the FYEP course with its higher rate of retention from semester 1 to 3. After semester 3, the factors affecting attrition seem to act on both groups equally—the lines are effectively parallel after semester 3. Thus the important impact of FYEP is in the first two terms. The visual story is clear when we include the missing term.\n\n\nR code\n# add the missing term 1 \nsem1 &lt;- data.table(term = c(1, 1), status = c(\"fyep\", \"nonfyep\"), pct = c(100, 100))\ndt2 &lt;- rbindlist(list(sem1, dt))\n\n# substitute data in the previous chart\ng &lt;- f %+% dt2\ng\n\n\n\n\n\nFigure 3: Adding the first term in which both cohorts are at 100%.\n\n\n\n\nOne final design point: unlike a bar graph, a scatter plot does not require a zero baseline. However, I can include the full 0–100% range to display that the lowest rate of retention is still above 50%, an important result (as discussed in the prose of the article) compared to the retention rates of non-engineering disciplines. e.g., 42% in biological sciences and 30% in math and physical sciences.\n\n\nR code\n# start with the previous chart, replace the scale\nh &lt;- g %+% \n  scale_y_continuous(limits = c(0, 100), breaks = seq(0, 100, 20)) +\n  \n  # label the lines\n  annotate(\"text\", \n           x = c(4, 2), \n           y = c(85, 68), \n           label = c(\"FYEP students\\n(N = 2218)\", \"Non-FYEP students\\n(N = 2942)\"), \n           hjust = 0)\nh\n\n\n\n\n\nFigure 4: Providing a full scale on the y-axis."
  },
  {
    "objectID": "posts/2015-07-20-the-missing-relation/index.html#context",
    "href": "posts/2015-07-20-the-missing-relation/index.html#context",
    "title": "The missing relation",
    "section": "Context",
    "text": "Context\nMy critique to this point has focused on clarity and a minimalist design aesthetic, hallmarks of the “rhetoric of science” (Kostelnick, 2008). However, recognizing that these downward sloping curves represent decisions made by real students invites us to ask about the human stories in these data. Are the students in either group better off? Are the students who leave engineering graduating in other disciplines? Is retention even a concern to students? In light of such questions, the graphs seem inadequate, as if we’ve missed an opportunity to tell important human stories.\nIn the text of the article, the authors do address such concerns. Retention rates reflect student decisions that are influenced almost exclusively by human factors: “student’s background, college administrative issues, academic and social integration, attitude and motivation, and fit within an institution.” Thus the retention data are a surrogate measure of some combination of these factors.\nThe original graph displays only six paired values (rate and semester). And though my redesigned graph corrects the distortions of the original, it still displays only eight paired values. Neither graph has the visual impact it might have had if designed to convey the important story the authors tell in their prose."
  },
  {
    "objectID": "posts/2022-01-14-stacked-bar-alternatives/index.html",
    "href": "posts/2022-01-14-stacked-bar-alternatives/index.html",
    "title": "Stacked-bar alternatives",
    "section": "",
    "text": "Summary\n\n\n\nThe rhetorical shortcomings of stacked-bar charts are overcome using alternative designs better suited for making visual comparisons. Both examples start as stacked-bar charts but end as different types, based on the variables to be shown and the messages to be conveyed.\nIn a recent article, the authors used two stacked bar charts to illustrate data on postdoctoral engineering PhDs (Main et al., 2021). The charts are truthful but not particularly informative—the visual logic of stacked bars tends to obscure rather than inform insight. By redesigning the charts, I hope to better align the logic of the visuals with the logic of the arguments.\nThe data in the article are summarized from the National Science Foundation’s (NSF) 1993–2013 Survey of Doctorate Recipients (SDR) and 1985–2013 Survey of Earned Doctorates (SED). I couldn’t find the original NSF annual data tables, so I approximated the values for my charts by measuring the lengths of bar segments in the original figures.\nMy purpose is not to find fault with the authors. Indeed, stacked bar charts like these are found throughout the NSF publications that report on the raw data. The authors can hardly be faulted for conforming to visual conventions sustained by the NSF.\nThe R code for the post is listed under the “R code” pointers.\nR code\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"GGally\")"
  },
  {
    "objectID": "posts/2022-01-14-stacked-bar-alternatives/index.html#eliminating-the-trivial",
    "href": "posts/2022-01-14-stacked-bar-alternatives/index.html#eliminating-the-trivial",
    "title": "Stacked-bar alternatives",
    "section": "Eliminating the trivial",
    "text": "Eliminating the trivial\nThe original chart\nIn this chart, the authors display responses to the survey question “What was your primary reason for taking this postdoc?” by PhD completion year.\n\n\n\nOriginal Figure 2: Primary reasons for obtaining postdoctoral training among engineering PhDs: 1995–2011 (Main et al., 2021).\n\n\nData structure:\n\npercentage of respondents annually selecting reasons, quantitative variable\n\nreasons for postdoctoral training, nominal categorical variable, 6 levels\n\nyear of PhD completion, discrete ordinal categorical variable, 1995–2011\n\nThe data are available in the blog data directory as a CSV file.\nExploratory design: evolutions\nA prominent element of this stacked-bar design is time on the horizontal axis. A visual convention in this discourse community is that independent variables occupy horizontal axes, implying here that time is the independent variable. Conventionally, time-dependent variables are best graphed as scatterplots with connected dots to display their evolution over time (Doumont, 2009, p. 141).\nThe figure below shows the stacked-bar data as a collection of evolutions—how survey percentages vary over time conditioned by the reason for obtaining postdoc training. The panels are organized in “graph order”, that is, increasing median value from left to right and from bottom to top (the panel median is shown as a horizontal reference line in each panel).\n\n\nR code\ndt &lt;- fread(\"data/fig2.csv\")\n\ndt[, med_pct := median(pct), by = \"reason\"]\n\nggplot(data = dt, aes(x = year, y = pct)) +\n  geom_line(linetype = 2, color = \"gray\") +\n  geom_hline(aes(yintercept = med_pct), linetype = 3) + \n  geom_point() +\n  facet_wrap(vars(reorder(reason, pct, median)), ncol = 2, as.table = FALSE) +\n  labs(x = \"PhD completion year\", y = \"\")\n\n\n\n\n\nFigure 1: Graphing the data as a set of time-series.\n\n\n\n\nNo particular time-dependent trend stands out. However, two of the panels—Training in areas outside of PhD field and Other employment not available—appear to be inversely correlated. This made me wonder if and to what extent correlations exist.\nExploratory design: correlations\nOne approach to visualizing multivariate correlations is a scatterplot matrix, a grid of scatterplots of pairwise relationships (Emerson et al., 2013) as shown below. The lower triangle shows graphs of data taken two variables at a time with each data marker representing a year (the time variable is still present); the diagonal shows the distribution of each reason; and the upper triangle gives the pairwise Pearson correlation coefficients.\n\n\nR code\ndtwide &lt;- copy(dt)\ndtwide &lt;- dtwide[.(reason = c(\n  \"Additional training in PhD field\", \n  \"Training in areas outside of PhD field\", \n  \"Postdoc generally expected for career in this field\", \n  \"Work with a specific person or place\", \n  \"Other\", \n  \"Other employment not available\"), \n  to = c(\n    \"In field\", \n    \"Out of field\", \n    \"Expected\", \n    \"Specific\", \n    \"Other\", \n    \"Unavailable\")), \n  on = \"reason\", \n  reason := i.to]\n\ndtwide &lt;- dcast(dtwide, year ~ reason, value.var = \"pct\", fill = 0)\n\n# https://stackoverflow.com/questions/30720455/how-to-set-same-scales-across-different-facets-with-ggpairs\nlowerfun &lt;- function(data, mapping){\n  ggplot(data = data, mapping = mapping) +\n    geom_point() +\n    scale_x_continuous(limits = c(0, 45)) +\n    scale_y_continuous(limits = c(0, 45)) +\n    geom_smooth(formula = y ~ x, method = \"loess\", se = FALSE, size = 0.5)\n} \n\nggpairs(dtwide, \n        columns = 2:7, \n        upper = list(continuous = wrap(\"cor\", size = 3)), \n        lower = list(continuous = wrap(lowerfun))\n        )\n\n\n\n\n\nFigure 2: Looking for correlations.\n\n\n\n\nTwo variable pairs have correlation coefficients greater than 0.6, which for human behavior indicates a fairly strong correlation. Both are negative (inverse) correlations: one (that I noticed in the scatterplots) between “other employment unavailable” and “training outside the PhD field”; and a second between “expected in the field” and “other”. In the figure below. I extract both pairs of data to isolate the pairings.\n\n\nR code\nggpairs(dtwide, \n        columns = c(7, 5), \n        upper = list(continuous = wrap(\"cor\", size = 3)), \n        lower = list(continuous = wrap(lowerfun))\n        )\n\n\n\n\n\nFigure 3: Inverse correlation between “other employment unavailable” and “training outside the PhD field”.\n\n\n\n\n\n\nR code\nggpairs(dtwide, \n        columns = c(2, 4), \n        upper = list(continuous = wrap(\"cor\", size = 3)), \n        lower = list(continuous = wrap(lowerfun))\n        )\n\n\n\n\n\nFigure 4: Inverse correlation between “expected in the field” and “other”.\n\n\n\n\nThe messages appears to be that\n\nyears with a higher percentage of “Other employment not available” correlate with a lower percentage of “Training in areas outside of PhD field”, suggesting that when higher numbers of students accept postdoc positions because other employment is not to be found, they are not generally interested in obtaining new training outside their field.\nyears with a higher percentage of “Postdoc generally expected for career in this field” correlate to a lower percentage of “Other” reasons, which seems a reasonable if unremarkable result.\n\nMildly interesting, but not really compelling.\nFinal design: distributions\nThe evolution-assumption in the previous designs imposes an unnecessary barrier to finding stories in these data: time in this case is a prominent, but trivial, variable—the design emphasizes the trivial (Wainer, 1997, p. 30).\nHaving the year in the data does not mean we have to use it. Ignoring time, I’m left with a data set comprising distributions of percentages conditioned by reason. For comparing distributions, the appropriate graph design is a box and whisker plot—the conventions of the box and whisker plot should be familiar to members of this discourse community.\n\n\nR code\nggplot(dt, aes(x = pct, y = reorder(reason, pct, median))) +\n  geom_boxplot(width = 0.6) +\n  labs(x = \"Percentage reasons cited, 1995-2011\", y = \"\") +\n  theme_light(base_size = 12) +\n  theme(legend.position = \"none\") \n\n\n\n\n\nFigure 5: Visual comparisons are more definitive once we eliminate the trivial (time) as the independent variable.\n\n\n\n\nHere the reasons are positioned from bottom to top in order of increasing median value. I conclude (as do the authors) that the top reason for accepting a postdoc position is additional training in field. The next three reasons have similar median values, so their ordering is less meaningful.\nBy comparing ranges and interquartile ranges (IQRs) I can draw a conclusion obscured by the previous designs. The range and IQR of “Additional training in PhD field” are noticeably less dispersed than the same measures of all other reasons. Thus in-field-training is both the most frequent reason and the least variable year to year–a conclusion that is hidden when we emphasize time dependence.\nLastly, with no legend to decode, the audience should find the redesign easier to interpret than the original. And from a publisher’s perspective, the new chart occupies less space on the page and does not have to be printed in color—less important for online documents, but important considerations for printed works.\nImpact\nIn discussing the stacked bar chart in the article, the authors state,\n\nThe most frequently indicated reasons for taking a postdoc after PhD completion were additional training in PhD field, additional training outside of PhD field, and postdoc training is generally expected for careers in their PhD field. The reasons that engineering PhDs provided for obtaining postdoc training fluctuates between 1995 and 2011.\n\nMy conclusions differ slightly from theirs and have a bit more nuance, though in neither case are the messages particularly compelling.\nNevertheless, the redesigned chart has an impact in serving its rhetorical goal better than the original. What one sees in the chart and what one reads in the text are more closely aligned than in the original. As Tufte (1997, p. 53) writes,\n\n…if displays of data are to be truthful and compelling, the design logic of the display must reflect the intellectual logic of the analysis…"
  },
  {
    "objectID": "posts/2022-01-14-stacked-bar-alternatives/index.html#making-visual-comparisons",
    "href": "posts/2022-01-14-stacked-bar-alternatives/index.html#making-visual-comparisons",
    "title": "Stacked-bar alternatives",
    "section": "Making visual comparisons",
    "text": "Making visual comparisons\nThe original chart\nIn this chart, the authors provide a “descriptive summary of the relationship between postdoctoral training and subsequent career outcomes 7–9 years after PhD completion.”\n\n\n\nOriginal Figure 3: Percentage of non-postdocs and postdocs by employment sector and primary work activity 7-9 years after PhD completion, 1993-2013. (Main et al., 2021).\n\n\nData structure (Figure 3a)\n\npercentage of respondents in an employment sector, quantitative variable\n\nemployment sectors, nominal categorical variable, 5 levels\n\npostdoc status, nominal categorical variable, 2 levels\n\nData structure (Figure 3b)\n\npercentage of respondents in a work activity, quantitative variable\n\nprimary work activity, nominal categorical variable, 5 levels\n\npostdoc status, nominal categorical variable, 2 levels\n\nThe data are available in the blog data directory as a CSV file.\nRedesign: dot chart\nStacked bars of this type, like pie charts, show fractions of a whole. Such charts are a “good choice for lay audiences, but they certainly lack the accuracy of alternative representations” (Doumont, 2009, p. 134). Such data are often best encoded using dot charts—charts with dots along a common scale (Cleveland, 1984).\n\n\nR code\ndt &lt;- fread(\"data/fig3.csv\")\n\ndt3a &lt;- dt[type %chin% c(\"Employment sector\")]\ndt3a[, sector := type_levels]\ndt3a[, type := NULL]\ndt3a[, type_levels := NULL]\n\ndt3b &lt;- dt[type %chin% c(\"Primary work activity\")]\ndt3b[, activity := type_levels]\ndt3b[, type := NULL]\ndt3b[, type_levels := NULL]\n\ndt3b &lt;- dt3b[.(activity = c(\n  \"Research and development\", \n  \"Teaching\", \n  \"Management and admin\", \n  \"Computer applications\", \n  \"Other\"), \n  to = c(\n    \"Res & dev\", \n    \"Teaching\", \n    \"Mgmt & admin\", \n    \"Computer apps\", \n    \"Other\")), \n  on = \"activity\", \n  activity := i.to]\n\n\nIn the original stacked bar chart, the authors have one column for each postdoc status level, indicating the rhetorical goal of comparing the experiences of PhDs with postdocs to those without. This lends itself to plotting the dots for postdocs and non-postdocs along the same row of the chart, facilitating a direct visual comparison.\nBelow, the employment sector graph is redesigned as a dot chart with rows ordered from bottom to top in order of increasing mean percentage.\n\n\nR code\nggplot(dt3a, aes(x = fraction, y = reorder(sector, fraction, mean), color = status, fill = status)) +\n  geom_point(size = 3, shape = 21) +\n  labs(x = \"Percentage\", \n       y = \"\", \n       title = \"Employment sector\") +\n  theme_light(base_size = 12) +\n  theme(legend.title = element_blank()) + \n  scale_color_manual(values = c(\"black\", \"black\")) + \n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  scale_x_continuous(limits = c(0, 60), breaks = seq(0, 100, 10))\n\n\n\n\n\nFigure 6: Quantitative comparisons are visually more accessible when data are plotted along a common horizontal scale.\n\n\n\n\nIn discussing the stacked bar chart in the article, the authors state,\n\nMost postdoctoral scholars and non-postdocs worked in industry 7–9 years after the PhD. However, the share of PhDs who worked in industry is lower among the postdoc group compared to non-postdocs. Compared with non-postdocs, a greater share of postdocs go on to tenure-track and non-tenure-track faculty positions.\n\nThe argument is also supported by the dot chart but qualitative terms “most”, “lower”, etc. could be replaced with quantitative comparisons. For example, the dot chart supports the argument that industry employs just over 40% of postdocs and more than 50% of non-postdocs. Moreover, industry employs twice the number of postdocs than the next most popular sector (tenure-track), and more than three times the number of non-postdocs.\nA similar outcome can be seen in the redesigned work activity chart.\n\n\nR code\nggplot(dt3b, aes(x = fraction, y = reorder(activity, fraction, mean), color = status, fill = status)) +\n  geom_point(size = 3, shape = 21) +\n  labs(x = \"Percentage\", \n       y = \"\", \n       title = \"Primary work activity \") +\n  theme_light(base_size = 12) +\n  theme(legend.title = element_blank()) + \n  scale_color_manual(values = c(\"black\", \"black\")) + \n  scale_fill_manual(values = c(\"white\", \"black\")) +\n  scale_x_continuous(limits = c(0, 60), breaks = seq(0, 100, 10))\n\n\n\n\n\nFigure 7: Dot charts are superior to stacked-bar charts for audiences expecting visual access to quantitative comparisons.\n\n\n\n\nIn discussing the stacked bar chart in the article, the authors state,\n\nCompared with non-postdocs, a greater proportion of postdoctoral scholars engage in research and development as their primary work activity. Meanwhile, a greater proportion of non-postdocs perform management and administrative duties.\n\nAgain, the discussion is also consistent with the new chart but a quantitative comparison is easier to see: research and development employs over 40% of non-postdocs and over 50% of postdocs, which for both groups is at least four times the number in the next most popular sector (teaching).\nImpact\nDot charts are superior to stacked bar charts for data of this type. As illustrated above, relative magnitudes are more easily visualized and quantified than with stacked bars. Even if one prefers not to include quantities in the verbal discussion, readers can easily make quantitative inferences on their own.\nAlso, in this particular case, the dot chart legend is much easier to decode, with two entries compared to 5 entries for the original stacked bars."
  },
  {
    "objectID": "posts/2022-01-14-stacked-bar-alternatives/index.html#recognizing-conventional-grip",
    "href": "posts/2022-01-14-stacked-bar-alternatives/index.html#recognizing-conventional-grip",
    "title": "Stacked-bar alternatives",
    "section": "Recognizing conventional grip",
    "text": "Recognizing conventional grip\nI mentioned in the introduction that NSF reports from which some of these data were obtained sustain the visual convention of stacked bar charts even though the stacked-bar design is deficient compared to alternative designs available. This persistence of convention highlights what Kostelnick and Hassett call the tenacity of conventional grip (2003, p. 171). They write,\n\nReaders become highly invested in the status quo because it cuts a well-worn path, and deviations from that familiar path can seem arduous, risky, and unnecessary. The well-worn path can give sanction to conventuional practices that seem to violate perceptual principles of effective design…once readers have acquired a knack for reading these conventions, readers become quite proficient and may strongly resist wandering off that path.\n\nHowever, conventional grip can create problems for both authors and audiences (ibid., 182),\n\nAlthough grip can greatly benefit users by creating a stable environment for shaping and interpreting visual language, conventions can also become so entrenched that they interfere with meaning making by not changing to match conditions or by leading to mindless, unwarranted conformity. Designers can easily succumb to conventional inertia and perhaps not even realize its rhetorical drawbacks."
  },
  {
    "objectID": "posts/2022-02-01-plot-the-variable-of-interest/index.html",
    "href": "posts/2022-02-01-plot-the-variable-of-interest/index.html",
    "title": "Plot the variable of interest",
    "section": "",
    "text": "Summary\n\n\n\nI redesign a stacked-bar chart to attempt to better align the logic of the display with the logic of the argument. The visual structure of the redesigned chart suggests a concomitant reorganization of the prose, illustrating the interplay between visual and verbal rhetoric.\nIn a 2021 article, the authors summarize 134 engineering-curricula articles for teachers of pre-college students. The authors classify the articles by grade level and the type of engineering inquiry (Purzer et al., 2021).\nIn section 7, the authors include a data table of numbers of articles by grade level and inquiry type. A stacked-bar chart represents the numbers in the table. In supporting their argument, however, the authors cite ratios of these values—ratios that might be inferred, but are not directly displayed, in the chart provided.\nI take the text to represent the authors’ intended message and look for ways to more effectively represent that message visually.\nMy intention is not to critique the article as a whole, which develops a “honeycomb of engineering” research framework for examining how engineering concepts and practices are taught at pre-college levels. I focus instead on the interplay between data, argument, and charts in one section only.\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"cowplot\")"
  },
  {
    "objectID": "posts/2022-02-01-plot-the-variable-of-interest/index.html#data-structure",
    "href": "posts/2022-02-01-plot-the-variable-of-interest/index.html#data-structure",
    "title": "Plot the variable of interest",
    "section": "Data structure",
    "text": "Data structure\nIn the original Table 2, the authors classify 134 articles by grade level and the type of engineering inquiry. These are the raw numbers they cite in their discussion. The parenthetical titles in the column heading are the journals in which the articles appear.\n\n\n\n(original Table 2) The distribution of the number of articles by engineering inquiry and grade level band. (Purzer et al., 2021.)\n\n\nBecause the structure of a data set is an important factor in designing a chart, I classify the variables in Table 1. The multiway dot chart was designed specifically for data of this type: one quantitative variable depending on two independent categorical variables (Cleveland, 1993, p. 302).\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nnumber of published articles\nquantitative\n\n\ntype of engineering inquiry\ncategorical, nominal, 6 levels\n\n\ngrade level\ncategorical, ordinal, 3 levels\n\n\n\nData structure\n\nThe data are available in the blog data directory as a CSV file.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/honeycomb-2021.csv\")\n\n# order the levels of grade level\ndt &lt;- dt[, grade_level := factor(grade_level, levels = c(\"Elementary School\", \"Middle School\", \"High School\"))]\n\n# compute percentage of articles by grade level\ndt[, pct_level := round(N / N_level * 100, 0)]\n\n# obtain the total N of articles\narticle_total &lt;- sum(dt$N)"
  },
  {
    "objectID": "posts/2022-02-01-plot-the-variable-of-interest/index.html#limitations-of-stacked-bars",
    "href": "posts/2022-02-01-plot-the-variable-of-interest/index.html#limitations-of-stacked-bars",
    "title": "Plot the variable of interest",
    "section": "Limitations of stacked bars",
    "text": "Limitations of stacked bars\nThe original Figure 3 is a conventional stacked-bar representation of the numbers in Table 2. The authors improve on the usual default software settings by ordering the rows of the chart from top to bottom by increasing totals.\n\n\n\n(original Figure 3) The representation of the six engineering inquiries as three grade-level bands published in the three NSTA journals between 2005 and 2019 (Purzer et al., 2021.)\n\n\nThe perceptual limitations of stacked-bar charts are well-known: visual comparisons are effective for quantities having a common baseline and are ineffective otherwise.\n\nStacked bars work well when you want to compare totals across different categories, and then within a given category, you want some understanding of the subcomponent pieces. Notice though, that they work less well if you want to compare those subcomponent pieces across categories. This is because as soon as you get past the first series, you no longer have a consistent baseline to use to compare. This is a harder comparison for our eyes to make, so something to keep in mind when reaching for stacked bars. —Cole Nussbaumer Knaflic (2017)"
  },
  {
    "objectID": "posts/2022-02-01-plot-the-variable-of-interest/index.html#whats-your-point",
    "href": "posts/2022-02-01-plot-the-variable-of-interest/index.html#whats-your-point",
    "title": "Plot the variable of interest",
    "section": "What’s your point?",
    "text": "What’s your point?\nReferring to the stacked-bar chart (original Figure 3) and the data table (original Table 2), the authors state,\n\nA total of 134 articles that made explicit connections to engineering, included sufficient details on design challenges and engineering problems, and were labeled as engineering lessons (see Figure 3). Table 2 presents the frequency of these categories across three grade-level bands. Of the 134 articles, 63 (47%) were published at the elementary level in Science & Children, 51 (38%) in Science Scope representing the middle school, and 20 (15%) in The Science Teacher at the high school level.\n\nThe original Figure 3 does not reflect the logic of this argument. The stacked-bar chart visually compares article totals by type of inquiry and (as Knaflic says) offers some “understanding of the sub-component pieces,” but neither of these arguments are made in the text.\nSo what’s the point of the chart?\n\nWhat’s your point? Seriously, that’s the most important question to ask when creating a data visualization. —Stephanie Evergreen (2021)\n\nIf the point of the chart is to illustrate the three percentages mentioned in the text (15%, 38%, and 47%), then a combined bar chart and table would serve.\n\n\nR code\n# aesthetic assignments across charts\ntext_col_x      &lt;- 105\ntheme_text_size &lt;- 11\ngeom_text_size  &lt;- 4\ngeom_point_size &lt;- 3.5\nbar_fill        &lt;- \"#C2A5CF\" # light purple\ngray_text       &lt;- \"#636363\"\n\n# subset selected data for this chart\nsel &lt;- dt[, .(grade_level, N_level)]\nsel &lt;- unique(sel)\n\n# basic bar chart\np &lt;- ggplot(data = sel, mapping = aes(x = N_level, y = grade_level)) +\n  geom_bar(stat = \"identity\", width = 0.7, fill = bar_fill) +\n  scale_x_continuous(limits = c(-10, text_col_x)) +\n  coord_cartesian(clip = \"off\") +\n  labs(x = \"\", y = \"\")\n\n# edit theme \np &lt;- p + \n  theme_minimal() +\n  theme(panel.grid  = element_blank(), \n        axis.text.x = element_blank(), \n        axis.text.y = element_text(size = theme_text_size), \n        plot.title  = element_text(size = theme_text_size, face = \"plain\", hjust = 0))\n\n# column of percentages\np &lt;- p + \n  geom_text(aes(x = -5, y = reorder(grade_level, N_level), label = paste0(round(100 * N_level/article_total, 0), \"%\")), \n            hjust = 1, color = gray_text, size = geom_text_size)\n\n# column of N\np &lt;- p + \n  geom_text(aes(x = text_col_x, y = reorder(grade_level, N_level), label = N_level), \n            hjust = 1, color = gray_text, size = geom_text_size)\n\n# total N\np &lt;- p + \n  geom_text(x = text_col_x, y = 0, label = paste0(\"Total: \", article_total), \n            hjust = 1, color = gray_text, size = geom_text_size)\n\n# print the chart\np\n\n\n\n\n\nFigure 1: Number of articles by grade level, 2005-2019.\n\n\n\n\nIn this design, based on (Doumont, 2009, p. 135), bars are drawn horizontally so the labels are easily read. Combined with the tabulated numbers, a scale is unnecessary. To respect the proportion among the data, bars are shown in full (starting at zero). Because grade level is an ordinal category (ordered levels), I order the rows in “graph order,” that is, with increasing grade level from bottom to top.\nIn contrast, if the point of the original chart is to compare totals by type of inquiry—which the original figure supports but the text neglects—then a similar chart-table could be used. Because inquiry type is a nominal category (unordered levels), rows are ordered by the number of articles by type.\n\n\nR code\nggplot(data = dt, mapping = aes(x = N, y = reorder(inquiry, N_inquiry))) +\n  geom_bar(stat = \"identity\", width = 0.75, fill = bar_fill) +\n  scale_x_continuous(limits = c(-10, text_col_x)) +\n  coord_cartesian(clip = \"off\") +\n  labs(x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid  = element_blank(), \n        axis.text.x = element_blank(), \n        axis.text.y = element_text(size = theme_text_size), \n        plot.title  = element_text(size = theme_text_size, face = \"plain\", hjust = 0)) + \n\n  # column of percentages\n  geom_text(aes(x     = -5, \n                y     = reorder(inquiry, N_inquiry), \n                label = paste0(round(100 * N_inquiry/article_total, 0), \"%\")), \n            hjust = 1, color = gray_text) + \n  \n  # column of N\n  geom_text(aes(x = text_col_x, y = reorder(inquiry, N_inquiry), label = N_inquiry), \n            hjust = 1, color = gray_text, size = geom_text_size) +\n  \n  # total N\n  geom_text(x = text_col_x, y = 0, label = paste0(\"Total: \", article_total), \n            hjust = 1, color = gray_text, size = geom_text_size)\n\n\n\n\n\nFigure 2: Number of articles by type of engineering inquiry, 2005-2019.\n\n\n\n\nBoth of my suggested charts avoid the limitations of the stacked-bar chart and both support specific—and different—arguments. A short paragraph comparing grade levels is quite distinct from one comparing inquiry types. Deciding which to use (or both) depends on the point(s) you want to make. The text is not clear on what point the original figure is making."
  },
  {
    "objectID": "posts/2022-02-01-plot-the-variable-of-interest/index.html#interplay-between-chart-and-argument",
    "href": "posts/2022-02-01-plot-the-variable-of-interest/index.html#interplay-between-chart-and-argument",
    "title": "Plot the variable of interest",
    "section": "Interplay between chart and argument",
    "text": "Interplay between chart and argument\nFollowing the paragraph quoted earlier, the article goes on to compare different ratios of numbers from the data table, with paragraphs organized around inquiry type. For example, the Design-Build-Test (DBT) discussion begins with,\n\nThe DBT model was observed across all grade levels. These types of lessons and design projects were most popular at the elementary level, featured in 75% (47 of 63) of engineering lessons published at this grade level band, followed by 67% (34 of 51) at the middle-school level and 50% (10 of 20) at the high school level. In the DBT engineering lessons… [continues with project examples]\n\nHere the variable of interest is the “popularity” of DBT as a percentage of articles in a grade level band. In this next chart, I compare the stated percentages visually.\n\n\nR code\n# subset selected data for this chart\nsel &lt;- dt[abbr == \"(DBT)\", .(grade_level, N, N_level, pct_level)]\n\nggplot(data = sel, mapping = aes(x = pct_level, y = grade_level)) +\n  geom_point(size = geom_point_size) +\n  scale_x_continuous(limits = c(0, 80), breaks = seq(0, 100, 10), expand = c(0, 0)) +\n  labs(x = \"Fraction of N articles at grade level (%)\", y = \"\", title = \"Design-Build-Test\") +\n  \n  # edit theme\n  theme_light() +\n  theme(axis.text  = element_text(size = theme_text_size),\n        plot.title = element_text(size = theme_text_size, face = \"plain\", hjust = 0), \n        panel.grid.minor = element_blank()) + \n\n  # total N per row\n  geom_text(aes(x = 1, y = grade_level, label = paste0(\"(N = \", N_level, \")\")),\n            hjust = 0, color = gray_text, size = geom_text_size)\n\n\n\n\n\nFigure 3: Comparing fraction of DBT articles at three grade levels.\n\n\n\n\nThe perceptual difficulty I have with this chart is that each row is a percentage of a different N. Conventionally, a comparison of percentages are based on a common denominator. Yet the chart accurately reflects the argument in the text—my perceptual difficulty with the chart suggests an underlying difficulty in the argument.\nWe encounter the same difficulty with the original stacked-bar chart. Visually, the DBT segments are shown as components of the DBT bar. Verbally, the DBT segments are discussed as percentages of sums of segments across all bars.\nThis is not to say there is anything wrong with the argument—only that the logic of the argument is not reflected in the design logic of the chart.\nChart redesign\nI designed Figure 4 to better reflect the intellectual logic of the argument by plotting percentages of grade-level articles in the same panel. Thus all values in a panel have the same denominator and the percentages in a panel sum to 100%. Consistent with the data structure, the basic design (as expected) is a multiway dot chart.\nTo compare key values across panels, I used color to highlight the three inquiries the grade-level bands have in common. Rows and panels are in graph order. The small bar charts along the side of the multiway chart show the relative proportions of N for each row and their sum is given at the bottom of each column. The names of the source journals are included in the panel headings.\n\n\nR code\n# subset for this chart, percentages set to NA don't print\nsel &lt;- dt[pct_level == 0, pct_level := NA]\nsel[, level_journal := paste0(grade_level, \" (\", journal, \")\")]\n\n# plot 1: multiway dot plot\nplot1 &lt;- ggplot(data = sel, mapping = aes(x = pct_level, y = reorder(inquiry, N_inquiry))) +\n  geom_point(size = geom_point_size, shape = 21, fill = \"white\") +\n  facet_wrap(vars(reorder(level_journal, unclass(grade_level))), ncol = 1, as.table = FALSE) +\n  scale_x_continuous(limits = c(0, 80), breaks = seq(0, 100, 10), expand = c(0, 0)) +\n  coord_cartesian(clip = \"off\") + \n  labs(x = \"Percentage of articles by grade level\", y = \"\") +\n  theme_light() +\n  theme(panel.grid.minor = element_blank(), \n        axis.text        = element_text(size = theme_text_size),\n        strip.background = element_blank(), \n        strip.text  = element_text(size = theme_text_size, color = \"black\", face = \"plain\", hjust = 0),\n        plot.title  = element_text(size = theme_text_size, face = \"plain\", hjust = 0), \n        # top, right, bottom, and left\n        plot.margin = unit(c(0, 0, 0, 0), \"mm\")) +\n  \n  # overprint the top three inquiry dots\n  geom_point(data = sel[abbr == \"(DBT)\"], shape = 21, fill = \"#1b9e77\", size = geom_point_size) + \n  geom_point(data = sel[abbr == \"(UCD)\"], shape = 21, fill = \"#d95f02\", size = geom_point_size) + \n  geom_point(data = sel[abbr == \"(ENS)\"], shape = 21, fill = \"#7570b3\", size = geom_point_size)\n\n# plot 2: bar chart on the side\nplot2 &lt;- ggplot(data = sel, mapping = aes(x = N, y = reorder(inquiry, N_inquiry))) +\n  geom_bar(stat = \"identity\", width = 0.75, fill = bar_fill) +\n  facet_wrap(vars(grade_level), ncol = 1, as.table = FALSE) +\n  scale_x_continuous(limits = c(-20, max(sel$N))) +\n  coord_cartesian(clip = \"off\") + \n  labs(x = \"\", y = \"\", title =) +\n  \n  # edit theme\n  theme_light() +\n  theme(panel.grid = element_blank(), \n        axis.ticks = element_blank(), \n        panel.border = element_blank(), \n        axis.text.x = element_text(color = \"white\"),\n        axis.text.y = element_blank(), \n        strip.background = element_blank(), \n        strip.text = element_text(color = \"white\"),\n        # margins top, right, bottom, and left\n        plot.margin = unit(c(0, 0, 0, -5), \"mm\")) +\n\n  # \"N\" at the top of column\n  annotate(\"text\", x = -5, y = 7.25, label = \"N\", hjust = 1, color = gray_text, size = geom_text_size) +\n  \n  # column of N\n  geom_text(aes(x = -5, y = reorder(inquiry, N_inquiry), label = N),\n            hjust = 1, color = gray_text, size = geom_text_size) +\n  \n  # short line under column of numbers\n  geom_segment(aes(x = -20, y = 0.45,  xend = -0.8, yend = 0.45), color = gray_text) +\n  \n  # sum of N per level\n  geom_text(aes(x = -5, y = 0, label = N_level), hjust = 1, color = gray_text, size = geom_text_size)\n\n# combine the two plots\ncowplot::ggdraw() +\n  draw_plot(plot1, x = 0, y = 0, width = 0.8, height = 1) +\n  draw_plot(plot2, x = 0.82, y = 0, width = 0.18, height = 1)\n\n\n\n\n\nFigure 4: Popularity of inquiry type by grade-level band. Color indicates inquiries the grade levels have in common. Bars show relative number of articles by row.\n\n\n\n\nThe important feature of this design is that the majority of the percentages compared in the original text are visually compared in the figure. It also conveys all the textual information from the original data table.\nLike the missing entries in the original data table, missing inquiry types are easy to spot by the absence of a data marker. (In a strict sense, these are not missing data; the actual values are zero—as shown in the numerical columns of the side bar-charts.)\nWe can also see that the top three types of inquiry have the same rank order in all grade levels. The bar charts help us recognize the extent to which the top two inquiry types dominate the literature.\nAn added rhetorical advantage of this design to potential audiences such as pre-college teachers or creators of pre-college engineering materials is the snapshot it provides of the state of their area of interest.\nLastly, that the organization of the chart is visually more effective suggests that a reorganization of the argument could also be more effective. The primary organization of the chart is by grade-level band. If I were to align the logic of the argument with the logic of the display, then the text too would be organized by grade-level band. For example, I might start the discussion as follows:\n\n\n\n\n\n\nOrganizing the argument by grade level\n\n\n\nThe elementary school grade level had only three types of inquiry but the highest number of articles overall (N = 63). DBT is by far the largest portion of articles (75%) with UCD (21%) and ENS (4%) making up the rest.\nCompared to the elementary level, the middle school level had nearly as many articles (N = 51) and the same top three inquiry types. But with more inquiry types, the top three inquiries comprised smaller percentages of the whole: DBT (67%), UCD (18%), ENS (6%). Middle school is also the only level in which all six inquiry types are present.\nIn contrast, the high school level … etc. \n\n\nAligning visual rhetoric and verbal rhetoric in this fashion can be an important aid to a reader trying to follow the discussion. Moreover, this process can be an important aid to an authorial team in developing their message in the first place. The iterative interplay between constructing a chart and constructing an argument—between visual rhetoric and verbal rhetoric—is one of the most important aspects of data visualization."
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html",
    "title": "Survey data chart designs",
    "section": "",
    "text": "Summary\n\n\n\nI reconstruct three alternative chart designs for Likert-style survey data from a 2018 post by Lisa Charlotte Muth and Gregor Aisch and share the source data and my R code. Comparing the charts in light of their arguments, I agree that the 100% stacked-bar chart is the more effective of the three designs.\nFor a recent presentation, I needed to graph the results of a Likert-style survey.\nIn the past I tended to use the diverging stacked-bar design by Robbins and Heiberger (2014; 2011). Browsing for alternatives, I found the essays by Stephen Few (2016) and Lisa Charlotte Muth and Gregor Aisch (2018) to be well-reasoned and useful.\nIn this post, I reconstruct the three chart designs from Muth and Aisch with two goals in mind: to reproduce and comment on the comparisons they make using a data set of my choosing; and to share the source data and my R code.\nHere, however, I do not discuss the R code in any detail. In a companion post, I focus on R for preparing different forms of source data for the likert package and editing the results for publication-ready charts.\nI primarily use data.table, ggplot2, and likert R packages. An appealing feature of likert is its compatibility with data.table and ggplot2 functionality. Note that to reproduce this work, likert must be at least version 1.3.6.\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"likert\")\nlibrary(\"cowplot\")\n\n# chart design elements\nneutral_color &lt;- \"gray90\"\nmy_breaks &lt;- seq(-100, 100, 10)\nmy_vline  &lt;- geom_vline(xintercept = my_breaks, color = \"white\", size = 0.25)\nmy_hline  &lt;- geom_hline(yintercept = my_breaks, color = \"white\", size = 0.25)\n\n# ggplot() theme settings\nmy_theme_elements &lt;- theme(panel.background = element_blank(),\n                           legend.key.size = unit(4, \"mm\"),\n                           legend.title = element_blank(),\n                           axis.ticks = element_blank(), \n                           legend.justification = 0.5, \n                           legend.position = \"top\")\n\n# labeling vectors\nopinion_labels &lt;- c(\"Strongly Disagree\", \n                    \"Disagree\", \n                    \"Neutral\", \n                    \"Agree\", \n                    \"Strongly Agree\")\n\nquestion_labels &lt;- c(\"Beyond the content\", \n                     \"Analyze errors\", \n                     \"Provide facts\", \n                     \"Develop writing\", \n                     \"Independent learning\")\n\n# rename opinion columns\nsetnames_opinion_labels &lt;- function(x) {\n  setnames(x, \n           old = c(\"str_disagree\", \"disagree\", \"neutral\", \"agree\", \"str_agree\"), \n           new = opinion_labels, \n           skip_absent = TRUE)\n}"
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html#data",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html#data",
    "title": "Survey data chart designs",
    "section": "Data",
    "text": "Data\nThe practice data in my example are from an engineering education article by Ashanthi Maxworth (2021), selected because the data are compact and the survey includes a Neutral option. The table from the original article is shown below. There were 31 respondents.\n\n\n\n(Original Table 3) Percentage student responses for each question in the feedback form.\n\n\nSurvey data are most likely to be reported in one of three forms: summary percentages (as above), summary counts, or row-records. The likert() function accepts any of these forms as input. The practice data, in all three forms, are available in the blog data directory in CSV files, though for this post I will use the summary count form only.\nRead the prepared data file in summary count form.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-count.csv\")\n\n\n\n\n\n\n\nq_no\nstr_disagree\ndisagree\nneutral\nagree\nstr_agree\n\n\n\n\nQ1\n2\n0\n8\n12\n9\n\n\nQ2\n2\n2\n7\n14\n6\n\n\nQ3\n1\n1\n5\n9\n15\n\n\nQ4\n0\n2\n10\n12\n7\n\n\nQ5\n2\n0\n6\n11\n12\n\n\n\n\n\nI rename the first column Item, and the data frame is ready to input to the likert() function. Because my goal is comparing chart designs, I’m not interested in the specific survey questions, so I leave the question labels in their abbreviated form (Q1, Q2, …).\n\n\nR code\n# rename first column\nsetnames(dt, \"q_no\", \"Item\", skip_absent = TRUE)\n\n# examine the result\ndt[]\n\n\n     Item str_disagree disagree neutral agree str_agree\n   &lt;char&gt;        &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1:     Q1            2        0       8    12         9\n2:     Q2            2        2       7    14         6\n3:     Q3            1        1       5     9        15\n4:     Q4            0        2      10    12         7\n5:     Q5            2        0       6    11        12\n\n\nThe salient characteristics of the data frame are:\n\nOne row per question\nFirst column is named Item and contains the question labels\nRemaining columns are named for the opinion levels in increasing order left to right\nColumn values are the counts of respondents choosing that option\nThe sum of row counts is the number of respondents answering that question"
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html#diverging-stacked-bars",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html#diverging-stacked-bars",
    "title": "Survey data chart designs",
    "section": "Diverging stacked bars",
    "text": "Diverging stacked bars\nA defining feature of the divergent stacked-bar is that the Neutral segment is split half to the left and half to the right of the zero reference line. Also, because each row of the data table sums to 100%, each bar of the chart has a horizontal length of 100%, partitioned to show the component percentages.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# set scale limits to fill the data rectangle\nmy_limits &lt;- c(-25, 86)\n\n# recode the opinion options\nsetnames_opinion_labels(likert_list$results)\n\n# create the chart\nplot(likert_list, \n     centered = TRUE,              # diverging\n     include.center  = TRUE,       # neutral included\n     plot.percent.low     = FALSE,\n     plot.percent.neutral = FALSE,\n     plot.percent.high    = FALSE) +\n  \n  # additional ggplot components\n  scale_y_continuous(limits = my_limits, \n                     breaks = my_breaks, \n                     labels = abs(my_breaks)) +\n  my_theme_elements +\n  my_hline\n\n\n\n\n\nFigure 1: Diverging stacked-bar design\n\n\n\n\nThe top-down row order is by decreasing agreement totals (Agree + Strongly Agree). In contrast, by Heiberger and Robbins’ definition, the row position of Q2 and Q4 would be swapped so that the maximum endpoint monotonically decreases top to bottom.\nDescribing a diverging stacked bar chart, Robbins and Heiberger say,\n\nIt is difficult to compare lengths without a common baseline. In this situation, we are primarily interested in the total percent to the right or left of the zero line; the breakdown into strongly or not is of lesser interest so that the primary comparisons do have a common baseline of zero (Robbins & Heiberger, 2011, p. 1060).\n\nI agree—if we assume the Neutrals can treated as half positive and half negative. Muth and Aisch point out that we have no way of knowing that this is true. Because being truthful is a first principle of ethical data visualization, this assumption makes me uneasy. A lot depends on how the survey questions are worded and how the people surveyed interpret the Neutral response. In this specific case, I’m not certain the Neutrals can treated as half positive and half negative. Thus, the zero reference line does not establish a common baseline and we lose the ability to make effective visual comparisons.\nWe can recover the common baseline at zero by moving Neutral to a side chart of its own."
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html#neutral-on-the-side",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html#neutral-on-the-side",
    "title": "Survey data chart designs",
    "section": "Neutral on the side",
    "text": "Neutral on the side\nBy removing the Neutral responses, the zero reference line is a common baseline for visually comparing total agreement. We can see that the top-down row order is by decreasing agreement totals (Agree + Strongly Agree). And for a given row, we can visually compare total disagreement to total agreement.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# set scale limits to fill the data rectangle\nmy_limits &lt;- c(-13, 78)\n\n# recode the opinion options\nsetnames_opinion_labels(likert_list$results)\n\n# extract Neutrals for second chart\nlikert_list_neutral &lt;- likert_list$results[, .(Item, Neutral)]\n\n# delete Neutrals from likert list (removing neutral from legend)\nlikert_list$results[, Neutral := NULL]\nlikert_list$levels  &lt;- likert_list$levels[!likert_list$levels %in% \"neutral\"]\nlikert_list$nlevels &lt;- likert_list$nlevels - 1\n\n# create the chart\nplot1 &lt;- plot(likert_list, \n              centered = TRUE,               # diverging\n              plot.percent.low     = FALSE,\n              plot.percent.neutral = FALSE,\n              plot.percent.high    = FALSE) +\n  scale_y_continuous(limits = my_limits, \n                     breaks = my_breaks, \n                     labels = abs(my_breaks), \n                     expand = c(0, 0)) +\n  my_theme_elements +\n  my_hline\n\n# display\nplot1\n\n\n\n\n\nFigure 2: Diverging stacked bar, neutral omitted.\n\n\n\n\nNext I construct the second part of the chart (using ggplot2 functions only) to plot the Neutral responses alone.\n\n\nR code\n# use Neutral data frame from earlier\nlikert_list &lt;- likert_list_neutral\n\n# set scale limits to fill the data rectangle\nmy_limits &lt;- c(0, 33)\n\n# extract order of questions (factors) from previous chart object\nfactor_levels &lt;- levels(plot1$data$Item)\n\n# factors for ordering rows \nlikert_list[, Item := factor(Item, levels = factor_levels)]\n\n# assign a variable to fill by and create a legend\nlikert_list[, opinion := \"Neutral\"]\n\n# create the chart\nplot2 &lt;- ggplot(data = likert_list, mapping = aes(x = Neutral, y = Item, fill = opinion)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"\", y = \"\") +\n  scale_x_continuous(limits = my_limits, \n                     breaks = my_breaks, \n                     expand = c(0, 0)) +\n  scale_fill_manual(values = neutral_color) +\n  my_theme_elements +\n  my_vline\n  \n# display\nplot2\n\n\n\n\n\nFigure 3: Neutral responses only.\n\n\n\n\nI combine the two charts and adjust their proportions to make equal scale divisions the same length.\n\n\nR code\n# edit Neutral bar chart aesthetics before combining \nplot2 &lt;- plot2 +\n  theme(axis.text.y = element_blank(), \n        legend.justification = -0.25)\n\n# set plot proportions by trial and error until scales match\nwidth_1 &lt;- 0.71\nwidth_2 &lt;- 1 - width_1\n\n# combine plots \nggdraw() +\n  draw_plot(plot1, x = 0      , y = 0, width = width_1, height = 1) +\n  draw_plot(plot2, x = width_1, y = 0, width = width_2, height = 1)\n\n\n\n\n\nFigure 4: Diverging stacked bar chart with neutral on the side.\n\n\n\n\n\nDesigned in this way, differences between positive and negative results now stand out a bit more, the sum of Agree and Strongly Agree are easier to read, and the Neutral values are both easier to read and compare (Few, 2016).\n\nAs Muth and Aisch point out, this design gives a good idea of the “competition” between agreement and disagreement. In this case, across all questions more than 60% of the respondents agreed and in all but one instance fewer than 10% disagreed.\nIn addition, between 15-30% responded Neutral. We don’t know if that means “I don’t know” or “I have no opinion” or “Sometimes I agree and sometimes I don’t”or “I’m tired of answering Likert-style surveys” or something else—which is a very good reason to graph Neutral on the side.\nI like this design."
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html#stacked-bars",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html#stacked-bars",
    "title": "Survey data chart designs",
    "section": "100% stacked bars",
    "text": "100% stacked bars\nThe final of the three designs is the 100% stacked bar—which until now I have not considered as effective as diverging stacked-bars at conveying survey results. What makes the difference today is Muth and Aisch’s suggestion to add a secondary scale along the top of the chart—a simple and elegant contribution.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# set scale limits\nmy_limits &lt;- c(0, 100)\n\n# recode the opinion options\nsetnames_opinion_labels(likert_list$results)\n\n# create the chart\nplot(likert_list, \n     centered = FALSE,              # 100% stacked bars\n     include.center  = TRUE,        # include neutral\n     plot.percent.low     = FALSE,\n     plot.percent.neutral = FALSE,\n     plot.percent.high    = FALSE) +\n  scale_y_continuous(limits = my_limits, \n                     breaks = my_breaks, \n                     sec.axis = sec_axis( # second scale\n                       trans = function(z) z - 100, \n                       breaks = my_breaks, \n                       labels = as.character(abs(my_breaks)))) +\n  my_theme_elements +\n  my_hline\n\n\n\n\n\nFigure 5: 100% stacked bar chart.\n\n\n\n\nWith the right boundary as a baseline, I read the top scale for agreement percentages; with the left boundary as a baseline, I read the bottom scale for disagreement percentages. We can easily quantify a comparison between strong opinions (outer left and outer right) or between total agreement and total disagreement. Neither of the divergent stacked-bar charts allow this level of direct visual access (though the bar segments could be directly labeled with their respective percentages).\nLike the previous chart, the rationale for ordering the rows is clear—the agreement total monotonically increase from bottom to top. Of lesser importance, this design also immediately communicates that the bar segments are parts of a whole—that each bar represents 100% of responses.\nThe only disadvantage of this chart compared to the previous one is that the relative proportions of the Neutral responses are harder to compare. Neutrals are important data but the main story is usually a comparison between people who have opinions—that is, the bar segments to the left and right of the Neutral center.\nI have to agree with Muth and Aisch—this is an effective design."
  },
  {
    "objectID": "posts/2022-02-12-survey-data-chart-designs/index.html#back-to-the-story",
    "href": "posts/2022-02-12-survey-data-chart-designs/index.html#back-to-the-story",
    "title": "Survey data chart designs",
    "section": "Back to the story",
    "text": "Back to the story\nWere I to prepare this chart to accompany the original article, I might label the rows with shortened forms of the questions and cite the full questions in the text or in the data table. (Alternatively, likert() does have an argument to help manage longer question text.)\n\n\nR code\ndt_story &lt;- copy(dt)\n\n# recode the opinion options\nsetnames_opinion_labels(dt_story)\n\n# recode the question labels\ndt_story[, Item := question_labels]\n\n# create the likert list\nlikert_list &lt;- likert(summary = dt_story)\n\n# set scale limits\nmy_limits  &lt;- c(0, 100)\n\n# create the chart\nplot(likert_list, \n     centered = FALSE, \n     include.center  = TRUE, \n     plot.percent.low     = FALSE,\n     plot.percent.neutral = FALSE,\n     plot.percent.high    = FALSE) +\n  scale_y_continuous(limits = my_limits, \n                     breaks = my_breaks, \n                     sec.axis = sec_axis( # second scale\n                       trans = function(z) z - 100, \n                       breaks = my_breaks, \n                       labels = as.character(abs(my_breaks)))) +\n  my_theme_elements +\n  my_hline\n\n\n\n\n\nFigure 6: Editing the legend key and question labels for readability.\n\n\n\n\nAs an example of the results discussion in the article, the “provide facts” paragraph states,\n\nIn the third feedback question, 24 out of 31 students (77.4%) agreed that the case study motivated them to provide facts such as calculations and simulations to support their answers. Unlike a typical textbook problem where there is a definite answer, making an argument in a case study requires thinking beyond the material delivered in class. Therefore, the use of additional calculations and especially simulations were needed to support the argument (Maxworth, 2021).\n\nThis is a perfectly straightforward description of the result and the chart supports the argument visually. In terms of the larger narrative, however, the chart provides a rationale for revising the narrative framework—instead of discussing the results in question order (Q1, Q2, …), discuss the results in order of level of agreement (highest to lowest), supported visually by the row order in the chart (top to bottom).\nI think the chart provides evidence for an additional assertion: that the preponderance of responses are positive,\n\nBetween 61-77% of responses were positive over the full range of survey statements.\nThe largest negative response was to the Analyze errors/misconceptions assertion at 13% (4 out of 31 responses); all other negatives were at 6.5% (2 of 31).\n\nGiven the overall positive response, the small number of negatives may have resulted from a mismatch between a student and the case they selected. As the author states in their conclusion,\n\nIn future implementations, an effort is needed to balance the choice distribution of cases. This can be done by designing the cases with the same level of difficulty, familiarity, and applicability.\n\nSo while a chart was not necessary to support the author’s points, I think it would have added a small but important summary point that the case study approach was successful and warranted the further development outlined in the concluding section."
  },
  {
    "objectID": "posts/2022-02-13-survey-data-io/index.html",
    "href": "posts/2022-02-13-survey-data-io/index.html",
    "title": "Survey data I/O with likert",
    "section": "",
    "text": "Summary\n\n\n\nGiven Likert-style survey data in one of three common forms, I shape the data to suit the input requirements of the likert R package and use the output to create 100% stacked-bar charts. In each case, I illustrate two routine revision tasks: editing the question labels on the bars and editing the opinion levels in the legend.\nThis post is a tutorial on how to prepare different forms of Likert-style survey data for the R likert package and using its output to create 100% stacked-bar charts. I focus on preparing the data for likert() input and editing its output for the final chart. For exploring the package functionality more fully, I recommend the tutorials by Laura Mudge (2019) and Jake Chanenson (2021).\nIn a companion post I develop the R script for constructing the 100% stacked-bar chart and discuss the rationale for selecting it as a more effective design for Likert-style survey data.\nI use data.table, ggplot2, and likert R packages. An appealing feature of likert is its compatibility with data.table and ggplot2 functionality. Note that to reproduce this work, likert must be at least version 1.3.6 (currently the development version).\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"likert\")\n\n# function based on likert.plot to construct a 100% stacked bar chart \nmy_breaks &lt;- seq(-100, 100, 10)\nlikert_100_pct_bar &lt;- function(likert_list) {\n  plot(likert_list, \n       plot.percent.neutral = FALSE,\n       plot.percent.high = FALSE,\n       plot.percent.low = FALSE,\n       neutral.color = \"grey90\", \n       include.center = TRUE, \n       centered = FALSE) +\n    geom_hline(yintercept = my_breaks, color = \"white\", size = 0.25) +\n    scale_y_continuous(limits = c(0, 100), \n                       breaks = my_breaks, \n                       sec.axis = sec_axis( # second scale\n                         trans = function(z) z - 100, \n                         breaks = my_breaks, \n                         labels = as.character(abs(my_breaks)))) +\n    theme(panel.background = element_blank(),\n          legend.key.size = unit(4, \"mm\"),\n          legend.title = element_blank(),\n          axis.ticks = element_blank(), \n          legend.justification = 0.5, \n          legend.position = \"top\")\n}\n\n# labeling vectors\nopinion_labels &lt;- c(\"Strongly Disagree\", \n                    \"Disagree\", \n                    \"Neutral\", \n                    \"Agree\", \n                    \"Strongly Agree\")\n\nquestion_labels &lt;- c(\"Beyond the content\", \n                     \"Analyze errors\", \n                     \"Provide facts\", \n                     \"Develop writing\", \n                     \"Independent learning\")\n\n# functions for renaming columns\nsetnames_Item &lt;- function(x) {\n  setnames(x, old = \"q_no\", new = \"Item\", skip_absent = TRUE)\n}\n\nsetnames_opinion_labels &lt;- function(x) {\n  setnames(x, \n           old = c(\"str_disagree\", \n                   \"disagree\", \n                   \"neutral\", \n                   \"agree\", \n                   \"str_agree\"), \n           new = opinion_labels, \n           skip_absent = TRUE)\n}"
  },
  {
    "objectID": "posts/2022-02-13-survey-data-io/index.html#data",
    "href": "posts/2022-02-13-survey-data-io/index.html#data",
    "title": "Survey data I/O with likert",
    "section": "Data",
    "text": "Data\nThe practice data in my example are from an engineering education article by Ashanthi Maxworth (2021), selected because the data are compact and the survey includes a Neutral option. The table from the original article is shown below. There were 31 respondents.\n\n\n\n(Original Table 3) Percentage student responses for each question in the feedback form.\n\n\nSurvey data are most likely to be reported in one of three forms: summary percentages (as above), summary counts, or row-records. The likert() function accepts any of these forms as input. The practice data, in all three forms, are available in the blog data directory as CSV files.\n\nsummary counts\n\nsummary percentages\n\nrow-records"
  },
  {
    "objectID": "posts/2022-02-13-survey-data-io/index.html#summary-counts",
    "href": "posts/2022-02-13-survey-data-io/index.html#summary-counts",
    "title": "Survey data I/O with likert",
    "section": "Summary counts",
    "text": "Summary counts\nRead the prepared data file in summary count form.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-count.csv\")\n\n\n\n\n\n\n\nq_no\nstr_disagree\ndisagree\nneutral\nagree\nstr_agree\n\n\n\n\nQ1\n2\n0\n8\n12\n9\n\n\nQ2\n2\n2\n7\n14\n6\n\n\nQ3\n1\n1\n5\n9\n15\n\n\nQ4\n0\n2\n10\n12\n7\n\n\nQ5\n2\n0\n6\n11\n12\n\n\n\n\n\n\nlikert() input\nI rename the first column Item for consistency with the likert() function.\n\n\nR code\n# rename first column\nsetnames_Item(dt)\n\n# examine the result\ndt[]\n\n\n     Item str_disagree disagree neutral agree str_agree\n   &lt;char&gt;        &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;     &lt;int&gt;\n1:     Q1            2        0       8    12         9\n2:     Q2            2        2       7    14         6\n3:     Q3            1        1       5     9        15\n4:     Q4            0        2      10    12         7\n5:     Q5            2        0       6    11        12\n\n\nThe likert() function accepts input data frames having this structure. The salient characteristics are:\n\none row per question\nfirst column is named Item and contains the question labels\nremaining columns are named for the opinion levels in increasing order left to right\ncolumn values are the counts of respondents choosing that option\nthe sum of row counts is the number of respondents answering that question\n\n\n\nlikert() output\nTo operate on this data frame, we assign it to the summary argument of the likert() function. The result is a list of various statistics about the Likert-style data. Note that the results output preserves the data.table structure of the input.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# examine its structure\nstr(likert_list)\n\n\nList of 5\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item        : chr [1:5] \"Q1\" \"Q2\" \"Q3\" \"Q4\" ...\n  ..$ str_disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ disagree    : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ neutral     : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ agree       : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ str_agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   : NULL\n $ grouping: NULL\n $ nlevels : num 5\n $ levels  : chr [1:5] \"str_disagree\" \"disagree\" \"neutral\" \"agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nThe components of the list are:\n\nresults\n\nData frame. Percentage of responses by question, opinion level, and group.\n\nitems\n\nData frame. Copy of original row-record input (NULL in this example).\n\ngrouping\n\nCopy of original grouping vector that subsets results (NULL in this example).\n\nnlevels\n\nInteger. Number of opinion levels used in the calculations.\n\nlevels\n\nCharacter. Ordered vector of opinion level labels.\n\n\n\n\n\nBasic chart\nTo use this list to create a chart, we assign it as the first argument of the plot() function.\n\n\nR code\n# create the basic chart (default digits = 0 throws an error)\nplot(likert_list, digits = 1)\n\n\n\n\n\nFigure 1: Default chart design from likert().\n\n\n\n\n\n\n100% stacked bar chart\nThe same list can be used to create a 100% stacked-bar chart by assigning it as the first argument of likert_100_pct_bar()—a function (defined at the top of the post) that wraps likert.plot and sets the likert arguments and ggplot2 functions that produce my preferred design.\n\n\nR code\n# customize the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 2: Summary-count data, 100% stacked-bar chart.\n\n\n\n\n\n\nLegend key\nThe legend key is edited via the column names of likert_list$results. Viewing its column names,\n\n\nR code\nnames(likert_list$results)\n\n\n[1] \"Item\"         \"str_disagree\" \"disagree\"     \"neutral\"      \"agree\"       \n[6] \"str_agree\"   \n\n\nUsing a vector of opinion labels defined at the top of the post, I rename the opinion columns of the data frame.\n\n\nR code\n# recode the opinion options \nsetnames_opinion_labels(likert_list$results)\n\n# examine the result\nstr(likert_list)\n\n\nList of 5\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Q1\" \"Q2\" \"Q3\" \"Q4\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   : NULL\n $ grouping: NULL\n $ nlevels : num 5\n $ levels  : chr [1:5] \"str_disagree\" \"disagree\" \"neutral\" \"agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nThe change can be seen in the structure above and in the revised figure.\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 3: Editing the legend key.\n\n\n\n\n\n\nQuestion labels\nThe question labels are edited via the values in the Item column of likert_list$results. Viewing the first column in vector form,\n\n\nR code\nlikert_list$results[[\"Item\"]]\n\n\n[1] \"Q1\" \"Q2\" \"Q3\" \"Q4\" \"Q5\"\n\n\nUsing a vector of question labels defined at the top of the post, I substitute them for the values in the original Item column.\n\n\nR code\n# recode the question labels\nlikert_list$results[, Item := question_labels]\n\n# examine the result\nstr(likert_list)\n\n\nList of 5\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Beyond the content\" \"Analyze errors\" \"Provide facts\" \"Develop writing\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   : NULL\n $ grouping: NULL\n $ nlevels : num 5\n $ levels  : chr [1:5] \"str_disagree\" \"disagree\" \"neutral\" \"agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nAgain, the change is seen in the structure above and in the revised figure.\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 4: Editing the question labels.\n\n\n\n\nThis approach is somewhat ad-hoc, but works as long as you are careful to write the substitutions in the correct order. If I were programming these steps, I would create additional tables (as in a database) and join the substitutions by clearly assigned key variables.\n\n\nOr edit the labels first\nAlternatively one can produce the same result by editing the opinion labels and question labels of the initial data frame before submitting it to likert(). The row and column structure reflects the changes.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-count.csv\")\n\n# rename columns\nsetnames_Item(dt)\nsetnames_opinion_labels(dt)\n\n# recode the question labels\ndt[, Item := question_labels]\n\n# examine the result\ndt[]\n\n\n                   Item Strongly Disagree Disagree Neutral Agree Strongly Agree\n                 &lt;char&gt;             &lt;int&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt;          &lt;int&gt;\n1:   Beyond the content                 2        0       8    12              9\n2:       Analyze errors                 2        2       7    14              6\n3:        Provide facts                 1        1       5     9             15\n4:      Develop writing                 0        2      10    12              7\n5: Independent learning                 2        0       6    11             12\n\n\nThe likert list that results is nearly identical to the previous version except the levels vector uses the new opinion labels.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# examine the result\nstr(likert_list)\n\n\nList of 5\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Beyond the content\" \"Analyze errors\" \"Provide facts\" \"Develop writing\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   : NULL\n $ grouping: NULL\n $ nlevels : num 5\n $ levels  : chr [1:5] \"Strongly Disagree\" \"Disagree\" \"Neutral\" \"Agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 5: Summary-count data, editing labels before invoking likert()."
  },
  {
    "objectID": "posts/2022-02-13-survey-data-io/index.html#summary-percentages",
    "href": "posts/2022-02-13-survey-data-io/index.html#summary-percentages",
    "title": "Survey data I/O with likert",
    "section": "Summary percentages",
    "text": "Summary percentages\nRead the prepared data file in summary percentage form. The percentages are directly from the table in the source article. Like before, I rename the first column Item for consistency with the likert() function.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-percent.csv\")\n\n# rename first column\nsetnames_Item(dt)\n\n\n\n\n\n\n\nItem\nstr_disagree\ndisagree\nneutral\nagree\nstr_agree\n\n\n\n\nQ1\n6.5\n0.0\n25.8\n38.7\n29.0\n\n\nQ2\n6.5\n6.5\n22.6\n45.2\n19.4\n\n\nQ3\n3.2\n3.2\n16.1\n29.0\n48.4\n\n\nQ4\n0.0\n6.5\n32.3\n38.7\n22.6\n\n\nQ5\n6.5\n0.0\n19.4\n35.5\n38.7\n\n\n\n\n\n\nOption 1: Convert percentages to counts\nThis option is the most direct approach, assuming we know the number of respondents to each question. In this example we do (though this is not always the case). In this case study we have 31 respondents and all respondents replied to all the questions.\n\n\nR code\n# number of respondents in this example\nN_respondents &lt;- 31\n\n# identify the numeric columns\nsel_cols &lt;- names(dt)[sapply(dt, is.numeric)]\n\n# convert percentages to integer counts\ndt[, c(sel_cols) := lapply(.SD, function(x) round(N_respondents * x/100, 0)), .SDcols = sel_cols]\n\n\n\n\n\n\n\nItem\nstr_disagree\ndisagree\nneutral\nagree\nstr_agree\n\n\n\n\nQ1\n2\n0\n8\n12\n9\n\n\nQ2\n2\n2\n7\n14\n6\n\n\nQ3\n1\n1\n5\n9\n15\n\n\nQ4\n0\n2\n10\n12\n7\n\n\nQ5\n2\n0\n6\n11\n12\n\n\n\n\n\nThis data structure is identical to the one we worked with in the previous section, so we know how to work with it.\n\n\nOption 2: Use percentages as-is\nThis option might be necessary if we do not know the number of respondents replying to each question. Start by reading the data file and again rename the first column Item for consistency with the likert() function.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-percent.csv\")\n\n# rename first column\nsetnames_Item(dt)\n\n\n\n\n\n\n\nItem\nstr_disagree\ndisagree\nneutral\nagree\nstr_agree\n\n\n\n\nQ1\n6.5\n0.0\n25.8\n38.7\n29.0\n\n\nQ2\n6.5\n6.5\n22.6\n45.2\n19.4\n\n\nQ3\n3.2\n3.2\n16.1\n29.0\n48.4\n\n\nQ4\n0.0\n6.5\n32.3\n38.7\n22.6\n\n\nQ5\n6.5\n0.0\n19.4\n35.5\n38.7\n\n\n\n\n\nWith one row per question, the row percentages should sum to 100%. They do, but with an error due to rounding in the reported percentages.\n\n\nR code\n# check row totals of numeric columns\nsel_cols &lt;- names(dt)[sapply(dt, is.numeric)]\nrow_sum  &lt;- rowSums(dt[, .SD, .SDcols = sel_cols])\n\n# examine result\ndt[, row_total := row_sum]\ndt[, rounding_error := row_sum - 100]\ndt[, .(Item, row_total, rounding_error)]\n\n\n     Item row_total rounding_error\n   &lt;char&gt;     &lt;num&gt;          &lt;num&gt;\n1:     Q1     100.0            0.0\n2:     Q2     100.2            0.2\n3:     Q3      99.9           -0.1\n4:     Q4     100.1            0.1\n5:     Q5     100.1            0.1\n\n\nIf we ignore the rounding error, it can introduce small but noticeable errors in the bar lengths in the chart. A simple remediation is to subtract the small errors from the neutral columns so that all rows sum to 100% exactly. The adjusted Neutrals are shown below.\n\n\nR code\n# subtract error from neutral\ndt[, adjusted_neutral := neutral - rounding_error]\n\n# examine the result\ndt[, .(Item, neutral, rounding_error, adjusted_neutral)]\n\n\n     Item neutral rounding_error adjusted_neutral\n   &lt;char&gt;   &lt;num&gt;          &lt;num&gt;            &lt;num&gt;\n1:     Q1    25.8            0.0             25.8\n2:     Q2    22.6            0.2             22.4\n3:     Q3    16.1           -0.1             16.2\n4:     Q4    32.3            0.1             32.2\n5:     Q5    19.4            0.1             19.3\n\n\n\n\nlikert() input\nReplacing neutral with the adjusted neutral and deleting the temporary information columns yields the data structure I need for the summary percentage form:\n\n\nR code\n# adjust neutral\ndt[, neutral := adjusted_neutral]\n\n# delete temporary information columns\ndt[, c(\"row_total\", \"rounding_error\", \"adjusted_neutral\") := NULL]\n\n# examine the result\ndt[]\n\n\n     Item str_disagree disagree neutral agree str_agree\n   &lt;char&gt;        &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;num&gt;     &lt;num&gt;\n1:     Q1          6.5      0.0    25.8  38.7      29.0\n2:     Q2          6.5      6.5    22.4  45.2      19.4\n3:     Q3          3.2      3.2    16.2  29.0      48.4\n4:     Q4          0.0      6.5    32.2  38.7      22.6\n5:     Q5          6.5      0.0    19.3  35.5      38.7\n\n\nData structure:\n\none row per question\nfirst column is named Item and contains the question labels\nremaining columns are named for the opinion levels in increasing order left to right\ncolumn values are the percentages of respondents choosing that option\nthe sum of row percentages is exactly 100%\n\nTo prepare the data frame for graphing, I use the “edit the labels first” approach described earlier.\n\n\nR code\n# recode the opinion options\nsetnames_opinion_labels(dt)\n\n# recode the question labels\ndt[, Item := question_labels]\n\n# examine the result\ndt[]\n\n\n                   Item Strongly Disagree Disagree Neutral Agree Strongly Agree\n                 &lt;char&gt;             &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;num&gt;          &lt;num&gt;\n1:   Beyond the content               6.5      0.0    25.8  38.7           29.0\n2:       Analyze errors               6.5      6.5    22.4  45.2           19.4\n3:        Provide facts               3.2      3.2    16.2  29.0           48.4\n4:      Develop writing               0.0      6.5    32.2  38.7           22.6\n5: Independent learning               6.5      0.0    19.3  35.5           38.7\n\n\n\n\nlikert() output\nTo operate on this data frame, we again use the summary argument of likert(). The result is a list similar to that produced when we operated on summary counts and the same familiar chart.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(summary = dt)\n\n# examine its structure\nstr(likert_list)\n\n\nList of 5\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Beyond the content\" \"Analyze errors\" \"Provide facts\" \"Develop writing\" ...\n  ..$ Strongly Disagree: num [1:5] 6.5 6.5 3.2 0 6.5\n  ..$ Disagree         : num [1:5] 0 6.5 3.2 6.5 0\n  ..$ Neutral          : num [1:5] 25.8 22.4 16.2 32.2 19.3\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   : NULL\n $ grouping: NULL\n $ nlevels : num 5\n $ levels  : chr [1:5] \"Strongly Disagree\" \"Disagree\" \"Neutral\" \"Agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nR code\n# 100% stacked bar chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 6: Summary-percentage data, 100% stacked-bar chart."
  },
  {
    "objectID": "posts/2022-02-13-survey-data-io/index.html#row-records",
    "href": "posts/2022-02-13-survey-data-io/index.html#row-records",
    "title": "Survey data I/O with likert",
    "section": "Row records",
    "text": "Row records\nIn row-record form, everything we want to know about an individual is in one row, that is, a row-record for that individual. Thus the number of rows equals the number of respondents.\nI made up a practice data set in row-record form with 31 rows and 6 columns. These are fictitious data I designed specifically to have the same summary characteristics as the published summary data used earlier.\nRead the prepared data file in row-record form and view the data frame.\n\n\nR code\n# read observed data\ndt &lt;- fread(\"data/case-study-2021-row-record.csv\")\n\n# examine the result\ndt[]\n\n\n      obs    Q1    Q2    Q3    Q4    Q5\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     1     3     4     3     4     4\n 2:     2     5     1     5     3     5\n 3:     3     5     5     4     5     4\n 4:     4     3     4     5     4     5\n 5:     5     4     4     5     2     4\n 6:     6     4     3     5     3     4\n---                                    \n26:    26     5     5     5     4     5\n27:    27     5     2     3     4     1\n28:    28     3     4     5     3     5\n29:    29     3     3     4     3     4\n30:    30     4     4     5     3     1\n31:    31     4     4     5     5     5\n\n\nThe first column is a fictitious respondent ID. The remaining columns represent responses to the survey questions. For basic charts like those shown here, all columns should be question responses, so I delete the ID. Though I don’t cover it here, additional non-question columns are allowed for grouping the results. See, for example, (Mudge, 2019).\n\n\nR code\n# delete the ID column\ndt[, obs := NULL]\n\n# examine the result\ndt[]\n\n\n       Q1    Q2    Q3    Q4    Q5\n    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1:     3     4     3     4     4\n 2:     5     1     5     3     5\n 3:     5     5     4     5     4\n 4:     3     4     5     4     5\n 5:     4     4     5     2     4\n 6:     4     3     5     3     4\n---                              \n26:     5     5     5     4     5\n27:     5     2     3     4     1\n28:     3     4     5     3     5\n29:     3     3     4     3     4\n30:     4     4     5     3     1\n31:     4     4     5     5     5\n\n\n\nlikert() input\nFor the likert() function to accept data in this form, all question response columns must be factors with identical sets of levels. Reformatting the columns and checking the structure yields,\n\n\nR code\n# reformat columns as factors\nsel_cols &lt;- names(dt)\ndt[, c(sel_cols) := lapply(.SD, function(x) factor(x, levels = 1:5)), .SDcols = sel_cols]\n\n# examine the result\ndt[]\n\n\n        Q1     Q2     Q3     Q4     Q5\n    &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt;\n 1:      3      4      3      4      4\n 2:      5      1      5      3      5\n 3:      5      5      4      5      4\n 4:      3      4      5      4      5\n 5:      4      4      5      2      4\n 6:      4      3      5      3      4\n---                                   \n26:      5      5      5      4      5\n27:      5      2      3      4      1\n28:      3      4      5      3      5\n29:      3      3      4      3      4\n30:      4      4      5      3      1\n31:      4      4      5      5      5\n\n\nInput data structure\n\nOne row per respondent. The number of rows equals the number of respondents.\nOne column per question. The column name is the question label. The number of columns equals the number of survey questions.\n\nEach column is a factor with an identical set of levels. The number of levels equals the number of answer options in the survey.\nColumn values are the encoded opinions of the respondent: 1 (Strongly Disagree), 2 (Disagree), 3 (Neutral), etc.\n\n\n\nlikert() output\nTo operate on a row-record data frame, we assign it to the items argument of the likert() function. The result is again a list.\nHowever, unlike the previous output lists, the data.table structure of the input has not been preserved. I use data.table syntax in subsequent operations, so I convert both results and items to data.tables.\n\n\nR code\n# create likert list \nlikert_list &lt;- likert(items = dt)\n\n# convert output data frames to data.tables\nsetDT(likert_list$results)\nsetDT(likert_list$items)\n\n# examine the result\nstr(likert_list)\n\n\nList of 6\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item: chr [1:5] \"Q1\" \"Q2\" \"Q3\" \"Q4\" ...\n  ..$ 1   : num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ 2   : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ 3   : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ 4   : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ 5   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   :Classes 'data.table' and 'data.frame':  31 obs. of  5 variables:\n  ..$ Q1: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 5 3 4 4 3 1 4 5 ...\n  ..$ Q2: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 1 5 4 4 3 2 5 5 4 ...\n  ..$ Q3: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 4 5 5 5 4 4 4 5 ...\n  ..$ Q4: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 3 5 4 2 3 4 3 5 5 ...\n  ..$ Q5: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 4 5 4 4 4 3 4 3 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ grouping: NULL\n $ factors : NULL\n $ nlevels : int 5\n $ levels  : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nThe components of the list are:\n\nresults\n\nData frame. Percentage of responses by question, opinion level, and group.\n\nitems\n\nData frame. Copy of original row-record input.\n\ngrouping\n\nCopy of original grouping vector that subsets results (NULL in this example).\n\nfactors\n\nCopy of original vector matching columns to factors (NULL in this example).\n\nnlevels\n\nInteger. Number of opinion levels used in the calculations.\n\nlevels\n\nCharacter. Ordered vector of opinion level labels.\n\n\n\n\nDraft chart\nWith row-record data, the plot function requires both results and items from the output list. The chart is familiar, but the opinion labels are now the integers used to encode the survey results.\n\n\nR code\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 7: Row-record data, 100% stacked-bar chart.\n\n\n\n\n\n\nLegend key\nAs before, the legend key is edited via the column names of likert_list$results. Note the corresponding changes in the likert list and chart.\n\n\nR code\n# recode the opinion options\nsetnames(likert_list$results, \n         old = as.character(1:5), \n         new = opinion_labels, \n         skip_absent = TRUE)\n\n# examine the result\nstr(likert_list)\n\n\nList of 6\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Q1\" \"Q2\" \"Q3\" \"Q4\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   :Classes 'data.table' and 'data.frame':  31 obs. of  5 variables:\n  ..$ Q1: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 5 3 4 4 3 1 4 5 ...\n  ..$ Q2: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 1 5 4 4 3 2 5 5 4 ...\n  ..$ Q3: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 4 5 5 5 4 4 4 5 ...\n  ..$ Q4: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 3 5 4 2 3 4 3 5 5 ...\n  ..$ Q5: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 4 5 4 4 4 3 4 3 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ grouping: NULL\n $ factors : NULL\n $ nlevels : int 5\n $ levels  : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 8: Row-record data, editing the legend key.\n\n\n\n\n\n\nQuestion labels\nWith row-record data, both results and items data frames must be revised to edit the question labels. Note the corresponding changes in the likert list and chart.\n\n\nR code\n# recode Item column of $results\nlikert_list$results[, Item := question_labels]\n\n# recode column names of $items\nsetnames(likert_list$items, \n         old = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"), \n         new = question_labels, \n         skip_absent = TRUE)\n\n# examine the result\nstr(likert_list)\n\n\nList of 6\n $ results :Classes 'data.table' and 'data.frame':  5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Beyond the content\" \"Analyze errors\" \"Provide facts\" \"Develop writing\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ items   :Classes 'data.table' and 'data.frame':  31 obs. of  5 variables:\n  ..$ Beyond the content  : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 5 3 4 4 3 1 4 5 ...\n  ..$ Analyze errors      : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 1 5 4 4 3 2 5 5 4 ...\n  ..$ Provide facts       : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 3 5 4 5 5 5 4 4 4 5 ...\n  ..$ Develop writing     : Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 3 5 4 2 3 4 3 5 5 ...\n  ..$ Independent learning: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 5 4 5 4 4 4 3 4 3 ...\n  ..- attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n $ grouping: NULL\n $ factors : NULL\n $ nlevels : int 5\n $ levels  : chr [1:5] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 9: Row-record data, editing the question labels.\n\n\n\n\n\n\nOr edit the labels first\nAs before, we have an alternative approach: one can produce the same result by editing the opinion labels and question labels of the data frame before submitting it to likert(). Question labels are substituted for the column names. Opinion levels (as text) are substituted for the encoded integers, i.e., 1 = Strongly Disagree through 5 = Strongly Agree.\nTo illustrate, I start with a fresh row-record data set.\n\n\nR code\n# read prepared data\ndt &lt;- fread(\"data/case-study-2021-row-record.csv\")\n\n# delete the ID column\ndt &lt;- subset(dt, select = -c(obs))\n\n# recode the question labels in the column names\nsetnames(dt, \n         old = c(\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"), \n         new = question_labels, \n         skip_absent = TRUE)\n\n# recode integer values with opinion options  \nsel_cols &lt;- names(dt)\ndt[, (sel_cols) := lapply(.SD, function(x) fcase(\n  x == 1, opinion_labels[1],\n  x == 2, opinion_labels[2],\n  x == 3, opinion_labels[3],\n  x == 4, opinion_labels[4],\n  x == 5, opinion_labels[5])),\n  .SDcols = sel_cols]\n\n# convert columns to factors  \ndt &lt;- dt[, lapply(.SD, function(x) factor(x, levels = opinion_labels)), .SDcols = sel_cols]\n\n# examine the result\ndt[]\n\n\n    Beyond the content    Analyze errors  Provide facts Develop writing\n                &lt;fctr&gt;            &lt;fctr&gt;         &lt;fctr&gt;          &lt;fctr&gt;\n 1:            Neutral             Agree        Neutral           Agree\n 2:     Strongly Agree Strongly Disagree Strongly Agree         Neutral\n 3:     Strongly Agree    Strongly Agree          Agree  Strongly Agree\n 4:            Neutral             Agree Strongly Agree           Agree\n 5:              Agree             Agree Strongly Agree        Disagree\n 6:              Agree           Neutral Strongly Agree         Neutral\n---                                                                    \n26:     Strongly Agree    Strongly Agree Strongly Agree           Agree\n27:     Strongly Agree          Disagree        Neutral           Agree\n28:            Neutral             Agree Strongly Agree         Neutral\n29:            Neutral           Neutral          Agree         Neutral\n30:              Agree             Agree Strongly Agree         Neutral\n31:              Agree             Agree Strongly Agree  Strongly Agree\n    Independent learning\n                  &lt;fctr&gt;\n 1:                Agree\n 2:       Strongly Agree\n 3:                Agree\n 4:       Strongly Agree\n 5:                Agree\n 6:                Agree\n---                     \n26:       Strongly Agree\n27:    Strongly Disagree\n28:       Strongly Agree\n29:                Agree\n30:    Strongly Disagree\n31:       Strongly Agree\n\n\nInput to likert() produces the familiar chart.\n\n\nR code\n# create the likert list\nlikert_list &lt;- likert(items = dt)\n\n# examine the result\nstr(likert_list)\n\n\nList of 6\n $ results :'data.frame':   5 obs. of  6 variables:\n  ..$ Item             : chr [1:5] \"Beyond the content\" \"Analyze errors\" \"Provide facts\" \"Develop writing\" ...\n  ..$ Strongly Disagree: num [1:5] 6.45 6.45 3.23 0 6.45\n  ..$ Disagree         : num [1:5] 0 6.45 3.23 6.45 0\n  ..$ Neutral          : num [1:5] 25.8 22.6 16.1 32.3 19.4\n  ..$ Agree            : num [1:5] 38.7 45.2 29 38.7 35.5\n  ..$ Strongly Agree   : num [1:5] 29 19.4 48.4 22.6 38.7\n $ items   :'data.frame':   31 obs. of  5 variables:\n  ..$ Beyond the content  : Factor w/ 5 levels \"Strongly Disagree\",..: 3 5 5 3 4 4 3 1 4 5 ...\n  ..$ Analyze errors      : Factor w/ 5 levels \"Strongly Disagree\",..: 4 1 5 4 4 3 2 5 5 4 ...\n  ..$ Provide facts       : Factor w/ 5 levels \"Strongly Disagree\",..: 3 5 4 5 5 5 4 4 4 5 ...\n  ..$ Develop writing     : Factor w/ 5 levels \"Strongly Disagree\",..: 4 3 5 4 2 3 4 3 5 5 ...\n  ..$ Independent learning: Factor w/ 5 levels \"Strongly Disagree\",..: 4 5 4 5 4 4 4 3 4 3 ...\n $ grouping: NULL\n $ factors : NULL\n $ nlevels : int 5\n $ levels  : chr [1:5] \"Strongly Disagree\" \"Disagree\" \"Neutral\" \"Agree\" ...\n - attr(*, \"class\")= chr \"likert\"\n\n\nR code\n# create the chart\nlikert_100_pct_bar(likert_list)\n\n\n\n\n\nFigure 10: Row-record data, editing labels before invoking likert().\n\n\n\n\n\n\nData table\nThe results component can also be used to construct a summary data table.\n\n\nR code\nlikert_list$results\n\n\n                  Item Strongly Disagree Disagree  Neutral    Agree\n1   Beyond the content          6.451613 0.000000 25.80645 38.70968\n2       Analyze errors          6.451613 6.451613 22.58065 45.16129\n3        Provide facts          3.225806 3.225806 16.12903 29.03226\n4      Develop writing          0.000000 6.451613 32.25806 38.70968\n5 Independent learning          6.451613 0.000000 19.35484 35.48387\n  Strongly Agree\n1       29.03226\n2       19.35484\n3       48.38710\n4       22.58065\n5       38.70968\n\n\nRounding the digits, we produce a publication-ready table. I’m assuming the abbreviated question labels are OK—if not, each could be replaced with its complete assertion. In this form, the rows of the table are in the same order as the rows of the chart—a structure that could be useful to the reader.\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem\nStrongly Disagree\nDisagree\nNeutral\nAgree\nStrongly Agree\n\n\n\n\nBeyond the content\n6.5\n0.0\n25.8\n38.7\n29.0\n\n\nAnalyze errors\n6.5\n6.5\n22.6\n45.2\n19.4\n\n\nProvide facts\n3.2\n3.2\n16.1\n29.0\n48.4\n\n\nDevelop writing\n0.0\n6.5\n32.3\n38.7\n22.6\n\n\nIndependent learning\n6.5\n0.0\n19.4\n35.5\n38.7\n\n\n\n\n\nThe values in this table were computed by likert from the fictitious row-record data. The numbers agree with the source data table."
  },
  {
    "objectID": "posts/2022-02-19-facets-not-small-multiples/index.html",
    "href": "posts/2022-02-19-facets-not-small-multiples/index.html",
    "title": "Facets that are not small multiples",
    "section": "",
    "text": "Summary\n\n\n\nFor data structures having two categorical variables and one quantitative variable, this post illustrates how category independence or dependence influences chart design. Graphs with independent categories (multiway data) are easily created in ggplot2 with the facet_wrap() function; data with dependent categories require facet_grid() with its scales and space arguments to treat unequal vertical scales.\nSmall multiple design is a good design choice for many data structures. To facilitate visual comparisons, every small-multiple frame (variously called panels, facets, or subplots) has identical scales.\nMultiway data—a data structure comprising two independent categorical variables and one quantitative response variable—is well-matched to small multiple design.\nHowever, if the categories happen to be dependent, the data are not multiway and the small multiple design fails. Having come across data of this type in a recent article, I thought it would be interesting to illustrate, given one quantitative variable and two categorical variables, how category independence or dependence influences chart design.\nI start with a multiway data set and point out that, as a small-multiple design, the panels share common scales. In ggplot2, small multiples are usually created using facet_wrap().\nThe second example has data with dependent categories. The panels share a common, horizontal, quantitative scale but have different category scales, yielding facets that are not small multiples. I use facet_grid() with its scales and space arguments to treat the unequal vertical scales.\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages used \nlibrary(\"data.table\")\nlibrary(\"ggplot2\")"
  },
  {
    "objectID": "posts/2022-02-19-facets-not-small-multiples/index.html#multiway-example",
    "href": "posts/2022-02-19-facets-not-small-multiples/index.html#multiway-example",
    "title": "Facets that are not small multiples",
    "section": "Multiway example",
    "text": "Multiway example\nThe literal textbook example of multiway data and chart design is the livestock chart by Cleveland (1993, p. 303). The data set contains counts of 5 types of farm animals in 26 countries in 1967. The independent categories are country and animal type; the quantitative variable is the count.\nI obtained a reproduction of the data from (UCLA, 2021) and saved it in the blog data directory as a CSV file.\n\n\nR code\n# read the livestock data\nDT &lt;- fread(\"data/livestock-1967.csv\")\n\n# examine the data\nDT[]\n\n\n     livestock_type        country    count\n             &lt;char&gt;         &lt;char&gt;    &lt;num&gt;\n  1:         Cattle        Albania   478000\n  2:         Cattle        Austria  2536000\n  3:         Cattle        Belgium  3246000\n  4:         Cattle       Bulgaria  1796000\n  5:         Cattle Czechoslovakia  5131000\n ---                                       \n126:          Sheep    Switzerland   336000\n127:          Sheep         Turkey 70093000\n128:          Sheep United Kingdom 32888000\n129:          Sheep  Russia et al. 21000000\n130:          Sheep     Yugoslavia  7384000\n\n\nThe countries are assigned to the rows such that the median count by country increases from bottom to top. The livestock are assigned to the panels such that median count by livestock type increases in graphical order: increasing from left to right and from bottom to top. The logarithm of counts is used for the quantitative scale counts vary by more than four powers of 10.\n\n\nR code\nggplot(data = DT, mapping = aes(x = log10(count), y = reorder(country, count, median))) +\n  geom_point() + \n  facet_wrap(vars(reorder(livestock_type, count, median)), as.table = FALSE) +\n  labs(x = expression('Log'[10]~'number of livestock'), y = \"\")\n\n\n\n\n\nFigure 1: Cleveland’s livestock multiway chart illustrates small-multiple design.\n\n\n\n\nTypical of small multiple charts in general, all five panels have identical horizontal scales (the count) and identical vertical axes (countries). It follows that the size and aspect ratio of each panel are also identical."
  },
  {
    "objectID": "posts/2022-02-19-facets-not-small-multiples/index.html#when-categories-are-dependent",
    "href": "posts/2022-02-19-facets-not-small-multiples/index.html#when-categories-are-dependent",
    "title": "Facets that are not small multiples",
    "section": "When categories are dependent",
    "text": "When categories are dependent\nThe data set that inspired this post is from an article on a creative expression assignment used in a first-year engineering course (Chambers & Reid, 2021). In response to COVID-19, the course had transitioned from a highly interactive, in-person experience to an asynchronous, online model.\n\nThe instructors designed an assignment to encourage students to creatively express how the course interruption and transition online was affecting them. The assignment encouraged complete freedom of expression and choice of media.\n\nA data table in the article reports how the student work is coded: 28 creative genres (prose, lyrics, music performance, dance, YouTube, etc.) grouped into 6 media categories (writing, performance, video, graphics, etc.). The quantitative variable is the count of encodings by genre.\nSome submissions covered multiple media and genres, for example, an original song in a video would be coded as both. Thus the total of the count column (304 encodings) exceeds the number of submissions (N = 265). The percent column is a derived variable that reports the count as a percentage of N. The sum of the percent column is therefore greater than 100%.\nThe data are available in the blog data directory as a CSV file.\n\n\nR code\n# import the data\nDT &lt;- fread(\"data/creative-assignment-2021.csv\")\n\n# print the data table\nDT[]\n\n\n         medium                 genre count percent\n         &lt;char&gt;                &lt;char&gt; &lt;int&gt;   &lt;num&gt;\n 1:     Writing                 Prose     6     2.3\n 2:     Writing                 Essay    32    12.1\n 3:     Writing              Word art     6     2.3\n 4:     Writing    Poetry (not haiku)    32    12.1\n 5:     Writing                 Haiku     4     1.5\n 6:       Music                Lyrics     3     1.1\n 7:       Music              Playlist     3     1.1\n 8: Performance Music from instrument     6     2.3\n 9: Performance  Performance of music     6     2.3\n10: Performance                 Dance     2     0.8\n11: Performance           Spoken word     2     0.8\n12:   Sculpture      Arranged objects     6     2.3\n13:   Sculpture             Sculpture     2     0.8\n14:   Sculpture                  Lego     2     0.8\n15:       Video                TikTok     7     2.6\n16:       Video               YouTube     4     1.5\n17:       Video            Short-form     9     3.4\n18:       Video                 Music     3     1.1\n19:       Video           Spoken word     5     1.9\n20:    Graphics               Collage    13     4.9\n21:    Graphics              Painting    18     6.8\n22:    Graphics       Drawing by hand    58    21.9\n23:    Graphics   Drawing by computer    10     3.8\n24:    Graphics           Photography    15     5.7\n25:    Graphics               Cartoon    18     6.8\n26:    Graphics                 Emoji     6     2.3\n27:    Graphics                  Meme    22     8.3\n28:    Graphics            Web design     4     1.5\n         medium                 genre count percent\n\n\nThe graph I have in mind would have media type assigned to rows and media category assigned to panels. Unlike the livestock example, however, the categorical variables in these data are not independent. For example, web design is associated only with graphics, essay is associated only with writing, etc.\nIn my first attempt, I use facet_wrap() like I did with the livestock data. The visual problem is obvious…every row has a data marker in one panel only. Because the categories are not independent, the small-multiple design fails.\n\n\nR code\n# create the first chart \np &lt;- ggplot(data = DT, mapping = aes(x = percent, y = reorder(genre, count))) +\n  geom_point() + \n  facet_wrap(vars(reorder(medium, count)) , as.table = FALSE) +\n  labs(y = \"\")\n\n# display the result\np\n\n\n\n\n\nFigure 2: Small-multiple chart design does not work when categories are dependent.\n\n\n\n\nThe scales = \"free_y\" argument of facet_wrap() replaces the common y-scale with only those row labels associated with a panel. I also use the ncol argument to stack the panels in one column with all rows labels aligned on the left. This graph has the essential layout I had in mind—a common quantitative scale but different vertical scales, yielding facets that are not small multiples.\n\n\nR code\n# edit the previous chart \np &lt;- p +\n   facet_wrap(vars(reorder(medium, count)) , as.table = FALSE, scales = \"free_y\", ncol = 1)\n\n# display the result\np\n\n\n\n\n\nFigure 3: The quantitative scale is shared; the categorical scales are not.\n\n\n\n\nBecause facet_wrap() creates panels of equal height, the rows in this case are unequally spaced.\nI switch to facet_grid() to space the rows equally in panels of unequal height. The space = \"free_y\" argument is added to make the panel height proportional to the length of the y scale.\n\n\nR code\n# edit the previous chart \np &lt;- p  +\n  facet_grid(rows = vars(reorder(medium, count)), \n             as.table = FALSE, \n             scales = \"free_y\",\n             space = \"free_y\")\n\n# display the result\np\n\n\n\n\n\nFigure 4: Applying consistent row spacing, facet height depends on the number of categorical levels."
  },
  {
    "objectID": "posts/2022-02-19-facets-not-small-multiples/index.html#editing-the-chart",
    "href": "posts/2022-02-19-facets-not-small-multiples/index.html#editing-the-chart",
    "title": "Facets that are not small multiples",
    "section": "Editing the chart",
    "text": "Editing the chart\nWhile the overall layout above is what I want, I would prefer to move the panel labels from the right-hand side to the top of each panel. I could not find a way to do that, so I removed the right-hand label and wrote the media label inside the panel. I edited some other aesthetics as well.\n\n\nR code\n# create a y-coordinate for a text geom\nDT &lt;- DT[, y_coord_media := .N/2 + 0.5, by = medium]\n\n# create secondary axis data, omit some labels to avoid overprinting\ncount_scale &lt;- sort(unique(DT$count))\ntop_axis &lt;- data.table(count_scale)\ntop_axis[, count_label := as.character(count_scale)]\nomit_labels &lt;- c(\"3\", \"5\", \"7\", \"9\", \"15\")\ntop_axis[count_label %chin% omit_labels, count_label := \"\"]\n\n# ratio for secondary axis transformation\nscale_trans &lt;- sum(DT$count) / sum(DT$percent)\n\n# graph\nggplot(data = DT, mapping = aes(x = percent, y = reorder(genre, count))) +\n  geom_point(size = 2) +\n  facet_grid(rows = vars(reorder(medium, count)), \n             as.table = FALSE, \n             scales = \"free_y\",\n             space = \"free_y\") +\n  labs(x = \"Genre encodings (%)\", y = \"Genre\") +\n  theme_light() +\n  theme(strip.text.y = element_blank(), \n        panel.grid.minor = element_blank(),\n        axis.title.y = element_text(angle = 0, hjust = 1, vjust = 0.5)) +\n  geom_text(aes(x = 24, y = y_coord_media, label = medium, hjust = 1), color = \"gray50\") +\n  scale_x_continuous(sec.axis = sec_axis(trans = function(z) (z * scale_trans), \n                                         name = \"Count of genre encodings\", \n                                         breaks = top_axis$count_scale, \n                                         labels = top_axis$count_label))\n\n\n\n\n\nFigure 5: The final design incorporates a second horizontal scale along the top of the figure with counts corresponding to specific data markers.\n\n\n\n\nFeatures of the graph\n\nfacet_grid() as shown earlier for panels of unequal height with rows equally spaced.\ngeom_text() for panel labels, vertically centered.\nsec_axis() for a secondary scale along the top of the chart showing counts of genre encodings from the data table.\ntheme_light() for gray lines on white background to focus attention on the data.\nAll text is oriented horizontally for ease of reading.\nMedia are ordered by increasing counts (total count rather than median) from bottom to top.\nWithin a panel, genres are similarly ordered."
  },
  {
    "objectID": "posts/2022-02-19-facets-not-small-multiples/index.html#final-thoughts",
    "href": "posts/2022-02-19-facets-not-small-multiples/index.html#final-thoughts",
    "title": "Facets that are not small multiples",
    "section": "Final thoughts",
    "text": "Final thoughts\nRegarding their data table, the authors write,\n\n… some media were selected by large numbers of students, such as drawings by hand or computer, and writing such as poems and essays.\n\nThe graph supports this assertion but emphasizes visually a couple of other points as well. Drawings by hand, essays, and non-haiku poetry were by far the most popular genres (adding up to nearly 50% of the submission encodings) and graphics was by far the most popular medium (about 60% of the encodings).\nMainly this chart gave me a chance to clarify something that my students would sometimes overlook—that a data structure with two categories and one quantitative variable are multiway data if and only if the categories are independent and a value of the response exists for each combination of levels of the two categories. Otherwise, we cannot expect to use a small-multiple design."
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html",
    "title": "Revisiting a lying chart",
    "section": "",
    "text": "Summary\n\n\n\nI critique and correct a well-known visual lie from a 2015 Congressional hearing in which the authors abuse chart-making conventions to serve their rhetorical goals. I explore what authentic stories these data tell and what authentic questions the data raise. Originally posted in 2017, here I update the data set through 2020, comment on the emerging trends, and add R code and provide data sets for reproducibility.\nOn 2015-09-29 in a televised Congressional hearing, Representative Jason Chaffetz, Chairman of the US House Committee on Oversight and Government Reform, challenged Cecile Richards, president of the Planned Parenthood Federation of America (PPFA), to respond to this graph,\nAt the hearing, Ms. Richards responded to the graph’s misleading message, saying “It doesn’t feel like we’re trying to get to the truth here.”\nInterviewed two days later, Chairman Chaffetz denied the graph was misleading. He ignored questions about the visual message and disingenuously focused on the text, saying, “I stand by the numbers.”\nChaffetz’s denial notwithstanding, visual displays convey visual messages. And the data for this graph were compiled in a 2015 propaganda piece by Americans United for Life (AUL), a forced-birth organization dedicated to restricting reproductive rights of pregnant parents.\nImmediately following the hearing, a number of data visualization experts published critiques, for example, Linda Qiu, Keith Collins, Tessa Dignum, or Kevin Drum.\nIn this post, I focus on how Chaffetz abuses the conventions of visual rhetoric to serve his rhetorical goals and what truthful stories are actually told by these data.\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages used \nlibrary(\"ggplot2\")\nlibrary(\"data.table\")"
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html#rhetorical-abuses-of-the-original-chart",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html#rhetorical-abuses-of-the-original-chart",
    "title": "Revisiting a lying chart",
    "section": "Rhetorical abuses of the original chart",
    "text": "Rhetorical abuses of the original chart\nThe vertical scales for the two arrows are different (a “dual y-axis”), implying that 327,000 is greater than 935,573. To conceal the problem, the authors purposefully omit the vertical scales while retaining the horizontal time scale.\n\nThe authors use a conventional numerical horizontal time scale to create the perception that their message has a rational basis in the data, yet they visually distort the relationships among these numbers to support their predetermined message. Visually, in 2013, abortions appear to exceed cancer screenings by a ratio of more than of 3:1.\n\nThe authors promote their views further by selecting value-laden colors. Pink, associated in the US with breast cancer awareness, is good (“life-saving” in the graph subtitle), while red is bad, what the AUL calls “life-destructive”, implying blood and death (the AUL casts Planned Parenthood as “Abortion, Inc”).\nThe arrowheads distort the story, suggesting a “strong continuation of these trends, which we don’t know to be true” (Noah Iliinsky, interviewed by Linda Qiu).\nThe time scale implies that 8 years of data are displayed. However, numerical values are shown for only 2006 and 2013. And data markers—a visual convention of the scatterplot form—are not shown at all. Edward Tufte has argued that graphics like this have achieved a “graphical absolute zero, a null data-ink ratio” (Tufte, 1983, p. 95).\nFurthermore, that the lines cross creates the unsubtle visual implication that the trend in abortions is a dominant trend—which is the entire point of the deception.\nHere, I show the four data markers, omit the arrowhead drama, and emphasize the text (the numbers that Chaffetz “stands behind”). The revised graph forces us to see how little information is being conveyed and how the relationships have been visually distorted."
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html#redesigning-the-graph",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html#redesigning-the-graph",
    "title": "Revisiting a lying chart",
    "section": "Redesigning the graph",
    "text": "Redesigning the graph\nIn redesigning the graph, as others did immediately following the hearing, my goal is to discover what undistorted stories these data tell. I obtained the data from Planned Parenthood annual reports (e.g., 2016, 2015, 2014, etc.), the same sources claimed by Jason Chaffetz and the AUL.\nI start by reading in the data (updated to include data through 2020). The data are available in the blog data directory as a CSV file.\n\n\nR code\n# read the PPFA data set, all years, all services\ndt &lt;- fread(\"data/ppfa-data-update.csv\", header = TRUE)\n\n# examine the result\ndt[]\n\n\n     year       service       N\n    &lt;int&gt;        &lt;char&gt;   &lt;int&gt;\n 1:  2006 contraception 3977333\n 2:  2007 contraception 3889980\n 3:  2009 contraception 3685437\n 4:  2010 contraception 3685437\n 5:  2011 contraception 3436813\n---                            \n80:  2016         other  108309\n81:  2017         other  103585\n82:  2018         other  103640\n83:  2019         other   87990\n84:  2020         other   92570\n\n\nI start by showing the same four numbers with a common vertical scale and applying a color scheme that is not value-laden.\n\n\nR code\n# for this chart, restrict the data to two years and two services \nx &lt;- dt[year %in% c(2006, 2013) & service %chin% c(\"cancer\", \"abortion\")]\n\n# graph \nf &lt;- ggplot(x, aes(x = year, y = N/1e6, color = service)) +\n  geom_line() +\n  geom_point(size = 3) +\n  theme_light(base_size = 12) +\n  theme(legend.position = \"none\", \n        panel.grid.minor.y = element_blank()) + \n  labs(x = \"Year\", y = \"Services (millions)\") +\n  scale_y_continuous(limits = c(0, 4.5), breaks = c(0, 1, 2, 3, 4)) +\n  scale_x_continuous(limits = c(2006, 2014)) +\n  scale_color_manual(values = c(\"#8c510a\", \"#01665e\")) +\n  annotate(\"text\", \n           x = c(2006, 2006), \n           y = c(0.5, 2.2), \n           label = c(\"abortion\", \"cancer screening and prevention\"), \n           hjust = c(0, 0))\n\n# display\nf\n\n\n\n\n\nFigure 1: Values from the deceptive chart graphed with a common y-scale.\n\n\n\n\nNext, I include the data for all available years, including 2014 data which has become available since the time of the hearing.\n\n\nR code\n# same two services but all pre-2014 data\nx &lt;- dt[year &lt;= 2014 & service %chin% c(\"cancer\", \"abortion\")]\n\n# edit the current chart object\nf &lt;- f %+% \n  x\n\n# display\nf\n\n\n\n\n\nFigure 2: Including missing data. Note that 2008 data are unavailable.\n\n\n\n\nAlready the story has a nuance. The data provoke the question: what caused the decline in cancer screening and prevention services after 2009? The AUL claims the decline is evidence of Planned Parenthood’s changing business model, from a provider of “life-saving” to “life-destructive” services. An explanation more in touch with reality is the enactment of the Affordable Care Act (ACA) in 2010, providing more people access to health insurance thereby potentially reducing their reliance on Planned Parenthood. In addition, especially since 2003, the medical community has recommended less frequent procedures for the early detection of cancer (American Cancer Society).\nThe Planned Parenthood annual reports indicate that abortion and cancer screening and prevention are not the only services they provide. The next figure adds contraception, STI/STD testing and treatment, pregnancy and prenatal care, and “other” services: all the services in the data set.\n\n\nR code\n# for this chart, all services from 2006 to 2014\nx &lt;- dt[year &lt;= 2014]\n\n# edit the current chart object\nf &lt;- f %+% \n  x +\n  scale_color_manual(values = c(\"#5ab4ac\", \"mediumorchid4\", \"#8c510a\", \"mediumpurple1\", \"#01665e\", \"tan\")) +\n  annotate(\"text\",\n           x = 2006,\n           y = c(0, 0.5, 1.4, 2.2, 2.8, 4.2),\n           label = c(\"other\",\n                     \"abortion\",\n                     \"pregnancy and prenatal care\",\n                     \"cancer screening and prevention\",\n                     \"STI/STD test/treat\",\n                     \"contraception\"),\n           hjust = 0)\n\n# display\nf\n\n\n\n\n\nFigure 3: Adding some context by including all PPFA services.\n\n\n\n\nThis graph suggests we might want to find out what caused the increase in STI/STD testing and treatment or what’s behind the gradual decline in contraceptive services. The graph also shows that compared to the number of cancer services, between two and four times as many STI/STD services were provided. A similar ratio obtains for cancer services compared to contraception. Moreover, on this common scale, abortion services appear to be fairly constant, especially compared to the dramatic changes seen in other types of service.\nSo why did the original graph compare abortion only to cancer screening? The answer is cherry picking (ignoring inconvenient data): the decline in cancer screening and prevention services is the only trend in these data that can be used to support AUL’s argument that Planned Parenthood is “abandoning life-saving” services.\nIn a final refinement of the data, I account for population increase. To avoid a distorted view of time-series data, numbers of services should be studied on a per capita basis—in the same vein as accounting for inflation when studying dollar amounts over time.\nI use total US population data from the US Census (World Bank, 2022-01-14). These data are available in the blog data directory as a CSV file.\nDividing the number of annual services by the annual population yields the per-capita version of the chart (strictly speaking, services per 1000 people). STI/STD services have seen a slight per capita decline since 2011, a trend not seen in the earlier figure. Otherwise, nothing stands out as particularly noteworthy compared to the earlier graph.\n\n\nR code\n# read in the population data \npop &lt;- fread(\"data/ppfa-data-us-pop.csv\", header = TRUE)\n\n# join and compute the per-capita values\nx &lt;- merge(x, pop, by = \"year\", all.x = TRUE)\nx[, per_thou := round(N / (us_pop / 1000), 1)]\n\n# graph\nf &lt;- ggplot(x, aes(x = year, y = per_thou, color = service)) +\n  geom_line() +\n  geom_point(size = 3) +\n  theme_light(base_size = 12) +\n  theme(legend.position = \"none\", \n        panel.grid.minor.y = element_blank()) + \n  labs(x = \"Year\", y = \"Services (per 1000 people)\") +\n  scale_y_continuous(limits = c(0, 16), breaks = seq(0, 16, 4)) + \n  scale_x_continuous(limits = c(2006, 2014)) +\n  scale_color_manual(values = c(\"#5ab4ac\", \"mediumorchid4\", \"#8c510a\", \"mediumpurple1\", \"#01665e\", \"tan\")) +\n  annotate(\"text\",\n           x = 2006,\n           y = 3.35*c(0, 0.5, 1.4, 2.2, 2.8, 4.2),\n           label = c(\"other\", \n                     \"abortion\", \n                     \"pregnancy and prenatal care\", \n                     \"cancer screening and prevention\", \n                     \"STI/STD test/treat\", \n                     \"contraception\"),\n           hjust = 0)\n\n# display\nf\n\n\n\n\n\nFigure 4: Adjusting for population increase.\n\n\n\n\nLastly, because Jason Chaffetz (following AUL) claims that Planned Parenthood’s business model is based on decreasing “life-saving” procedures and increasing abortions (by implication, “death-serving” procedures), the obvious comparison would be between abortion services and non-abortion services. The next figure shows that comparison directly.\n\n\nR code\n# again, all services from 2006 to 2014\nx &lt;- dt[year &lt;= 2014]\n\n# sum total services by year\nx[, total := sum(N), by = year]\n\n# create non-abortion summary\nx &lt;- x[service == \"abortion\"]\nx[, abortion := N]\nx[, nonabortion := total - N]\nx &lt;- x[, .(year, abortion, nonabortion)]\n\n# transform to row-record form\nx &lt;- melt(x, \n          id = \"year\", \n          measure.vars = c(\"abortion\", \"nonabortion\"), \n          variable.name = \"service\", \n          value.name = \"N\")\n\n# adjust for population \nx &lt;- merge(x, pop, by = \"year\", all.x = TRUE)\nx[, per_thou := round(N / (us_pop / 1000), 1)]\n\n# chart\nggplot(x, aes(x = year, y = per_thou, color = service)) +\n  geom_line() +\n  geom_point(size = 3) +\n  theme_light(base_size = 12) +\n  theme(legend.position = \"none\", \n        panel.grid.minor.y = element_blank()) + \n  labs(x = \"Year\", y = \"Services (per 1000 people)\") +\n  scale_y_continuous(limits = c(0, 40), breaks = seq(0, 40, 10)) + \n  scale_x_continuous(limits = c(2006, 2014)) +\n  scale_color_manual(values = c(\"#8c510a\", \"#01665e\")) +\n  annotate(\"text\",\n           x = 2006,\n           y = c(3, 37),\n           label = c(\"abortion\",\n                     \"all other health services\"),\n           hjust = 0)\n\n\n\n\n\nFigure 5: Adjusting the visual rhetoric of the chart to the verbal rhetoric used by Chaffetz and AUL.\n\n\n\n\nThe data support Planned Parenthood’s assertion that abortion services account for a fairly constant 3% of all services they provide. However, accounting for “services” rather than numbers of people served is problematic, as discussed by the Michelle Ye Hee Lee of the Washington Post. Nevertheless, I use numbers of services here because those are the data values Chaffetz and the AUL quoted."
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html#what-stories-do-the-data-tell",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html#what-stories-do-the-data-tell",
    "title": "Revisiting a lying chart",
    "section": "What stories do the data tell?",
    "text": "What stories do the data tell?\nExploring the data as we have raises new questions.\n\nWhy have non-abortion services (including cancer screening) dropped off in recent years?\nWhat explains the dramatic increase in STI/STD treatments?\n\nWe have to look beyond the raw numbers to find a larger context.\nRegarding the drop-off in PPFA non-abortion services, the ACA was enacted in 2010. Between 2010 and 2014, the US Census Bureau reports that the percentage of the US population who were uninsured decreased from 16% to about 12%.\nBecause a high percentage of Planned Parenthood’s patients can be assumed to be uninsured (in 2014, 75% had incomes at or below 150% of the federal poverty level), we might expect a correlation between decreases in the percent uninsured and the percent using PPFA services. And so it is. Between 2010 and 2014, the annual number of Planned Parenthood patients decreased from 3 M to about 2.7 M.\nThus a decrease in the number of uninsured is accompanied by a decrease in the number of PPFA patients.\nRegarding the increase in STI/STD services, the National Coalition of STD Directors report that STI/STD cases in the US have tripled (2003–2016) while at the same time opioid abuse has exploded and:\n\nOpioid users are at higher risk of contracting STI/STDs and less likely to seek treatment.\n\nDrug-resistant strains of STI/STDs have emerged.\nState and local funding for STD budgets (2003–2016) was cut 40%.\n\nWith the leveling off of PPFA STI/STD treatments after 2010, we again see a possible correlation with passage of the ACA. A decrease in the number of uninsured is accompanied by a decrease in the number of PPFA STI/STD patients.\nLastly, Planned Parenthood receives approximately $500 M in federal funds (out of a $3.65 T budget in 2017)—a minuscule 0.014% of the federal budget to serve about 1% of the population, only a fraction of which are abortion patients."
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html#conclusions",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html#conclusions",
    "title": "Revisiting a lying chart",
    "section": "Conclusions",
    "text": "Conclusions\nIn the bigger picture, the evidence suggests that increasing the number of people having health insurance reduces the number of people relying on Planned Parenthood for reproductive health care.\nAnd regardless of how one compares the number of abortion services to other services, PPFA abortion services per capita remain fairly constant and a small percentage of the population from year to year.\nThe original graph is a deliberate visual lie."
  },
  {
    "objectID": "posts/2022-05-27-revisit-lying-chart/index.html#update",
    "href": "posts/2022-05-27-revisit-lying-chart/index.html#update",
    "title": "Revisiting a lying chart",
    "section": "2022 update",
    "text": "2022 update\nUpdating the data set to 2020 (the most recent data available) yields the chart below. The story here is that since 2014, all but two classes of services have been fairly constant on a per-capita basis.\nThe story that stands out visually is the decrease in contraceptive services and the concomitant increase in STI/STD services.\n\n\nR code\n# use the updated data set\nx &lt;- copy(dt)\nx &lt;- merge(x, pop, by = \"year\", all.x = TRUE)\nx[, per_thou := round(N / (us_pop / 1000), 1)]\n\n# edit the previous chart object\nf &lt;- f %+% \n  x +\n  scale_y_continuous(limits = c(0, 17), breaks = seq(0, 20, 4)) + \n  scale_x_continuous(limits = c(2006, 2020), breaks = seq(2006, 2020, 2)) \n\n# display \nf\n\n\n\n\n\nFigure 6: Updating the chart with current data available.\n\n\n\n\nTo place these data in context, a recent study reports a significant decline in teen sex education (Lindberg & Kantor, 2022),\n\nYoung people today are less likely to receive instruction on key sex education topics than they were 25 years ago … in 1995, 87% of females and 81% of males reported sex education about birth control methods, compared with 64% and 63% in 2015–2019, respectively.\n\nA report by the CDC reinforces the importance of effective sexual health care (CDC, 2022),\n\nIn 2020, over half (53%) of reported cases of STDs were among adolescents and young adults aged 15–24 years. … disparities [by race/ethnicity] reflect differential access to quality sexual health care [emphasis mine], as well as differences in sexual network characteristics.\n\nThe decline in effective sexual health care and the ongoing abuse of opioids are likely causes behind the contraception/STD trends seem in these data.\nFrom this perspective, the story in these data has nothing to do with cancer screening or abortion services as conveyed in the original deceptive graph. Instead, the data should be compelling us to action to reign in the spread of STI/STDs by delivering widespread, effective sex education and increasing access and use of contraceptives.\nGrievously, in 2022, many US state legislatures are planning the converse."
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html",
    "href": "posts/2022-06-06-double-y-axis/index.html",
    "title": "Another case for redesigning dual axis charts",
    "section": "",
    "text": "Summary\n\n\n\nThe author of a recent article plots two time series in a dual axis chart as visual evidence of a correlation between the two series. I re-plot the data using three recommended alternatives: a side-by-side chart, an indexed chart, and a connected scatterplot. The alternate charts are superior to the dual axis chart for drawing inferences about the data.\nDual axis charts, often used to compare two data series, are problematic even when designed with care. As Lisa Charlotte Muth explains,\nMuth cites essays on the subject by Stephen Few (2008), Stephanie Evergreen, and Cole Nussbaumer Knaflic, plus a StackOverflow thread by Hadley Wickham on why ggplot2 does not support dual axis charts.\nIn this post, I critique a dual-axis chart of two time series, re-plot the data using a side-by-side chart, an indexed chart, and a connected scatterplot, and compare the messages conveyed by the new charts to those of the dual axis chart.\nFor reproducibility, the R code for the post is listed under the “R code” pointers.\nR code\n# packages used \nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"ggpubfigs\") # color vision deficient inclusive palettes"
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#the-original-chart",
    "href": "posts/2022-06-06-double-y-axis/index.html#the-original-chart",
    "title": "Another case for redesigning dual axis charts",
    "section": "The original chart",
    "text": "The original chart\nIn a recent article, Christopher Newfield (2021) addresses the structural racism of higher education funding. He uses data from the University of California (UC) system to illustrate several points.\nIn one instance, he uses the dual axis chart shown below to compare two time series of UC data from 1999 to 2017: declining percentage of White undergraduate enrollment and declining funding as a fraction of state personal income.\n\n\n\nOriginal chart. (Newfield, 2021)\n\n\nReferring to the chart, Newfield writes,\n\n… as figure 2 illustrates, when the Democrats started their still-unbroken one-party rule of the legislature in the late 1990s, they did not break the racialized funding pattern in the state universities that [Gov. Pete] Wilson had launched. As white enrollment goes down, the state reduces net funding. The strong Democratic majority has defunded the UC system in direct proportion [emphasis mine] with its decline in white student share—except when the Arnold Schwarzenegger cuts and Jerry Brown cuts, also bipartisan, made it even worse.\n\nTo a data science practitioner, “direct proportion” implies a linear correlation between data series. In a dual axis chart, however, the appearance of correlation can be misleading because the scales of the two axes are arbitrary.\nThe article continues,\n\nWhatever their intentions, Republicans and Democrats collectively gave less state support to the diverse UC than to the white UC on which both postwar parties had happily spent.\n\nAgreed. The chart yields this message as a direct observation. In a later chart I quantify these cuts, showing that levels of funding in 2017 are only 60% of their levels in 1999.\nHe concludes this section with,\n\nThey [the Democratic majority legislature] yoked … major gains in BA attainment for students of color to steady funding cuts that saved money for the state’s white voting majority.\nAs the Biden-Harris administration settles in, it’s worth remembering that the biggest blue state’s higher education policy has been a textbook case of the systemic racism that became a national byword in 2020.\n\nGiven the data at hand, I show that charts better suited to the visual task can reveal nuances in the story obscured by the dual axis design."
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#data-structure",
    "href": "posts/2022-06-06-double-y-axis/index.html#data-structure",
    "title": "Another case for redesigning dual axis charts",
    "section": "Data structure",
    "text": "Data structure\nThe data structure is outlined in Table 1: two quantitative variables evolving over time (yearly). The funding measurement is a ratio of (presumably) nominal dollars in each year, so adjusting for inflation is unnecessary.\n\n\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nyear\ncategorical, ordinal\n\n\npercent White enrollment\nquantitative\n\n\nUC funds fraction of state income\nquantitative\n\n\n\nTable 1: Data structure\n\n\n\nI transcribed the data values from the original chart. These data are available in the blog data directory as a CSV file. Here I add the governors’ names for annotating the charts.\n\n\nR code\n# read from the blog data directory\ndt &lt;- fread(\"data/budget-justice.csv\", \n            colClasses = list(double = c(\"year\", \"white_pct\")))\n\n# add governor names\ndt[, gov := fcase(\n  year &lt; 2004, \"Gray Davis\", \n  year &gt;= 2004 & year &lt; 2011, \"A. Schwarzenegger\", \n  year &gt;= 2011, \"Jerry Brown\")]\n\n# display\ndt[]\n\n\n     year white_pct fund_frac               gov\n    &lt;num&gt;     &lt;num&gt;     &lt;num&gt;            &lt;char&gt;\n 1:  1999        40      0.43        Gray Davis\n 2:  2000        40      0.44        Gray Davis\n 3:  2001        39      0.41        Gray Davis\n 4:  2002        38      0.42        Gray Davis\n 5:  2003        38      0.40        Gray Davis\n 6:  2004        37      0.35 A. Schwarzenegger\n 7:  2005        37      0.34 A. Schwarzenegger\n 8:  2006        36      0.33 A. Schwarzenegger\n 9:  2007        36      0.33 A. Schwarzenegger\n10:  2008        35      0.20 A. Schwarzenegger\n11:  2009        34      0.22 A. Schwarzenegger\n12:  2010        33      0.32 A. Schwarzenegger\n13:  2011        32      0.24       Jerry Brown\n14:  2012        30      0.24       Jerry Brown\n15:  2013        29      0.26       Jerry Brown\n16:  2014        28      0.27       Jerry Brown\n17:  2015        27      0.27       Jerry Brown\n18:  2016        26      0.28       Jerry Brown\n19:  2017        25      0.27       Jerry Brown"
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#side-by-side-chart",
    "href": "posts/2022-06-06-double-y-axis/index.html#side-by-side-chart",
    "title": "Another case for redesigning dual axis charts",
    "section": "Side-by-side chart",
    "text": "Side-by-side chart\nMuth and others recommend plotting two data series in separate charts. Stephen Few, for example, says\n\nThe first and most obvious [approach] is to place them in separate graphs, positioned close to one another so that the patterns in each can be compared to one another, but magnitude comparisons will be discouraged. (Few, 2008)\n\nFollowing this prescription, I plot the two data series in Figure 1 using separate panels having their own vertical, quantitative scales but sharing a common time scale. I select an aspect ratio that emphasizes the slopes of the line segments.\nNewfield mentions the terms of two California governors, so I annotate the chart to indicate the gubernatorial terms that span these data: Gray Davis (Democrat), Arnold Schwarzenegger (Republican), and Jerry Brown (Democrat).\n\n\nR code\n# transform to block record form\ndt_melt &lt;- melt(dt, \n                measure.vars  = c(\"white_pct\", \"fund_frac\"), \n                variable.name = \"data_series\", \n                value.name    = \"value\")\n\n# stand-alone data frame for annotating gubernatorial terms\ngov_text &lt;- data.frame(\n  label = c(\"Davis\", \"Schwarzenegger\", \"Brown\"),\n  data_series = \"fund_frac\",\n  x = c(2001, 2007.5, 2014),\n  y = 0.38\n)\n\n# vector for relabeling the facets\nfacet_labels &lt;- c(\"UC funding fraction of state personal income\", \n                  \"White undergraduate % enrollment\")\nnames(facet_labels) &lt;- c(\"fund_frac\", \"white_pct\")\n\n# chart\nggplot(data = dt_melt, mapping = aes(x = year, y = value)) +\n  facet_wrap(vars(data_series), \n             ncol = 2, \n             scales = \"free_y\", \n             labeller = labeller(data_series = facet_labels)) +\n  geom_line(size = 0.5, linetype = 2) +\n  geom_point(size = 2) +\n  scale_x_continuous(minor_breaks = seq(1999, 2017, 1)) +\n  labs(x = \"Year\", y = \"\") +\n  theme(panel.grid.minor.y = element_blank()) +\n  \n  # delineate gubernatorial terms\n  geom_vline(xintercept = c(2004, 2011), \n             color = \"gray30\", \n             size = 0.5, \n             linetype = 2) +\n  geom_text(data = gov_text,\n            mapping = aes(x = x, y = y, label = label),\n            size = 3)\n\n\n\n\n\nFigure 1: Redrawing the dual axis chart as a side-by-side chart with gubernatorial terms delineated.\n\n\n\n\nThe decrease in the White undergraduate population fraction is fairly steady over this time span. This is not surprising given the changing demographics of the state: in 2000, Non-Hispanic whites were 46% of the state population; in 2010, 40%; and in 2020, 35% (Allen & Turner, 2011; McGhee et al., 2021).\nUC funding decreases as well, but with greater irregularity, especially during the Schwarzenegger years. Even ignoring the cuts associated with the 2008 global financial debacle, Schwarzenegger’s tenure concludes with the largest cuts in the data set.\nYet contrary to Newfield’s assertion that Jerry Brown’s cuts “made it even worse,” the funding trend changes course under Brown. It doesn’t get much better, but it doesn’t get worse either."
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#indexed-chart",
    "href": "posts/2022-06-06-double-y-axis/index.html#indexed-chart",
    "title": "Another case for redesigning dual axis charts",
    "section": "Indexed chart",
    "text": "Indexed chart\nThe second choice of many experts is the indexed chart. Again, from Stephen Few,\n\nThe other less obvious solution, which works only for time series, is to convert all sets of values to a common quantitative scale by displaying percentage differences between each value and a reference (or index) value. For instance, select a particular point in time, such as the first interval that appears in the graph, and express each subsequent value as the percentage difference between it and the initial value. (Few, 2008)\n\nSelecting 1999 as the index year, in Figure 2 I graph enrollment change with respect to its index (40%) and funding change with respect to its index (0.43). Thus both series share a common quantitative scale as well as a common time scale.\nIn reading the chart, for example, a change of −10% indicates the variable has dropped by 10% relative to its 1999 index value.\n\n\nR code\n# transform to block record form \ndt_idx &lt;- melt(dt, \n               measure.vars = c(\"white_pct\", \"fund_frac\"), \n               variable.name = \"data_series\", \n               value.name = \"measure\")\n\n# compute percent change relative to 1999\nsetkeyv(dt_idx, c(\"data_series\", \"year\"))\ndt_idx[, idx := measure[1], by = c(\"data_series\")]\ndt_idx[, rel_change :=  100 * ((measure / idx) - 1)]\n\n# create the base chart for all versions of indexed charts\np &lt;- ggplot(data = dt_idx, \n            mapping = aes(x = year, y = rel_change, color = data_series)) +\n  \n  # 1999 baseline\n  geom_segment(aes(x = 1999, xend = 2017, \n                   y = 0, yend = 0), \n                   color = \"gray30\", \n                   size = 0.5) +\n  annotate(\"text\", \n           x = 2017, \n           y = 0, \n           label = \"1999 baseline\", \n           hjust = 1, \n           vjust = -0.5) +\n  \n  # delineate gubernatorial terms\n  geom_vline(xintercept = c(2004, 2011),\n             color = \"gray30\",\n             size = 0.5,\n             linetype = 2) +\n  geom_text(data = dt_idx[data_series == \"fund_frac\" & year %in% c(2001, 2008, 2014)], \n            mapping = aes(y = -56, label = gov), \n            color = \"black\") +\n  \n  # label the two data series\n  annotate(\"text\", \n           x = 2017, \n           y = c(-22, -44), \n           label = c(\"White enrollment share\", \"Funding\"), \n           hjust = c(1, 1), \n           vjust = c(0, 0)) +\n  \n  # points, scales, labels, theme\n  geom_point(size = 3) + \n  labs(x = \"Year\", y = \"Percent change since 1999\") +\n  scale_color_manual(values = friendly_pal(\"contrast_three\")) + \n  scale_x_continuous(breaks = seq(2000, 2020, 5), \n                     minor_breaks = seq(1999, 2020, 1)) + \n  scale_y_continuous(breaks = seq(-50, 0, 10), \n                     labels = c(paste0(\"\\U2212\", seq(50, 10, -10)), paste0(\"\\U00B1\", \"0%\"))) +\n  theme(panel.grid.minor.y = element_blank(), \n        legend.position = \"none\")\n\n# connect the dots for the first indexed chart\np + geom_line(linetype = 1, size = 0.5)\n\n\n\n\n\nFigure 2: How UC enrollment and funding change relative to their values in 1999.\n\n\n\n\nIn an indexed chart, lines that are locally parallel indicate a potential correlation between data series.\nTo check for parallels, I replace the connect-the-dots lines with regression curves. At first glance, the lines are approximately parallel, suggesting a correlation. The problem is lack of fit: a linear fit is a good model for the enrollment data but a poor model for the funding data.\n\n\nR code\n# plot overall regression lines\np + geom_smooth(method = \"lm\", se = FALSE, size = 0.5)\n\n\n\n\n\nFigure 3: Trying a linear fit to both data series.\n\n\n\n\nThe shape of the funding data is clearly different from one governor to the next. In the next chart, I create separate funding regressions by governor, ignoring the 2008-09 data as outliers, and I leave the enrollment linear fit as is. The aspect ratio is arbitrary: in this chart, relative parallelism is the indicator, not the slope itself.\n\n\nR code\n# leave enrollment fit as is\np + geom_smooth(data = dt_idx[data_series == \"white_pct\"], \n                method = \"lm\", \n                se = FALSE, \n                size = 0.5) +\n  \n  # add regression lines by governor\n  geom_smooth(data = dt_idx[data_series == \"fund_frac\" & year &lt;= 2004],\n              method = \"lm\", \n              se = FALSE, \n              size = 0.5) +\n  geom_smooth(data = dt_idx[data_series == \"fund_frac\" & year %in% c(2004:2007, 2010:2011)],\n              method = \"lm\",\n              se = FALSE,\n              size = 0.5) +\n  geom_smooth(data = dt_idx[data_series == \"fund_frac\" & year &gt;= 2011],\n              method = \"lm\", \n              se = FALSE, \n              size = 0.5)\n\n\n\n\n\nFigure 4: Applying separate funding regressions by governor.\n\n\n\n\nAgain looking for parallel lines, I think this chart brings the shapes of the funding trends by governor more clearly to the forefront.\n\nUnder Davis, the enrollment and funding lines are nearly parallel, indicating a potential correlation.\nUnder Schwarzenegger, the lines are again roughly parallel though at lower funding levels—again indicating a potential correlation.\nUnder Brown, the two lines are clearly not parallel—a negative slope for enrollment, a positive slope for funding. These results argue against a correlation under Brown."
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#connected-scatterplot",
    "href": "posts/2022-06-06-double-y-axis/index.html#connected-scatterplot",
    "title": "Another case for redesigning dual axis charts",
    "section": "Connected scatterplot",
    "text": "Connected scatterplot\nScatterplots are designed to reveal correlations between two quantitative variables.\nHowever, our two quantitative variables are both time series, adding a third variable to the story. To account for the time sequence, the data markers in a scatterplot are sequentially connected—yielding the “connected scatterplot” design. As Muth says,\n\nSuddenly, time doesn’t move from left to right, but wiggles through space.\n\nThe connected scatterplot for the UC enrollment/funding data is given in Figure 5: enrollment data along the x-axis, funding data along the y-axis; each data marker represents the enrollment-funding relationship in a given year; data markers are connected in sequence by year; and starting and ending years are labeled to orient the reader. The 1:1 aspect ratio makes relative diagonals easier to identify (more on this shortly).\n\n\nR code\nggplot(dt, aes(x = white_pct, y = fund_frac)) +\n  geom_path(linetype = 2) +\n  geom_point(size = 3) +\n  labs(x = \"White undergraduate enrollment (%)\", \n       y = \"UC funding / State  personal income\") +\n  geom_text(data = dt[year %in% c(1999, 2017)],\n           aes(label = year), \n           nudge_y = - 0.01)\n\n\n\n\n\nFigure 5: Initial connected scatterplot.\n\n\n\n\nThe non-intuitive aspect of this chart is that time reads from right to left, which is likely to confuse some readers.\nI reverse the direction of the time path in Figure 6 by transforming the enrollment variable to percent enrollment of students of color (which increases over time.) I also encode the gubernatorial terms using color and add several year labels to assist the reader. (The data marker for 2006 overprints the identical value for 2005.) The color palette is selected to accommodate viewers with a color vision deficiency.\n\n\nR code\n# add end-of-term dummy rows grouping by governor\ndum_row &lt;- copy(dt)\ndum_row &lt;- dum_row[year == 2004 | year == 2011]\ndum_row[year == 2004, gov := \"Gray Davis\"]\ndum_row[year == 2011, gov := \"A. Schwarzenegger\"]\ndt_conn &lt;- rbindlist(list(dt, dum_row))\n\n# add a column to ensure correct order\ndt_conn[, order_gov := fcase(\n  gov %chin% \"Gray Davis\", 1, \n  gov %chin% \"A. Schwarzenegger\", 2,\n  gov %chin% \"Jerry Brown\", 3 \n)]\nsetkeyv(dt_conn, c(\"year\", \"order_gov\"))\n\n# base plot for connected scatterplot. color = gov uses the dummy rows\np &lt;- ggplot(dt_conn, aes(x = 100 - white_pct, y = fund_frac, color = gov)) +\n  geom_point(size = 3) +\n  labs(x = \"Enrollment of students of color (%)\", \n       y = \"UC funding / State  personal income\") +\n  theme(legend.position = \"none\") +\n  \n  # label selected years with original dt\n  geom_text(data = dt[year %in% c(1999, 2004, 2008, 2011, 2017)],\n           aes(label = year), \n           color = \"black\", \n           nudge_x = 0.7  * c( 0,  -1, -1,  0,  0), \n           nudge_y = 0.01 * c(-1,   0, 0.5, -1, -1)) +\n  \n  # label governors once per span\n  geom_text(data = dt_conn[year %in% c(2001, 2008, 2014)], \n            mapping = aes(label = gov), \n            nudge_x = 0.5 * c(0, -1, 0), \n            nudge_y = 0.03 * c(-1, 2, -1), \n            fontface = \"bold\")\n\n# first connected scatterplot chart, connect the dots\nq &lt;- p + geom_path(linetype = 2)\nq\n\n\n\n\n\nFigure 6: Final connected scatterplot. UC funding as a function of percent enrollment of students of color.\n\n\n\n\nIn a connected scatterplot, lines on a relative diagonal indicate a potential correlation between variables.\n\n… positive correlations show up as relatively diagonal lines along the “forward slash” axis of the connected scatterplot, while negative correlations show up on the “backslash” axis. (Haroz et al., 2016)\n\nTo help tease out the underlying visual pattern, I add regression lines within the gubernatorial segments as before (ignoring the funding data in 2008–09 as outliers), yielding Figure 7. The 1:1 aspect ratio makes relative diagonals easier to identify.\n\n\nR code\n# plot regression lines over gubernatorial terms\nq + geom_smooth(data = dt_conn[!year %in% c(2008, 2009)], \n                method = \"lm\", \n                se = FALSE, \n                size = 1)\n\n\n\n\n\nFigure 7: Identifying potential correlations over different gubernatorial terms.\n\n\n\n\nThe chart yields a story consistent with my earlier findings,\n\nA negative diagonal, as in the Davis and Schwarzenegger years, indicates a negative correlation, that is, a correlation between decreasing funding and increasing percent enrollment of students of color.\nA relatively flat slope, as in the Brown years, indicates that funding is fairly static, independent of the change in population fraction. No correlation."
  },
  {
    "objectID": "posts/2022-06-06-double-y-axis/index.html#discussion",
    "href": "posts/2022-06-06-double-y-axis/index.html#discussion",
    "title": "Another case for redesigning dual axis charts",
    "section": "Discussion",
    "text": "Discussion\nEach of the three alternate chart designs is more effective than the dual axis chart at revealing and conveying the stories in these data.\nOf the three alternatives, I suspect that the side-by-side chart would work best for most audiences. The trends can be seen without having to explain the normalization of the indexed chart or the path-through-time of the connected scatterplot.\nHowever, the indexed chart and the connected scatterplot are better than the side-by-side chart at revealing correlations: parallel lines in the indexed chart; diagonal lines in the connected scatterplot.\nTaken all together, I think the revised charts support these statements:\n\nUC funding was cut under Democratic governor Gray Davis and cut even more under Republican governor Schwarzenegger.\nUC funding leveled off under Democratic governor Brown—not improving much but not getting worse.\nOver the same time period, the fraction of White undergraduate enrollment steadily decreased.\n\nAnd these inferences:\n\nThe increasing percentage of students of color is consistent with the increasing percentage of people of color in the state.\nThe correlation between race and funding is more readily demonstrated for Davis’ and Schwarzenegger’s terms in office than it is in Brown’s.\n\nYet, as Newfield reminds us—before, during, and after the time span of these data, the legislature has been under one-party, Democratic rule; a legislature that has repeatedly cut the UC budget such that in 2017 funding as a percent of state personal income is only 60% of what it was in 1999—a persistent devaluation of higher education as a public good."
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html",
    "href": "posts/2022-08-15-radar-charts/index.html",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "",
    "text": "Summary\n\n\n\nI compare the effectiveness of two charts—a radar (or spider) chart and a faceted dot chart—in communicating information about Burtin’s 1951 antibiotics/bacteria data. The faceted dot chart is more effective, suggesting in general that radar charts should be replaced by dot-chart designs configured for the data at hand.\nAt a recent virtual workshop, a participant asked my opinion of radar charts (also called spider charts). I replied that I think there are nearly always more effective alternatives.\nOther authors agree, e.g., Graham Odds (2011) and Stephen Few (2005), concluding that bar charts or line charts communicate information more effectively than radar charts in nearly every case. I concur, but I thought it would be interesting to compare a radar chart to an alternative using somewhat more complex data than those in typical examples.\nI selected data from a 1951 chart by graphic designer Will Burtin (1908–1972) displaying the effectiveness of three antibiotics in inhibiting the growth of 16 bacteria. My inspiration is Medical Illuminations (2014) in which Howard Wainer discusses in detail twenty different charts of these data. In this post, I compare two new displays of these data—a radar chart and a faceted dot chart.\nFor reproducibility, the R code for the post is listed under the “R code” pointers.\nR code\n# packages used \nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"ggradar\") # ggplot2 compatible radar charts\nFor consistent radar charts, I wrap ggradar() in a custom function. I selected the ggradar package because it supports editing using conventional ggplot2 functions.\nR code\n# custom function for consistent radar charts\nmake_radar_chart &lt;- function(dframe, subtitle) {\n  \n  # assign constants\n  tick &lt;- 0.3\n  grid_lines &lt;- c(-3, -1, 3)\n  medium_gray &lt;- \"gray70\"\n  \n  # delete all but the required columns\n  dframe &lt;- dframe[, .(Bacteria, Penicillin, Streptomycin, Neomycin)]\n\n  # create the radar chart using a subset of the data frame  \n  ggradar(plot.data = dframe, \n          \n          # three grid lines allowed\n          values.radar = grid_lines, \n          grid.min = grid_lines[1], \n          grid.mid = grid_lines[2], \n          grid.max = grid_lines[3],\n          \n          # manual adjustments for clear viewing\n          gridline.label.offset = 1.5, \n          plot.extent.x.sf = 1.5, \n          plot.extent.y.sf = 1.25,\n          centre.y = -4,\n          \n          # aesthetics\n          background.circle.colour = \"transparent\",\n          grid.label.size = 5, \n          group.line.width = 0.5, \n          group.point.size = 3) +\n    \n    # ggplot2 edits, including tick marks along the P-axis\n    labs(subtitle = subtitle) +\n    theme(plot.subtitle = element_text(size = 18, \n                                       face = \"bold\", \n                                       hjust = 0, \n                                       vjust = -4, \n                                       color = medium_gray), \n          legend.justification = c(0, 1), \n          legend.background = element_blank(), \n          legend.key.height = unit(6, \"mm\"), \n          legend.position = c(-0.04, 0.93), # c(-0.04, 1.05), \n          legend.text  = element_text(size = 12, face = \"italic\"), \n          legend.title = element_blank()) +\n    geom_segment(x = -tick, y = 2, xend = tick, yend = 2, color = medium_gray) +\n    geom_segment(x = -tick, y = 4, xend = tick, yend = 4, color = medium_gray) +\n    geom_segment(x = -tick, y = 5, xend = tick, yend = 5, color = medium_gray) +\n    geom_segment(x = -tick, y = 6, xend = tick, yend = 6, color = medium_gray) \n}"
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#data",
    "href": "posts/2022-08-15-radar-charts/index.html#data",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Data",
    "text": "Data\nI transcribed the data from Wainer (Table 2.1, p. 24) but updated the taxonomy of two bacteria that have been renamed since Burtin’s work.\n\n\nIn 1974, Diplococcus pneumoniae was renamed Streptococcus pneumoniae. In 1984, Streptococcus faecalis was renamed Enterococcus faecalis.\nOne of Wainer’s goals was to illustrate how some chart designs, had they been investigated sooner, could have revealed an odd pattern in Burtin’s data that might have led to an earlier reclassification of bacterium genus. My goal is different. I want to illustrate the relative effectiveness of two specific charts by considering the ease and accuracy of answering key domain-specific questions about the data.\nThe data with current taxonomy are saved in the blog data directory as a CSV file.\n\n\nR code\n# read the updated Burtin data set\nDT &lt;- fread(\"data/antibiotic-bacteria-mic.csv\")\n\n# examine the data\nDT[]\n\n\n                     Bacteria Gram_stain Penicillin Streptomycin Neomycin\n                       &lt;char&gt;     &lt;char&gt;      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n 1:      Aerobacter aerogenes   negative    8.7e+02         1.00    1.600\n 2:        Bacillus anthracis   positive    1.0e-03         0.01    0.007\n 3:          Brucella abortus   negative    1.0e+00         2.00    0.020\n 4:  Streptococcus pneumoniae   positive    5.0e-04        11.00   10.000\n 5:          Escherichia coli   negative    1.0e+02         0.40    0.100\n---                                                                      \n12:      Staphylococcus albus   positive    7.0e-03         0.10    0.001\n13:     Staphylococcus aureus   positive    3.0e-02         0.03    0.001\n14:     Enterococcus faecalis   positive    1.0e+00         1.00    0.100\n15: Streptococcus hemolyticus   positive    1.0e-03        14.00   10.000\n16:    Streptococcus viridans   positive    5.0e-03        10.00   40.000\n\n\nThe data set comprises three categorical variables and one quantitative variable as listed in Table 1. The quantitative variable is the minimum inhibitory concentration (MIC)—the least concentration of antibiotic that prevents growth of a bacterium in vitro, where concentration is the ratio of the drug to its liquid media in milligrams/deciliter (mg/dl).\n\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nbacteria\ncategorical, nominal, 16 levels\n\n\nantibiotic\ncategorical, nominal, 3 levels\n\n\nmin. inhibitory concentration (MIC)\nquantitative (mg/dl)\n\n\nGram stain\ncategorical, 2 levels, dependent on bacteria\n\n\n\nTable 1: Data structure\n\n\n\n\nThe Gram stain variable indicates a bacterium’s response to a cell-staining method, named after bacteriologist Hans Christian Gram (1853–1938). After staining and counter-staining, those that remain purple are called Gram-positive; those that turn pink are called Gram-negative."
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#shaping-the-data",
    "href": "posts/2022-08-15-radar-charts/index.html#shaping-the-data",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Shaping the data",
    "text": "Shaping the data\nI’m not using Gram staining as a graph element, so I append the Gram-positive information to the name of the bacterium and drop the Gram-staining variable.\n\n\nR code\n# append Gram-positive to bacteria names\nDT[Gram_stain == \"positive\", Bacteria := paste(Bacteria, \"(+)\")]\nDT[, Gram_stain := NULL]\n\n# display the result\nDT[]\n\n\n                         Bacteria Penicillin Streptomycin Neomycin\n                           &lt;char&gt;      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n 1:          Aerobacter aerogenes    8.7e+02         1.00    1.600\n 2:        Bacillus anthracis (+)    1.0e-03         0.01    0.007\n 3:              Brucella abortus    1.0e+00         2.00    0.020\n 4:  Streptococcus pneumoniae (+)    5.0e-04        11.00   10.000\n 5:              Escherichia coli    1.0e+02         0.40    0.100\n---                                                               \n12:      Staphylococcus albus (+)    7.0e-03         0.10    0.001\n13:     Staphylococcus aureus (+)    3.0e-02         0.03    0.001\n14:     Enterococcus faecalis (+)    1.0e+00         1.00    0.100\n15: Streptococcus hemolyticus (+)    1.0e-03        14.00   10.000\n16:    Streptococcus viridans (+)    5.0e-03        10.00   40.000\n\n\nI order the bacteria by the median of their three MIC values (per Wainer) and add a lowercase letter (a.–p.) to the bacteria names to provide a simple verification that the charts reflect the desired order.\nBecause MIC values span several orders of magnitude, I also apply a log10 transformation to the numerical columns.\n\n\nR code\n# order bacteria by row-wise median MIC\nDT[, median_MIC := apply(.SD, 1, median), .SDcols = c(\"Penicillin\", \"Streptomycin\", \"Neomycin\")]\nsetorder(DT, median_MIC)\n\n# lowercase letters to verify bacteria order in the charts\nDT[, order_ID := letters[1:nrow(DT)]]\nDT[, Bacteria := paste0(order_ID, \". \", Bacteria)]\nDT[, order_ID := NULL]\n\n# transform MIC by log10\nnumeric_cols &lt;- which(sapply(DT, is.numeric))\nDT[ , (numeric_cols) := lapply(.SD, log10), .SDcols = numeric_cols]\n\n# display the result\nDT[]\n\n\n                            Bacteria Penicillin Streptomycin Neomycin\n                              &lt;char&gt;      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n 1:        a. Bacillus anthracis (+)     -3.000       -2.000   -2.155\n 2:      b. Staphylococcus albus (+)     -2.155       -1.000   -3.000\n 3:     c. Staphylococcus aureus (+)     -1.523       -1.523   -3.000\n 4:              d. Proteus vulgaris      0.477       -1.000   -1.000\n 5:              e. Escherichia coli      2.000       -0.398   -1.000\n---                                                                  \n12:        l. Pseudomonas aeruginosa      2.929        0.301   -0.398\n13:    m. Mycobacterium tuberculosis      2.903        0.699    0.301\n14:  n. Streptococcus pneumoniae (+)     -3.301        1.041    1.000\n15: o. Streptococcus hemolyticus (+)     -3.000        1.146    1.000\n16:    p. Streptococcus viridans (+)     -2.301        1.000    1.602\n    median_MIC\n         &lt;num&gt;\n 1:     -2.155\n 2:     -2.155\n 3:     -1.523\n 4:     -1.000\n 5:     -0.398\n---           \n12:      0.301\n13:      0.699\n14:      1.000\n15:      1.000\n16:      1.000\n\n\nClinically plausible dosages are those less than 0.1 mg/dl or log10MIC ≤ –1. Thus concentrations greater than –1 indicate a bacterium that for clinical purposes can be considered resistant to the antibiotic.\nWainer illustrates (in Figure 3.3 by Brian Schmotzer, p. 54) that this resistance measure, summarized in Table 2, is a useful criterion by which the data in a chart can be organized to answer key questions.\n\n\n\n\n\n\n\nLabel\nBacterium is resistant to\n\n\n\n\nNone\nNone of the three antibiotics\n\n\nP\nPenicillin only\n\n\nPS\nPenicillin and Streptomycin only\n\n\nSN\nStreptomycin and Nemomycin only\n\n\nPSN\nAll three antibiotics\n\n\n\nTable 2: Bacterial resistance groupings\n\n\n\n\nI add these resistance labels to the data frame, creating a new categorical variable (resistance) that is dependent on the bacteria.\n\n\nR code\n# classify bacteria by resistance\nDT[, resistance := fcase(\n  Penicillin &gt;  -1 & Streptomycin &gt;  -1 & Neomycin &gt;  -1, \"PSN\", \n  Penicillin &gt;  -1 & Streptomycin &gt;  -1 & Neomycin &lt;= -1, \"PS\", \n  Penicillin &lt;= -1 & Streptomycin &gt;  -1 & Neomycin &gt;  -1, \"SN\", \n  Penicillin &gt;  -1 & Streptomycin &lt;= -1 & Neomycin &lt;= -1, \"P\", \n  Penicillin &lt;= -1 & Streptomycin &lt;= -1 & Neomycin &lt;= -1, \"None\" \n)]\n\n# display the result\nDT[]\n\n\n                            Bacteria Penicillin Streptomycin Neomycin\n                              &lt;char&gt;      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n 1:        a. Bacillus anthracis (+)     -3.000       -2.000   -2.155\n 2:      b. Staphylococcus albus (+)     -2.155       -1.000   -3.000\n 3:     c. Staphylococcus aureus (+)     -1.523       -1.523   -3.000\n 4:              d. Proteus vulgaris      0.477       -1.000   -1.000\n 5:              e. Escherichia coli      2.000       -0.398   -1.000\n---                                                                  \n12:        l. Pseudomonas aeruginosa      2.929        0.301   -0.398\n13:    m. Mycobacterium tuberculosis      2.903        0.699    0.301\n14:  n. Streptococcus pneumoniae (+)     -3.301        1.041    1.000\n15: o. Streptococcus hemolyticus (+)     -3.000        1.146    1.000\n16:    p. Streptococcus viridans (+)     -2.301        1.000    1.602\n    median_MIC resistance\n         &lt;num&gt;     &lt;char&gt;\n 1:     -2.155       None\n 2:     -2.155       None\n 3:     -1.523       None\n 4:     -1.000          P\n 5:     -0.398         PS\n---                      \n12:      0.301        PSN\n13:      0.699        PSN\n14:      1.000         SN\n15:      1.000         SN\n16:      1.000         SN"
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#radar-charts",
    "href": "posts/2022-08-15-radar-charts/index.html#radar-charts",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Radar charts",
    "text": "Radar charts\nThis data frame is correctly shaped for ggradar() in row-record form, in which every record about a bacterium is in a single row.\nExplaining a radar chart\nI subset one bacterium (one row and four columns) from the data frame to illustrate radar-chart terminology in Figure 1.\n\n\nR code\n# select one row only from the data set \ndata_group &lt;- DT[Bacteria %ilike% \"abortus\", .(Bacteria, Penicillin, Streptomycin, Neomycin)]\n\n# display the result\ndata_group[]\n# create the sample radar chart\np &lt;- make_radar_chart(data_group, subtitle = \"Radar chart components\") \n\n# annotate the features of the chart\np +\n  geom_text(x = 2.5, y = 2.5, label = \"dosage max grid line\", \n            hjust = 0, vjust = 1, size = 4, color = \"gray45\") +\n  geom_text(x = -0.1, y = 5.7, label = \"P-axis\", \n            hjust = 1.1, size = 4, color = \"gray45\") +\n  geom_text(x = -4, y = -3.1, label = \"N-axis\", \n            hjust = 0.3, size = 4, color = \"gray45\") +\n  geom_text(x = 4.1, y = -3.1, label = \"S-axis\", \n            hjust = 0.7, size = 4, color = \"gray45\") +\n  geom_text(x = 5.5, y = 5.5, label = \"scale max grid line\", \n            hjust = 0, vjust = 1, size = 4, color = \"gray45\") +\n  geom_text(x = 5, y = -1.6, label = expression(\"log\"[10]~\"MIC\"), \n            hjust = 0.2, vjust = 0.8, size = 4, color = \"gray45\") \n\n\n\n\n              Bacteria Penicillin Streptomycin Neomycin\n                &lt;char&gt;      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;\n1: h. Brucella abortus          0        0.301     -1.7\n\n\n \n\n\n\n\n\n\n\nFigure 1: Illustrating the components of a radar chart with the log10MIC values of three antibiotics on one bacterium.\n\n\n\n\n \n\n\n\n\nThe axes of the chart encode the antibiotics: a P-axis (Penicillin), an S-axis (Streptomycin), and an N-axis (Neomycin). The three radial axes are rotated 120 degrees apart.\nThe axes have identical scales marked with log10MIC equal to –3, –1, and +3 and connected with circular grid lines. Reference tick marks are added to the P-axis in integer increments.\nThe log10 concentration values are encoded as data markers on the axes—in this example, 0.0 on the P-axis, +0.3 on the S-axis, and –1.7 on the N-axis.\nData markers inside the –1 grid line indicate clinically plausible MICs; points outside the line indicate resistance to the antibiotic. In this example, Brucella abortus is resistant to Penicillin and Streptomycin but not to Neomycin.\nThe thin helper lines between data markers create a polygon for that bacterium to help distinguish it from other bacteria when more than one are graphed in the same chart. The polygons, while useful visual aids, are not generally useful for drawing inferences about the data.\nCreating the radar charts\nI use the resistance variable to subset the data frame and construct one radar chart per group, yielding the five charts assembled in Figure 2. The legend keys verify that the charts retain the desired order of the bacteria, from (a) to (p) in order of increasing median MIC.\n\n\nR code\ndata_group &lt;- DT[resistance == \"None\"]\nmake_radar_chart(data_group, subtitle = \"Resistant to None\")\n\ndata_group &lt;- DT[resistance == \"P\"]\nmake_radar_chart(data_group, subtitle = \"Resistant to P\")\n\ndata_group &lt;- DT[resistance == \"PS\"]\nmake_radar_chart(data_group, subtitle = \"Resistant to PS\")\n\ndata_group &lt;- DT[resistance == \"PSN\"]\nmake_radar_chart(data_group, subtitle = \"Resistant to PSN\")\n\ndata_group &lt;- DT[resistance == \"SN\"]\nmake_radar_chart(data_group, subtitle = \"Resistant to SN\")\n\n\n\n\n\n\n\n\nFigure 2: Radar charts of Burtin’s data with current bacteria taxonomy. Data points on the radial axes are log10MIC values.\n\n\nI assembled these images in a two-column format to make it possible to view all of them together. I made the font size as large as possible without overprinting important graphical elements. (The code for assembling the images is provided in an appendix.)"
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#faceted-dot-chart",
    "href": "posts/2022-08-15-radar-charts/index.html#faceted-dot-chart",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Faceted dot chart",
    "text": "Faceted dot chart\nShaping the data\nFor compatibility with ggplot(), I transform the data from row records to block records. In block-record form, everything about a bacterium occupies a “block” of rows. For example, Bacillus anthracis occupies the first three rows instead of the first row alone as it did previously. The utility of this form is that both Antibiotic and Concentration are explicit variables with one value per observation (row).\n\n\nR code\nDT_facet &lt;- melt(DT, \n           id.vars = c(\"Bacteria\", \"median_MIC\", \"resistance\"), \n           variable.name = \"Antibiotic\", \n           variable.factor = FALSE, \n           value.name = \"Concentration\")\nsetcolorder(DT_facet, c(\"Bacteria\", \"Antibiotic\", \"Concentration\",  \"median_MIC\", \"resistance\"))\n\n# order rows to illustrate block records\nDT_facet[order(median_MIC, Bacteria)]\n\n\n                            Bacteria   Antibiotic Concentration median_MIC\n                              &lt;char&gt;       &lt;char&gt;         &lt;num&gt;      &lt;num&gt;\n 1:        a. Bacillus anthracis (+)   Penicillin         -3.00      -2.15\n 2:        a. Bacillus anthracis (+) Streptomycin         -2.00      -2.15\n 3:        a. Bacillus anthracis (+)     Neomycin         -2.15      -2.15\n 4:      b. Staphylococcus albus (+)   Penicillin         -2.15      -2.15\n 5:      b. Staphylococcus albus (+) Streptomycin         -1.00      -2.15\n---                                                                       \n44: o. Streptococcus hemolyticus (+) Streptomycin          1.15       1.00\n45: o. Streptococcus hemolyticus (+)     Neomycin          1.00       1.00\n46:    p. Streptococcus viridans (+)   Penicillin         -2.30       1.00\n47:    p. Streptococcus viridans (+) Streptomycin          1.00       1.00\n48:    p. Streptococcus viridans (+)     Neomycin          1.60       1.00\n    resistance\n        &lt;char&gt;\n 1:       None\n 2:       None\n 3:       None\n 4:       None\n 5:       None\n---           \n44:         SN\n45:         SN\n46:         SN\n47:         SN\n48:         SN\n\n\nWe can add one more categorical variable (efficacy) to help visually distinguish between clinically plausible dosages (log10MIC ≤ –1) and bacterial resistance (log10MIC &gt; -1) in the faceted dot chart. Table 3 lists the complete augmented set of variables.\n\n\nR code\n# efficacy variable used as legend key\nDT_facet[, efficacy := fifelse(Concentration &lt;= -1, \"Effective\", \"Resistant\")]\n\n\n\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nbacteria\ncategorical, nominal, 16 levels\n\n\nantibiotic\ncategorical, nominal, 3 levels\n\n\nmin. inhibitory concentration (MIC)\nquantitative (mg/dl)\n\n\nGram stain\ncategorical, 2 levels, dependent on bacteria\n\n\nresistance profile\ncategorical, 5 levels, dependent on bacteria\n\n\nefficacy\ncategorical, 2 levels, dependent on MIC\n\n\n\nTable 3: Augmented data structure\n\n\n\n\nCreating the faceted dot chart\nBefore plotting, I edit the data to order the rows and panels and add a resistance subtitle.\n\n\nR code\n# manually order the columns of the panel grid\nDT_facet[, Antibiotic := lapply(.SD, factor, levels = c(\"Penicillin\", \"Streptomycin\", \"Neomycin\")), .SDcols = \"Antibiotic\"]\n\n# order the bacteria as a factor\nDT_facet[, Bacteria := lapply(.SD, factor, levels = rev(sort(unique(Bacteria)))), .SDcols = \"Bacteria\"]\n\n# add a subtitle for the resistance category \nDT_facet[resistance == \"None\", resistance := \"Resistant\\nto\\nNone\"]\n\n\nThe resulting chart has 15 facets in a 5 by 3 grid—five rows for the resistance variable and three columns for the antibiotic variable. Individual bacteria form an ordered vertical scale.\n\n\nR code\n# construct the faceted dot chart\nggplot(DT_facet, aes(x = Concentration, y = Bacteria, color = efficacy)) +\n  geom_point(size = 2) +\n  facet_grid(cols = vars(Antibiotic),\n             rows = vars(reorder(resistance, median_MIC)), \n             switch = \"y\", \n             scales = \"free_y\",\n             space  = \"free_y\") +\n  \n  # annotations\n  geom_vline(xintercept = -1, linetype = 2, linewidth = 0.5, color = \"gray40\") +\n  geom_text(data  = DT_facet[resistance == \"PSN\"], \n            mapping = aes(x = -1, y = 1.5, label = c(\"max dose\")),\n            vjust = -0.4, hjust = 0, angle = 90, color = \"gray30\", size = 3) +\n  \n  # scales\n  scale_x_continuous(limits = c(-3.5, 3.5), breaks = seq(-3, 3, 2)) +\n  scale_color_manual(values = c(\"black\", \"gray\")) +\n  scale_y_discrete(position = \"right\") + \n  labs(x = \"MIC (log10 mg/dl)\", y = \"\") +\n  \n  # theme arguments\n  theme_minimal() +\n  theme(\n    # MIC scale\n    axis.text.x   = element_text(size = 9),\n    # bacteria scale\n    axis.text.y   = element_text(size = 9, hjust = 0, angle = 0, face = \"italic\"), \n    # antibiotic labels\n    strip.text.x  = element_text(size = 10, hjust = 0),\n    # resistance labels\n    strip.text.y.left  = element_text(size = 10, hjust = 1, angle = 0), \n    # panels\n    panel.border  = element_rect(fill = NA, color = \"gray90\", linewidth = 0.7),\n    panel.spacing = unit(1, \"mm\"), \n    # legend\n    legend.position = \"bottom\", \n    legend.title = element_blank(), \n    legend.text  = element_text(size = 12), \n    legend.key.size = unit(2, \"mm\")\n  )\n\n\n\n\n\nFigure 3: Faceted dot chart of Burtin’s data with current bacteria taxonomy. Bacteria are ordered (a) through (p) by increasing median MIC.\n\n\n\n\nThe log10MIC variable is encoded on identical horizontal scales. The maximum clinically plausible dosage is indicated with a vertical dashed line, separating interactions that are effective from those that are resistant.\nFor ggplot2 users, the interesting features of the code include:\n\nfacet_grid(switch = \"y\") with scale_y_discrete(position = \"right\") to place the bacteria labels on the right and the resistance category on the left\nfacet_grid() arguments scales and space to create evenly spaced rows when, because of dependent categories, no facets are small multiples."
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#discussion",
    "href": "posts/2022-08-15-radar-charts/index.html#discussion",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Discussion",
    "text": "Discussion\nWainer poses five possible key questions that the data were gathered to answer (p. 30). Two of those questions arise naturally from the basic data structure—that MIC observations correspond to unique antibiotic-bacterium combinations. The first question focuses on the bacteria:\n\nHow do the bacteria group vis-à-vis their reaction to the antibiotics?\n\nAs we have seen, grouping bacteria by their resistance profiles succeeds in revealing similarities. Both the radar chart and the faceted dot chart support this grouping visually: in the radar chart, by the similarity of polygons in an individual sub-plot; in the faceted chart, by the similarity of dot patterns in an individual facet.\nThe second question focuses on the antibiotics:\n\nHow do the antibiotics group with respect to their efficacy in controlling the bacteria?\n\nConsider first the efficacy of the drugs on one bacterium, e.g., item (g), Salmonella schottmeulleri. From the radar chart, we can infer that Neomycin is effective and that the other two drugs are resistant. However, the overprinting of data markers on the three axes makes it difficult to visually quantify the differences between the P-, S-, and N-value of MIC for this bacterium.\nIn contrast, in the faceted dot chart, data markers for a bacterium are located in a row with one row per bacterium and one panel per antibiotic. There is no overprinting. Looking along row (g), we again see that Neomycin is effective and the others are resistant but we can also visually estimate the MIC values using the horizontal scale: Neomycin log10MIC ≈ –1, Streptomycin ≈ 0, and Penicillin ≈ 1.\nThe faceted chart, compared to the radar chart, makes such differences easier to quantify. Adding more circular grid lines to the radar chart might improve our ability to estimate its MIC values, but the radar chart has an intrinsic potential for data markers (and the polygons) overprinting one another.\nSecond, let’s consider differential efficacy of the drugs across groupings. For example, for how many of the resistance-groupings is penicillin effective? Using the radar charts, we examine the P-axis of all 5 charts and conclude that Penicillin is effective in two of the groupings. (The names of the groupings tell us this as well, but here I want to focus on the effectiveness of the visualization.)\nIn the faceted chart, the five panels in the Penicillin column yield the same conclusion at a glance.\nWainer’s other three questions are:\n\nWhat bacterium does this antibiotic kill?\nWhat antibiotic kills this bacterium?\nDoes the “Gram staining” variable help us make decisions regarding which antibiotic to use?\n\nAll three questions can be answered by both charts, but generally more quickly and clearly using the faceted dot chart. I leave the details of that comparison to the reader.\nIn general, the advantages of the faceted dot chart include:\n\nHorizontal grid lines suffice to distinguish one bacterium from another.\nGreater visual access to patterns of data in different combinations.\nRelatively compact without sacrificing readability.\nData markers do not overprint one another.\nConventional horizontal scales."
  },
  {
    "objectID": "posts/2022-08-15-radar-charts/index.html#conclusion",
    "href": "posts/2022-08-15-radar-charts/index.html#conclusion",
    "title": "Spiders, facets, and dots, oh my!",
    "section": "Conclusion",
    "text": "Conclusion\nUsing Will Burtin’s 1951 data on the efficacy of three antibiotics on 16 bacteria (with updated bacteria taxonomy), I compared the effectiveness of two types of charts by considering the ease and accuracy of answering key domain-specific questions.\nBy using the same data organized in the same way, both charts are designed to convey the same message. Thus any differences in perceived effectiveness should be due to differences in chart structure, that is, characteristics of the chart intrinsic to its type.\nFor the data at hand, a faceted dot chart communicates more effectively than a radar chart. Intrinsic differences between the two chart types suggest that in general appropriately configured dot charts are more effective than radar charts."
  },
  {
    "objectID": "posts/2022-09-05-sex-ed-studies/index.html",
    "href": "posts/2022-09-05-sex-ed-studies/index.html",
    "title": "Multiple studies, one chart",
    "section": "",
    "text": "Summary\n\n\n\nI redesign a grouped-bar chart to attempt to better align the logic of the display with the logic of the argument. The redesigned chart provides better visual access to year-by-year comparisons by different combinations of categories and reveals nuances in the data not obvious in the original chart.\nIn working on a previous post, I cited an article on US sex education for adolescents (Lindberg & Kantor, 2022). That article included a conventional grouped bar chart displaying results of five surveys over 25 years from the National Survey of Family Growth (NSFG).\nThe chart caught my attention because its visual design is slightly dissonant with its rhetorical purpose. Its purpose is to support the assertion that five surveys over 25 years reveal a decline in US sex education. Time is the implicit independent variable. The chart design, however, uses grouped bars in which evolution over time is not encoded to scale.\nThe original chart supports the authors’ argument, but I was curious if a redesigned chart, treating time as a scaled, explicit independent variable, could improve the visual rhetoric. And if so, does it reveal aspects of the data stories previously hidden?\nThe R code for the post is listed under the “R code” pointers.\nR code\n# packages used \nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"ggpubfigs\")"
  },
  {
    "objectID": "posts/2022-09-05-sex-ed-studies/index.html#the-original-chart",
    "href": "posts/2022-09-05-sex-ed-studies/index.html#the-original-chart",
    "title": "Multiple studies, one chart",
    "section": "The original chart",
    "text": "The original chart\nThe grouped bar chart by Lindberg and Kantor illustrates the changing fraction of US adolescents receiving sex education instruction from 1995 to 2019. The results are compiled from 5 studies, each with its own time span. Bars are grouped by educational topic and student sex.\n\n\n\nFigure 1: Original figure. Percentage of adolescents (aged 15–19 years) who received instruction on two specific sex education topics before the age of 18. (Lindberg & Kantor, 2022)\n\n\nIn their prose, the authors make the argument that\n\nYoung people today are less likely to receive instruction on key sex education topics than they were 25 years ago, as indicated by comparing the prevalence estimates from 2011–2015 and 2015–2019 calculated in this study to published estimates from earlier NSFG rounds.\n\nThe chart supports the assertion. The quantitative variable (percentage) is the bar height, encoded using a zero baseline that (correctly) avoids distorting visual comparisons. The survey years—the implicit independent variable—are in sequence from left to right and are encoded by a sequential gray-scale fill.\nGrouped-bar charts are generally OK for visually comparing bar heights within a group, but less useful for comparing between groups. For example, the evolution of “Say No to Sex” for Females clearly shows a descending trend; the same information for Males is fairly constant. The design does not, however, facilitate direct, year-by-year comparisons between data in different clusters. Nor does it visually encode the temporal span of a study within the overall 25 years.\nIn addition, much of the chart portrays what Edward Tufte calls “non-data-ink.” In this instance, only the bar height (percentage) and the gray-scale (survey years) encode data—neither the width of a bar nor the area of a bar convey information. Such non-data-ink can distract a viewer from the intended message. In this example, the filled-in bars create a visual geography that draws my eye to the center of a cluster area—a sort of visual center of gravity. I have to consciously focus on the bar tops to make comparisons.\nIn my redesigned chart, I attempt to address these issues."
  },
  {
    "objectID": "posts/2022-09-05-sex-ed-studies/index.html#data-structure",
    "href": "posts/2022-09-05-sex-ed-studies/index.html#data-structure",
    "title": "Multiple studies, one chart",
    "section": "Data structure",
    "text": "Data structure\nThe data structure is outlined in Table 1. The percentage of teens aged 15–19 receiving instruction is the single quantitative variable. Because three of the studies cover a span of years, I treat the starting and ending years as separate variables.\n\n\n\n\n\n\n\n\nvariable\nstructure\n\n\n\n\nyear study starts\ncategorical, ordinal, five levels\n\n\nyear study ends\ncategorical, ordinal, five levels\n\n\neducational topic\ncategorical, nominal, two levels\n\n\nstudent sex\ncategorical, nominal, two levels\n\n\npercent receiving instruction\nquantitative\n\n\n\nTable 1: Data structure\n\n\n\nI don’t have the original data tables, so I approximated the values by measuring the bar lengths in the original figure. These estimates are available in the blog data directory as a CSV file.\n\n\nR code\n# read the data \ndt &lt;- fread(\"data/sex-ed-topics-data.csv\", \n            colClasses = list(double = c(\"start\", \"end\", \"pct\")))\n\n# display\ndt[]\n\n\n    study start   end         Topic    Sex   pct\n    &lt;int&gt; &lt;num&gt; &lt;num&gt;        &lt;char&gt; &lt;char&gt; &lt;num&gt;\n 1:     1  1995  1995 Birth control Female    87\n 2:     1  1995  1995 Birth control   Male    81\n 3:     1  1995  1995 Say no to sex Female    92\n 4:     1  1995  1995 Say no to sex   Male    74\n 5:     2  2002  2002 Birth control Female    70\n 6:     2  2002  2002 Birth control   Male    66\n 7:     2  2002  2002 Say no to sex Female    86\n 8:     2  2002  2002 Say no to sex   Male    83\n 9:     3  2006  2010 Birth control Female    70\n10:     3  2006  2010 Birth control   Male    61\n11:     3  2006  2010 Say no to sex Female    89\n12:     3  2006  2010 Say no to sex   Male    82\n13:     4  2011  2015 Birth control Female    66\n14:     4  2011  2015 Birth control   Male    58\n15:     4  2011  2015 Say no to sex Female    84\n16:     4  2011  2015 Say no to sex   Male    82\n17:     5  2015  2019 Birth control Female    64\n18:     5  2015  2019 Birth control   Male    63\n19:     5  2015  2019 Say no to sex Female    81\n20:     5  2015  2019 Say no to sex   Male    80\n    study start   end         Topic    Sex   pct"
  },
  {
    "objectID": "posts/2022-09-05-sex-ed-studies/index.html#redesign",
    "href": "posts/2022-09-05-sex-ed-studies/index.html#redesign",
    "title": "Multiple studies, one chart",
    "section": "Redesign",
    "text": "Redesign\nTo organize the chart, I convert the Topic and Sex variables to factors. I select the order of the factor levels to control the placement of panels and legend entries.\n\n\nR code\n# manually order two categories\ndt$Topic &lt;- factor(dt$Topic, levels = c(\"Say no to sex\", \"Birth control\"))\ndt$Sex   &lt;- factor(dt$Sex,   levels = c(\"Female\", \"Male\"))\n\n\nThe original article argues that the decline in the quantitative variable evolves over time, thus a scatterplot design is my starting point. Time (discrete years) is on the horizontal scale and the percent receiving instruction on the vertical scale. One-year surveys are represented with “point” data markers and multi-year surveys with line segments. Topics are assigned to panels and sex to color.\n\n\nR code\nggplot() +\n  geom_point(data = dt[start == end], \n             mapping = aes(x = start, y = pct, color = Sex), \n             size = 2.5) +\n  geom_segment(data = dt[start != end], \n               mapping = aes(x = start, xend = end, y = pct, yend = pct, color = Sex), \n               size = 1.5) + \n  facet_wrap(vars(Topic), ncol = 2) + \n  \n  # scales\n  scale_x_continuous(limits = c(1994, 2020), \n                     breaks = c(1995, 2002, 2006, 2011, 2015, 2019)) +\n  scale_y_continuous(limits = c(50, 100)) +\n  scale_color_manual(values = friendly_pal(\"vibrant_seven\")) + \n  labs(x = \"Survey year or span, inclusive\", \n       y = \"Percentage (%)\", \n       title = \"Comparing results of 5 surveys over 25 years\") +\n  \n  # customize the theme \n  theme_minimal() + \n  theme(legend.position = \"right\", \n        panel.spacing.x = unit(5, \"mm\"), \n        panel.grid.minor.x = element_blank(), \n        panel.border = element_rect(fill = NA, color = \"gray70\"))\n\n\n\n\n\nFigure 2: Revised figure. Percentage of adolescents (aged 15–19 years) who received instruction on two specific sex education topics before the age of 18.\n\n\n\n\nUnlike the original chart, the time scale is in correct proportion. Scale markings match the survey dates.\nLike the original, the vertical scale is the percentage receiving instruction. Unlike bar charts however, this chart does not require a zero baseline. I select an aspect ratio to highlight, but not distort, the comparisons.\nWith topics in separate panels, the relative trends are much easier to see. With sex encoded by color, each facet permits a direct, year-by-year comparison between Female and Male survey responses on a topic.\nComparisons between panels is more difficult than comparisons within a panel, but removing the non-data-ink has made it easier to compare the data by topic and sex over time and in specific study years.\n\n\n\n\n\n\nTip for ggplot2 users\n\n\n\nAn interesting challenge was choosing “geoms” when some studies appear in one year only and other studies span several years. If I use the segment geom only, the one-year studies disappear from the chart—in discrete time, these data have zero width as line segments.\nMy solution is to use a point geom for the one-year surveys and a segment geom for the multi-year surveys with each geom acting on its own data subset."
  },
  {
    "objectID": "posts/2022-09-05-sex-ed-studies/index.html#discussion",
    "href": "posts/2022-09-05-sex-ed-studies/index.html#discussion",
    "title": "Multiple studies, one chart",
    "section": "Discussion",
    "text": "Discussion\nThe authors’ discussion of the original chart include the following points, that the revised chart also supports:\n\nYoung people today are less likely to receive instruction on key sex education topics than they were 25 years ago\nFor abstinence instruction, the share of female adolescents receiving instruction declined over the 25 year span; for males, the share increased once then remained stable.\nFor contraception instruction, the share declined over 25 years for both sexes, though the most recent survey showed a relative increase for males.\n\nThe revised chart supports each of these points with greater visual clarity than the original. Moreover, the new chart supports two additional, general observations:\n\nFemales receive instruction at higher rates than males (though the gap has closed in recent years).\nAbstinence is nearly always instructed at higher rates than contraception.\n\nIn my opinion, the first observation would seem to reflect a cultural bias that views pregnancy as a “womans’ issue” (Phillips, 2018). The second, our society’s willingness to indulge in (and fund) wishful thinking (Guttmacher Institute, 2021).\nThe authors’ original argument is supported by the new chart and with greater visual access to the underlying data than the original grouped bar chart. Thus in response to my original question, I think the visual rhetoric has been improved and previously hidden nuance has been revealed.\nLastly, encoding time to scale does not yield any particularly interesting insights regarding the survey results—but it does display the relative timing and duration of the surveys, possibly enhancing the credibility of the evidence."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html",
    "href": "posts/2022-11-18-formatdown/index.html",
    "title": "Introducing formatdown",
    "section": "",
    "text": "Summary\n\n\n\nConvert the elements of a numerical vector or data frame column to character strings in which the numbers are formatted using powers-of-ten notation in scientific or engineering form and delimited for rendering as inline equations in an rmarkdown document.\nInitial release of the formatdown R package providing tools for formatting output in rmarkdown or quarto markdown documents.\nThis first version has one function only, format_power(), for converting numbers to character strings formatted in powers-of-ten notation and delimited in $...$ for rendering as inline equations in .Rmd or .qmd output documents. Provides two powers-of-ten formatting options—scientific notation and engineering notation—with an option to omit powers-of-ten notation for a specified range of exponents.\nTo illustrate the different formats, I show in Table 1 the same number rendered using different formats, all with 4 significant digits.\nThe R code for the post is listed under the “R code” pointers. In the examples, I use data.table syntax for data manipulation, though the code can be translated into base R or dplyr syntax if desired.\nR code\nlibrary(\"formatdown\")\nlibrary(\"data.table\")\n\n# value\nx &lt;- 4.567E-4\n# omit power-of-ten\nx1 &lt;- format_power(x, digits = 4, omit_power = c(-6, 0)) \n# scientific\nx2 &lt;- format_power(x, digits = 4, format = \"sci\")        \n# engineering\nx3 &lt;- format_power(x, digits = 4)\nNotation\nName\nValue\nRendered_as\n\n\n\n\nwithout \\(10^n\\)\nx1\n$0.0004567$\n\\(0.0004567\\)\n\n\nscientific\nx2\n$4.567\\times{10}^{-4}$\n\\(4.567\\times{10}^{-4}\\)\n\n\nengineering\nx3\n$456.7\\times{10}^{-6}$\n\\(456.7\\times{10}^{-6}\\)\n\n\n\nTable 1: Rendering a number using different formats"
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#background",
    "href": "posts/2022-11-18-formatdown/index.html#background",
    "title": "Introducing formatdown",
    "section": "Background",
    "text": "Background\nMy first attempt to provide powers-of-ten formatting was in my 2016 package, docxtools. That implementation has several shortcomings.\nI wrote its formatting function to accept a data frame as input, which entailed a lot of programming overhead to separate numerical from non-numerical variable classes and to reassemble them after the numerical columns were formatted. This could have been simplified with judicious use of lapply(), with which I was not sufficiently experienced at the time. I also failed to take advantage of formatC() in constructing the output.\nWith formatdown, my goal is to provide similar functionality but with more concise code, greater flexibility, and a more balanced approach to package dependencies."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#improvements",
    "href": "posts/2022-11-18-formatdown/index.html#improvements",
    "title": "Introducing formatdown",
    "section": "Improvements",
    "text": "Improvements\nThe primary design change is that the format_power() function operates on a numerical vector instead of a data frame. The benefits of this change are: 1) simpler code that should be easier to revise and maintain; 2) scalar values can be formatted for rendering inline; and 3) data frames can still be formatted, by column, using lapply().\nTo illustrate formatting a scalar value inline, the markup for Avogadro’s number (x = 6.0221E+23) in engineering format is given by,\n    $N_A =$ `r format_power(x, digits = 5, format = \"engr\")`\nwhich is rendered (in this output document) as \\(N_A =\\) \\(602.21\\times{10}^{21}\\).\nThe second improvement is the addition of an option for scientific notation. For example, the markup for Avogadro’s number in scientific notation is given by,\n    $N_A =$ `r format_power(x, digits = 5, format = \"sci\")`\nwhich renders as \\(N_A =\\) \\(6.0221\\times{10}^{23}\\).\nThe third improvement is the addition of an option for omitting powers-of-ten notation over a range of exponents. For example, the markup for x = 1.23E-4 in decimal notation is given by,\n    $x =$ `r format_power(x = 1.234E-4, omit_power = c(-4, 0))`\nwhich renders as \\(x =\\) \\(0.000123\\).\nA final (internal) improvement is a more balanced approach to package dependencies. With a tighter focus on what formatdown is to accomplish compared to docxtools, I have reduced the dependencies to checkmate, wrapr, and data.table.\nThe package vignette illustrates package usage in detail.\nHowever, having successfully submitted the package to CRAN, I started working on this post and immediately (!) uncovered an issue that had not appeared while working on the package vignettes."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#delimiter-issue",
    "href": "posts/2022-11-18-formatdown/index.html#delimiter-issue",
    "title": "Introducing formatdown",
    "section": "Delimiter issue",
    "text": "Delimiter issue\nI wrote the package vignette using the rmarkdown::html_vignette output style per usual. All the formatted output rendered as expected in that document. I write this blog using quarto. As seen in the examples above, inline math is rendered as expected.\nThe issue arises when using knitr::kable() and kableExtra::kbl() to display data tables in this blog post. To illustrate, consider this data frame, included with formatdown (ideal gas properties of air at room temperature).\n\n\nR code\ndensity\n\n\n         date  trial humidity    T_K   p_Pa     R  density\n       &lt;Date&gt; &lt;char&gt;   &lt;fctr&gt;  &lt;num&gt;  &lt;num&gt; &lt;int&gt;    &lt;num&gt;\n1: 2018-06-12      a      low 294.05 101100   287 1.197976\n2: 2018-06-13      b     high 294.15 101000   287 1.196384\n3: 2018-06-14      c   medium 294.65 101100   287 1.195536\n4: 2018-06-15      d      low 293.35 101000   287 1.199647\n5: 2018-06-16      e     high 293.85 101100   287 1.198791\n\n\nFormatting the pressure column, the markup looks OK.\n\n\nR code\nDT &lt;- copy(density)\nDT$p_Pa &lt;- format_power(DT$p_Pa, 4)\nDT\n\n\n         date  trial humidity    T_K                   p_Pa     R  density\n       &lt;Date&gt; &lt;char&gt;   &lt;fctr&gt;  &lt;num&gt;                 &lt;char&gt; &lt;int&gt;    &lt;num&gt;\n1: 2018-06-12      a      low 294.05 $101.1\\\\times{10}^{3}$   287 1.197976\n2: 2018-06-13      b     high 294.15 $101.0\\\\times{10}^{3}$   287 1.196384\n3: 2018-06-14      c   medium 294.65 $101.1\\\\times{10}^{3}$   287 1.195536\n4: 2018-06-15      d      low 293.35 $101.0\\\\times{10}^{3}$   287 1.199647\n5: 2018-06-16      e     high 293.85 $101.1\\\\times{10}^{3}$   287 1.198791\n\n\nknitr::kable() yields the expected output with pressure formatted in engineering notation.\n\n\nR code\nknitr::kable(DT, align = \"r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\ntrial\nhumidity\nT_K\np_Pa\nR\ndensity\n\n\n\n\n2018-06-12\na\nlow\n294.05\n\\(101.1\\times{10}^{3}\\)\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n\\(101.0\\times{10}^{3}\\)\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n\\(101.1\\times{10}^{3}\\)\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n\\(101.0\\times{10}^{3}\\)\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n\\(101.1\\times{10}^{3}\\)\n287\n1.198791\n\n\n\n\n\n\nProblem\nkableExtra::kbl() does not render the math markup as expected.\n\n\nR code\nkableExtra::kbl(DT, align = \"r\")\n\n\n\n\n\ndate\ntrial\nhumidity\nT_K\np_Pa\nR\ndensity\n\n\n\n\n2018-06-12\na\nlow\n294.05\n$101.1\\times{10}^{3}$\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n$101.0\\times{10}^{3}$\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n$101.1\\times{10}^{3}$\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n$101.0\\times{10}^{3}$\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n$101.1\\times{10}^{3}$\n287\n1.198791\n\n\n\n\n\n\n\nIn fact, having loaded kableExtra above, knitr::kable() now fails in the same way.\n\n\nR code\nknitr::kable(DT, align = \"r\")\n\n\n\n\n\ndate\ntrial\nhumidity\nT_K\np_Pa\nR\ndensity\n\n\n\n\n2018-06-12\na\nlow\n294.05\n$101.1\\times{10}^{3}$\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n$101.0\\times{10}^{3}$\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n$101.1\\times{10}^{3}$\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n$101.0\\times{10}^{3}$\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n$101.1\\times{10}^{3}$\n287\n1.198791\n\n\n\n\n\n\n\n\n\nSolution\nI found a suggestion from MathJax to replace the $ ... $ delimiters with \\\\( ... \\\\). I wrote a short function (below) to do that.\n\n\nR code\n# Substitute math delimiters\nsub_delim &lt;- function(x) {\n  x &lt;- sub(\"\\\\$\", \"\\\\\\\\(\", x) # first $\n  x &lt;- sub(\"\\\\$\", \"\\\\\\\\)\", x) # second $\n}\n\nDT$p_Pa &lt;- sub_delim(DT$p_Pa)\nDT\n\n\n         date  trial humidity    T_K                       p_Pa     R  density\n       &lt;Date&gt; &lt;char&gt;   &lt;fctr&gt;  &lt;num&gt;                     &lt;char&gt; &lt;int&gt;    &lt;num&gt;\n1: 2018-06-12      a      low 294.05 \\\\(101.1\\\\times{10}^{3}\\\\)   287 1.197976\n2: 2018-06-13      b     high 294.15 \\\\(101.0\\\\times{10}^{3}\\\\)   287 1.196384\n3: 2018-06-14      c   medium 294.65 \\\\(101.1\\\\times{10}^{3}\\\\)   287 1.195536\n4: 2018-06-15      d      low 293.35 \\\\(101.0\\\\times{10}^{3}\\\\)   287 1.199647\n5: 2018-06-16      e     high 293.85 \\\\(101.1\\\\times{10}^{3}\\\\)   287 1.198791\n\n\nknitr::kable() yields the expected output.\n\n\nR code\nknitr::kable(DT, align = \"c\")\n\n\n\n\n\ndate\ntrial\nhumidity\nT_K\np_Pa\nR\ndensity\n\n\n\n\n2018-06-12\na\nlow\n294.05\n\\(101.1\\times{10}^{3}\\)\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n\\(101.0\\times{10}^{3}\\)\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n\\(101.1\\times{10}^{3}\\)\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n\\(101.0\\times{10}^{3}\\)\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n\\(101.1\\times{10}^{3}\\)\n287\n1.198791\n\n\n\n\n\n\n\nkableExtra::kbl() yields the expected output.\n\n\nR code\nkableExtra::kbl(DT, align = \"c\")\n\n\n\n\n\ndate\ntrial\nhumidity\nT_K\np_Pa\nR\ndensity\n\n\n\n\n2018-06-12\na\nlow\n294.05\n\\(101.1\\times{10}^{3}\\)\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n\\(101.0\\times{10}^{3}\\)\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n\\(101.1\\times{10}^{3}\\)\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n\\(101.0\\times{10}^{3}\\)\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n\\(101.1\\times{10}^{3}\\)\n287\n1.198791\n\n\n\n\n\n\n\nI can use the features from kableExtra to print a pretty table.\n\n\nR code\nlibrary(\"kableExtra\")\n\nvar_names &lt;- c(\"Date\", \"Trial\", \"Humidity\", \"Temperature\", \"Pressure\", \"Gas constant\", \"Density\" )\nvar_units &lt;- c(\"\", \"\", \"\", \"[K]\", \"[Pa]\", \"[J/(kg K)]\", \"[kg/m\\\\(^3\\\\)]\")\nvar_align &lt;- \"r\"\n\nDT |&gt; \n  kbl(align = var_align, col.names = var_units) |&gt;\n  column_spec(1:6, color = \"black\", background = \"white\") |&gt;\n  add_header_above(header = var_names, align = var_align, background = \"#c7eae5\", line_sep = 0) |&gt;\n  kable_paper(lightable_options = \"basic\", full_width = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTrial\n\n\nHumidity\n\n\nTemperature\n\n\nPressure\n\n\nGas constant\n\n\nDensity\n\n\n\n\n\n\n[K]\n[Pa]\n[J/(kg K)]\n[kg/m\\(^3\\)]\n\n\n\n\n2018-06-12\na\nlow\n294.05\n\\(101.1\\times{10}^{3}\\)\n287\n1.197976\n\n\n2018-06-13\nb\nhigh\n294.15\n\\(101.0\\times{10}^{3}\\)\n287\n1.196384\n\n\n2018-06-14\nc\nmedium\n294.65\n\\(101.1\\times{10}^{3}\\)\n287\n1.195536\n\n\n2018-06-15\nd\nlow\n293.35\n\\(101.0\\times{10}^{3}\\)\n287\n1.199647\n\n\n2018-06-16\ne\nhigh\n293.85\n\\(101.1\\times{10}^{3}\\)\n287\n1.198791\n\n\n\nTable 2: Data frame displayed using kableExtra\n\n\n\n\n\n\nFollow up\nTo address this issue, the next version of format_power() will include a new delim argument,\n    format_power(x, digits, format, omit_power, delim)\nthat allows a user to set the math delimiters to $ ... $ or \\\\( ... \\\\) or even custom left and right markup to suit their environment."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#fixed-exponents",
    "href": "posts/2022-11-18-formatdown/index.html#fixed-exponents",
    "title": "Introducing formatdown",
    "section": "Fixed exponents",
    "text": "Fixed exponents\nPreparing this post, I adapted a table of water properties from the hydraulics package to use as an example and discovered another, more subtle issue. First, I’ll construct the data frame.\n\n\nR code\n# Construct a table of water properties\ntemperature     &lt;- seq(0, 45, 10) + 273.15\ndensity         &lt;- c(1000, 1000, 998, 996, 992)\nspecific_weight &lt;- c(9809, 9807, 9793, 9768, 9734)\nviscosity       &lt;- c(173, 131, 102, 81.7, 67.0) * 1E-8\nbulk_modulus    &lt;- c(202, 210, 218, 225, 228) * 1E+7\n\nwater &lt;- data.table(temperature, density, specific_weight, viscosity,  bulk_modulus)\n\nwater\n\n\n   temperature density specific_weight viscosity bulk_modulus\n         &lt;num&gt;   &lt;num&gt;           &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:      273.15    1000            9809  1.73e-06     2.02e+09\n2:      283.15    1000            9807  1.31e-06     2.10e+09\n3:      293.15     998            9793  1.02e-06     2.18e+09\n4:      303.15     996            9768  8.17e-07     2.25e+09\n5:      313.15     992            9734  6.70e-07     2.28e+09\n\n\n\nProblem\nI format all the columns and change the delimiters as described earlier and display the result. The viscosity column reveals the problem.\n\n\nR code\nDT &lt;- copy(water)\n\n# 5 signif digits\ncols_to_format &lt;- c(\"temperature\")\nDT[, (cols_to_format) := lapply(.SD, function(x) format_power(x, 5)), .SDcols = cols_to_format]\n\n# 4 signif digits\ncols_to_format &lt;- c(\"specific_weight\")\nDT[, (cols_to_format) := lapply(.SD, function(x) format_power(x, 4)), .SDcols = cols_to_format]\n\n# 3 signif digits\ncols_to_format &lt;- c(\"viscosity\", \"bulk_modulus\")\nDT[, (cols_to_format) := lapply(.SD, function(x) format_power(x)), .SDcols = cols_to_format]\n\n# 3 signif digits omit powers\ncols_to_format &lt;- c(\"density\")\nDT[, (cols_to_format) := lapply(.SD, function(x) format_power(x, omit_power = c(0, 3))), .SDcols = cols_to_format]\n\n# change the delimiters\nDT &lt;- DT[, lapply(.SD, function(x) sub_delim(x))]\n\n# Table \nDT |&gt; \n  kbl(align = \"cclrrrr\") |&gt;\n  kable_paper(lightable_options = \"basic\", full_width = TRUE) |&gt;\n  row_spec(0, background = \"#c7eae5\") |&gt;\n  column_spec(1:5, color = \"black\", background = \"white\")\n\n\n\n\n\ntemperature\ndensity\nspecific_weight\nviscosity\nbulk_modulus\n\n\n\n\n\\(273.15\\)\n\\(1000\\)\n\\(9.809\\times{10}^{3}\\)\n\\(1.73\\times{10}^{-6}\\)\n\\(2.02\\times{10}^{9}\\)\n\n\n\\(283.15\\)\n\\(1000\\)\n\\(9.807\\times{10}^{3}\\)\n\\(1.31\\times{10}^{-6}\\)\n\\(2.10\\times{10}^{9}\\)\n\n\n\\(293.15\\)\n\\(998\\)\n\\(9.793\\times{10}^{3}\\)\n\\(1.02\\times{10}^{-6}\\)\n\\(2.18\\times{10}^{9}\\)\n\n\n\\(303.15\\)\n\\(996\\)\n\\(9.768\\times{10}^{3}\\)\n\\(817\\times{10}^{-9}\\)\n\\(2.25\\times{10}^{9}\\)\n\n\n\\(313.15\\)\n\\(992\\)\n\\(9.734\\times{10}^{3}\\)\n\\(670\\times{10}^{-9}\\)\n\\(2.28\\times{10}^{9}\\)\n\n\n\n\n\n\n\nThe viscosity column displays three values using \\(10^{-6}\\) and two using \\(10^{-9}\\). Visually comparing the values in a column is easier if the powers of ten are identical. The table below illustrates the desired result, created by manually editing the two viscosity values.\n\n\nR code\n# Manually edit strings to illustrate\nDT$viscosity[4] &lt;- \"\\\\(0.82\\\\times{10}^{-6}\\\\)\"\nDT$viscosity[5] &lt;- \"\\\\(0.67\\\\times{10}^{-6}\\\\)\"\n\n# Table \nDT |&gt; \n  kbl(align = \"cclrrrr\") |&gt;\n  kable_paper(lightable_options = \"basic\", full_width = TRUE) |&gt;\n  row_spec(0, background = \"#c7eae5\") |&gt;\n  column_spec(1:5, color = \"black\", background = \"white\")\n\n\n\n\n\ntemperature\ndensity\nspecific_weight\nviscosity\nbulk_modulus\n\n\n\n\n\\(273.15\\)\n\\(1000\\)\n\\(9.809\\times{10}^{3}\\)\n\\(1.73\\times{10}^{-6}\\)\n\\(2.02\\times{10}^{9}\\)\n\n\n\\(283.15\\)\n\\(1000\\)\n\\(9.807\\times{10}^{3}\\)\n\\(1.31\\times{10}^{-6}\\)\n\\(2.10\\times{10}^{9}\\)\n\n\n\\(293.15\\)\n\\(998\\)\n\\(9.793\\times{10}^{3}\\)\n\\(1.02\\times{10}^{-6}\\)\n\\(2.18\\times{10}^{9}\\)\n\n\n\\(303.15\\)\n\\(996\\)\n\\(9.768\\times{10}^{3}\\)\n\\(0.82\\times{10}^{-6}\\)\n\\(2.25\\times{10}^{9}\\)\n\n\n\\(313.15\\)\n\\(992\\)\n\\(9.734\\times{10}^{3}\\)\n\\(0.67\\times{10}^{-6}\\)\n\\(2.28\\times{10}^{9}\\)\n\n\n\n\n\n\n\nThis revision satisfies two conventions of tabulating empirical engineering information.\n\nUnits.   With all the reported values reported to the same power-of-ten, the units can all be interpreted in the same way. In this case for example, the units of the viscosity coefficients (1.73, 1.31, etc.) are all micro-Pascal-seconds (\\(\\mu\\)Pa-s).\nUncertainty.   In rewriting the two viscosity values, I changed from three significant digits to two decimal places, consistent with the assumption that empirical information is reported to the same level of uncertainty unless noted otherwise.\n\n\n\nPotential revision\nAdd the water data to formatdown and the following functionality to format_power().\n\nA new argument (perhaps fixed_power) that automatically selects a fixed exponent for a numerical vector or permits the user to directly assign a fixed exponent.\n format_power(x, digits, format, omit_power, delim, fixed_power)\nIn conjunction with the fixed power-of-ten, I would also round all numbers in a column to the same number of decimal places to address the uncertainty assumption. This could be a separate argument."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#units",
    "href": "posts/2022-11-18-formatdown/index.html#units",
    "title": "Introducing formatdown",
    "section": "Units",
    "text": "Units\nAnd now for something completely different!\nThinking about measurement units, I looked for relevant R packages and found units. With appropriate units, powers-of-ten notation can be practically eliminated. For example, a pressure reading of \\(2.02\\times{10}^{9}\\) Pa can be reported as \\(2.02\\) GPa.\nTo illustrate, I start with the basic water data,\n\n\nR code\nwater\n\n\n   temperature density specific_weight viscosity bulk_modulus\n         &lt;num&gt;   &lt;num&gt;           &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:      273.15    1000            9809  1.73e-06     2.02e+09\n2:      283.15    1000            9807  1.31e-06     2.10e+09\n3:      293.15     998            9793  1.02e-06     2.18e+09\n4:      303.15     996            9768  8.17e-07     2.25e+09\n5:      313.15     992            9734  6.70e-07     2.28e+09\n\n\nWith tools from the units package, I can define a symbol uP to represent micropoise (a non-SI viscosity unit equal to 10\\(^{-7}\\) Pa-s). And I can write a short function to convert the numbers from basic units to displayed units, for example, converting Pa to GPa (gigapascal) or Pa-s to \\(\\mu\\)P (micropoise).\n\n\nR code\nlibrary(\"units\")\n\n# Define the uP units\ninstall_unit(\"uP\", \"micropoise\", \"micropoise\")\n\n# Function to assign and convert units \nassign_units &lt;- function(x, base_unit, display_unit) {\n  \n  # convert x to \"Units\" class in base units\n  units(x) &lt;- base_unit\n  \n  # convert from basic to display units\n  units(x) &lt;- as_units(display_unit)\n  \n  # return value\n  x\n}\n\n\nConvert each column and output the results.\n\n\nR code\n# Apply to one variable at a time\nDT &lt;- copy(water)\nDT$temperature     &lt;- assign_units(DT$temperature, \"K\", \"degree_C\")\nDT$density         &lt;- assign_units(DT$density, \"kg/m^3\", \"kg/m^3\")\nDT$specific_weight &lt;- assign_units(DT$specific_weight, \"N/m^3\", \"kN/m^3\")\nDT$viscosity       &lt;- assign_units(DT$viscosity, \"Pa*s\", \"uP\")\nDT$bulk_modulus    &lt;- assign_units(DT$bulk_modulus, \"Pa\", \"GPa\")\n\n# Output\nDT |&gt; \n  kbl(align = \"r\") |&gt;\n  kable_paper(lightable_options = \"basic\", full_width = TRUE) |&gt;\n  row_spec(0, background = \"#c7eae5\") |&gt;\n  column_spec(1:5, color = \"black\", background = \"white\") \n\n\n\n\n\ntemperature\ndensity\nspecific_weight\nviscosity\nbulk_modulus\n\n\n\n\n0 [°C]\n1000 [kg/m^3]\n9.809 [kN/m^3]\n17.30 [uP]\n2.02 [GPa]\n\n\n10 [°C]\n1000 [kg/m^3]\n9.807 [kN/m^3]\n13.10 [uP]\n2.10 [GPa]\n\n\n20 [°C]\n998 [kg/m^3]\n9.793 [kN/m^3]\n10.20 [uP]\n2.18 [GPa]\n\n\n30 [°C]\n996 [kg/m^3]\n9.768 [kN/m^3]\n8.17 [uP]\n2.25 [GPa]\n\n\n40 [°C]\n992 [kg/m^3]\n9.734 [kN/m^3]\n6.70 [uP]\n2.28 [GPa]\n\n\n\n\n\n\n\nThe entries in the data frame are still numeric but are of the “Units” class, enabling math operations among values with compatible units. See the units website for details.\n\n\nR code\nstr(DT)\n\n\nClasses 'data.table' and 'data.frame':  5 obs. of  5 variables:\n $ temperature    : Units: [°C] num  0 10 20 30 40\n $ density        : Units: [kg/m^3] num  1000 1000 998 996 992\n $ specific_weight: Units: [kN/m^3] num  9.81 9.81 9.79 9.77 9.73\n $ viscosity      : Units: [uP] num  17.3 13.1 10.2 8.17 6.7\n $ bulk_modulus   : Units: [GPa] num  2.02 2.1 2.18 2.25 2.28\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nIf I were to refine this table further, I would report the numerical values without labels in each cell, moving the unit labels to a sub-header row. Possible future work.\n\nPotential revision\nIncorporate tools from the units package to create a new function (perhaps format_units()) that would convert basic units to display units that can substitute for powers-of-ten notation."
  },
  {
    "objectID": "posts/2022-11-18-formatdown/index.html#closing",
    "href": "posts/2022-11-18-formatdown/index.html#closing",
    "title": "Introducing formatdown",
    "section": "Closing",
    "text": "Closing\nThe new formatdown package formats numbers in powers-of-ten notation for inline math markup. A new argument is already in the works for managing the math delimiters. Potential new features include a fixed power-of-tens option as well as replacing powers-of-ten notation with deliberate manipulation of physical units."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html",
    "href": "posts/2023-07-08-midfieldr/index.html",
    "title": "midfieldr v1.0.1",
    "section": "",
    "text": "Summary\n\n\n\nA qualitative overview of the midfieldr package and its application following its initial CRAN release. midfieldr provides tools and recommended methods for working with individual undergraduate student-level records (registrar’s longitudinal data) in R.\nmidfieldr is designed to work with data from the MIDFIELD research database, a sample of which is available in the midfielddata data package. Tools in midfieldr include filters for US academic program codes, data sufficiency, and timely completion. Recommended methods—illustrated in the package website—include gathering blocs of records, computing quantitative metrics such as graduation rate, and creating charts to visualize comparisons."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html#background",
    "href": "posts/2023-07-08-midfieldr/index.html#background",
    "title": "midfieldr v1.0.1",
    "section": "Background",
    "text": "Background\nBegun in 2004 as an extension of the SUCCEED Longitudinal Database, MIDFIELD contains student records for all undergraduate, degree-seeking students at partner institutions, currently 2.4M unique students at 21 US institutions from 1987 to 2022.\nWhile originally intended for studying engineering programs, the database can be used to study any set of programs at the member institutions—the data are whole-population data, that is, student records for all undergraduates for the span of years provided by each institution.\nAn early version of the package was presented at useR! 2018 in Brisbane [slide deck]. In the five years since that talk, the package has undergone significant development, testing, and revision such that the current release (v1.0.1) is ready for dissemination via CRAN."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html#data-structure",
    "href": "posts/2023-07-08-midfieldr/index.html#data-structure",
    "title": "midfieldr v1.0.1",
    "section": "Data structure",
    "text": "Data structure\nThe functions in midfieldr are designed to interact with the data structure implemented in the MIDFIELD database. MIDFIELD data are organized in four tables (student, term, course, degree) linked by an anonymized student ID with variables as outlined below. (The midfielddata website provides a data dictionary.)\n\n\n\nFigure 1: MIDFIELD data structure.\n\n\nEach table is in block-record form consistent with Codd’s 2nd rule for relational databases:\n\nEach and every datum (atomic value) in a relational data base is guaranteed to be logically accessible by resorting to a combination of table name, primary key value, and column name.\n\nIn MIDFIELD, student, term, course, and degree are the tables; the anonymized student ID is the primary key; and the column names encode the variables outlined in Figure 1. Each row is an observation and each column a variable."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html#what-midfieldr-does",
    "href": "posts/2023-07-08-midfieldr/index.html#what-midfieldr-does",
    "title": "midfieldr v1.0.1",
    "section": "What midfieldr does",
    "text": "What midfieldr does\nThe purpose of midfieldr functions is to implement what we consider good practices for treating longitudinal student-level data.\nFor example, consider the concept of data sufficiency. The time span of MIDFIELD data varies by institution, each having their own lower and upper bounds. For some student records, being at or near these bounds creates unavoidable ambiguity when trying to assess degree completion. Such records must be identified and in most cases excluded to prevent false summary counts.\nTo illustrate, consider a student whose first term at an institution occurs two years before the upper limit of their institution’s data range. A researcher has no way of knowing the student’s completion status. The student may have: graduated in a timely manner (say, in 6 years or less); graduated, but in more than 6 years; or left the database without a degree. Failing to exclude such students leads to false counts when grouping and summarizing blocs of records, for example, when counting starters, graduates, ever-enrolled, etc.\nThe articles posted to the midfieldr website describe how the package is used to treat fundamental issues (like data sufficiency) in the context of longitudinal student-level data. Thus, midfieldr provides tools and recommended methods designed specifically for treating student-level records.\nTools include:\n\nadd_completion_status() Determine completion status for every student\nadd_data_sufficiency() Determine data sufficiency for every student\nadd_timely_term() Calculate a timely completion term for every student\nfilter_cip() Subset rows that include matches to search strings\norder_multiway() Order categorical variables of multiway data\nprep_fye_mice() Prepare First-Year Engineering (FYE) data for multiple imputation\n\nMethods include:\n\nPlanning. Identify the groups of students, programs, and metrics with which we intend to work.\nInitial processing. Filter for data sufficiency, degree-seeking, and academic programs.\nBlocs. Identify and label records to be treated as a unit, for example, starters, students ever-enrolled, graduates, transfer students, etc.\nGroupings. Add relevant grouping variables such as race/ethnicity, sex, and program label. Group and summarize.\n\nMetrics. Compute measures of academic success such as graduation rates, stickiness, etc., disaggregated by grouping variables.\nDisplays. Display results of quantitative metrics in charts and tables."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html#sample-result",
    "href": "posts/2023-07-08-midfieldr/index.html#sample-result",
    "title": "midfieldr v1.0.1",
    "section": "Sample result",
    "text": "Sample result\nFigure 2 displays a count of engineering graduates grouped by race/ethnicity, sex, and program—illustrating a typical set of grouping variables and a typical chart design (a Cleveland multiway chart). The script for this specific chart is given in the multiway article in the package website.\n\n\n\nFigure 2: Count of engineering graduates from the practice data in the midfielddata package\n\n\nNote that midfielddata is suitable for learning to work with student-level data but not for drawing inferences about program attributes or student experiences. midfielddata supplies practice data, not research data."
  },
  {
    "objectID": "posts/2023-07-08-midfieldr/index.html#notes-on-syntax",
    "href": "posts/2023-07-08-midfieldr/index.html#notes-on-syntax",
    "title": "midfieldr v1.0.1",
    "section": "Notes on syntax",
    "text": "Notes on syntax\nThroughout the work, we use the data.table package for data manipulation (Dowle & Srinivasan, 2021) and the ggplot2 package for charts (Wickham, 2016). Some users may prefer base R or dplyr for data (Wickham et al., 2023) or lattice for charts (Sarkar, 2008). Each system has its strengths—users are welcome to translate our examples to their preferred syntax.\nNote that midfieldr functions yield data.table-type data frames and do not preserve tibble structures. A user wanting to use the tibble form in tidyverse-style scripts would probably want to apply as_tibble() following each application of most midfieldr functions."
  }
]